{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10741822,"sourceType":"datasetVersion","datasetId":6661170},{"sourceId":10753537,"sourceType":"datasetVersion","datasetId":6669522},{"sourceId":10821326,"sourceType":"datasetVersion","datasetId":6718778}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport os","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:03:56.978157Z","iopub.execute_input":"2025-02-25T17:03:56.978465Z","iopub.status.idle":"2025-02-25T17:04:00.922409Z","shell.execute_reply.started":"2025-02-25T17:03:56.978430Z","shell.execute_reply":"2025-02-25T17:04:00.921528Z"},"id":"cu0Fn1p_CZC3","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\nBATCH_SIZE = 32\n# VALIDATION_SPLIT = 0.2\nTEST_SPLIT = 0.1\nNUM_EPOCHS = 50\n\n# Constants\nFRAME_HEIGHT = 224\nFRAME_WIDTH = 224\nSEQUENCE_LENGTH = 30\n\nARCHITECTURE = '2D_CNN_LSTM'\nCNN_TYPE = 'resnet18'\n# CNN_TYPE = 'mobilenet_v3'\nLSTM_HIDDEN_SIZE=512\nLSTM_LAYERS = 1\nDROPOUT = 0.5\nLEARNING_RATE = 0.001","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:00.923293Z","iopub.execute_input":"2025-02-25T17:04:00.923683Z","iopub.status.idle":"2025-02-25T17:04:00.928182Z","shell.execute_reply.started":"2025-02-25T17:04:00.923637Z","shell.execute_reply":"2025-02-25T17:04:00.927280Z"},"id":"VqZWlAq2mGn6","trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# # Mount Google Drive\n# from google.colab import drive\n# import os\n\n# drive.mount('/content/gdrive')\n# # Define base folder path\n# base_path = '/content/gdrive/MyDrive/RGB_data_stream'\n# !ls /content/gdrive/MyDrive/RGB_data_stream/\n# # base_path = '../'","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-25T17:04:00.929284Z","iopub.execute_input":"2025-02-25T17:04:00.929622Z","iopub.status.idle":"2025-02-25T17:04:00.946851Z","shell.execute_reply.started":"2025-02-25T17:04:00.929592Z","shell.execute_reply":"2025-02-25T17:04:00.945976Z"},"id":"6PapSuKZmGn6","outputId":"28e1a074-65bf-491b-a390-e6d4b520d541","trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# import os\n\n# # Gets label\n# csv_path = os.path.join(base_path,'data/3DYoga90_corrected.csv')\n# meta_info_path = os.path.join(base_path, 'data')\n# # Gets valid samples\n# sequence_path = os.path.join(base_path, 'short/downloaded_log.txt')\n# corrupted_path = os.path.join(base_path, 'short/corrupted_log.txt')\n\n# # Original Downloaded Seqeuence\n# video_dir = os.path.join(base_path, 'short')\n\n# # Pre_processed to tensor to make training fast # Makes video_dir redundant BUT NOT WITH KAGGLE IDK\n# preprocessed_dir = os.path.join(base_path, 'RESIZED_SHORTS')\n# os.makedirs(preprocessed_dir, exist_ok=True)\n# assert os.path.isdir(preprocessed_dir), f\"Directory '{preprocessed_dir}' does not exist.\"","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:00.948780Z","iopub.execute_input":"2025-02-25T17:04:00.949010Z","iopub.status.idle":"2025-02-25T17:04:00.963701Z","shell.execute_reply.started":"2025-02-25T17:04:00.948991Z","shell.execute_reply":"2025-02-25T17:04:00.962809Z"},"id":"QGr5-KAQmGn7","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# import os\n\n# # root directory of uploaded dataset\n# # dataset_root = \"/kaggle/input/3dyoga90/\"\n# dataset_root = \"/kaggle/input/\"\n\n# # video files directory\n# video_dir = os.path.join(dataset_root, \"short/short/\")\n\n# # metadata directory\n# metadata_dir = os.path.join(dataset_root, \"data/data/\")\n\n# # csv file containing labels\n# csv_path = os.path.join(metadata_dir, \"3DYoga90_corrected.csv\")\n\n# # path to store additional metadata files\n# meta_info_path = os.path.join('/kaggle/working/', 'fold_save')  # same as metadata_dir\n# os.makedirs(meta_info_path, exist_ok=True)\n# # paths for logging valid and corrupted samples\n# sequence_path = os.path.join(video_dir, \"downloaded_log.txt\")\n# corrupted_path = os.path.join('/kaggle/working/', \"corrupted_log.txt\")\n\n# # directory to store preprocessed videos\n# preprocessed_dir = \"/kaggle/tmp/preprocessed_videos/\"\n# os.makedirs(preprocessed_dir, exist_ok=True)\n\n# FOLD_CHECKPOINT_PATH = os.path.join('/kaggle/working/', 'FOLD_CHECKPOINT')\n\n\n# # print paths for verification\n# print(f\"dataset root: {dataset_root}\")\n# print(f\"video directory: {video_dir}\")\n# print(f\"metadata directory: {metadata_dir}\")\n# print(f\"csv path: {csv_path}\")\n# print(f\"metadata info path: {meta_info_path}\")\n# print(f\"valid sequences log: {sequence_path}\")\n# # print(f\"corrupted sequences log: {corrupted_log_path}\")\n# print(f\"preprocessed data directory: {preprocessed_dir}\")","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:00.965014Z","iopub.execute_input":"2025-02-25T17:04:00.965211Z","iopub.status.idle":"2025-02-25T17:04:00.981877Z","shell.execute_reply.started":"2025-02-25T17:04:00.965194Z","shell.execute_reply":"2025-02-25T17:04:00.981185Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\n\n# root directory of uploaded dataset\n# dataset_root = \"/kaggle/input/updated-yoga-dataset/\"\n# dataset_root = \"/kaggle/input/\"\nold_dataset_root = \"/kaggle/input/yoga-dataset/\"\ndataset_root = old_dataset_root\n# video files directory\nvideo_dir = os.path.join(old_dataset_root, \"short/short/\")\n\n# metadata directory\nmetadata_dir = os.path.join(dataset_root, \"data/data/\")\n\n# csv file containing labels\ncsv_path = os.path.join(metadata_dir, \"3DYoga90_corrected.csv\")\n\n# path to store additional metadata files\nmeta_info_path = os.path.join('/kaggle/working/', 'fold_save')  # same as metadata_dir\nos.makedirs(meta_info_path, exist_ok=True)\n# paths for logging valid and corrupted samples\nsequence_path = os.path.join(video_dir, \"downloaded_log.txt\")\ncorrupted_path = os.path.join('/kaggle/working/', \"corrupted_log.txt\")\n\n# directory to store preprocessed videos\npreprocessed_dir = os.path.join(dataset_root, \"RESIZED_DATA\")\nos.makedirs(preprocessed_dir, exist_ok=True)\n\nFOLD_CHECKPOINT_PATH = os.path.join('/kaggle/working/', 'FOLD_CHECKPOINT')\n\n\n# print paths for verification\nprint(f\"dataset root: {dataset_root}\")\nprint(f\"video directory: {video_dir}\")\nprint(f\"metadata directory: {metadata_dir}\")\nprint(f\"csv path: {csv_path}\")\nprint(f\"metadata info path: {meta_info_path}\")\nprint(f\"valid sequences log: {sequence_path}\")\n# print(f\"corrupted sequences log: {corrupted_log_path}\")\nprint(f\"preprocessed data directory: {preprocessed_dir}\")","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:00.982642Z","iopub.execute_input":"2025-02-25T17:04:00.982954Z","iopub.status.idle":"2025-02-25T17:04:01.011444Z","shell.execute_reply.started":"2025-02-25T17:04:00.982927Z","shell.execute_reply":"2025-02-25T17:04:01.010745Z"},"trusted":true},"outputs":[{"name":"stdout","text":"dataset root: /kaggle/input/yoga-dataset/\nvideo directory: /kaggle/input/yoga-dataset/short/short/\nmetadata directory: /kaggle/input/yoga-dataset/data/data/\ncsv path: /kaggle/input/yoga-dataset/data/data/3DYoga90_corrected.csv\nmetadata info path: /kaggle/working/fold_save\nvalid sequences log: /kaggle/input/yoga-dataset/short/short/downloaded_log.txt\npreprocessed data directory: /kaggle/input/yoga-dataset/RESIZED_DATA\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!ls /kaggle/input/yoga-dataset","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:01.012357Z","iopub.execute_input":"2025-02-25T17:04:01.012693Z","iopub.status.idle":"2025-02-25T17:04:01.146899Z","shell.execute_reply.started":"2025-02-25T17:04:01.012647Z","shell.execute_reply":"2025-02-25T17:04:01.145975Z"},"trusted":true},"outputs":[{"name":"stdout","text":"data  RESIZED_DATA  short\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!ls /kaggle/input/3dyoga90/data/data/","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:01.147878Z","iopub.execute_input":"2025-02-25T17:04:01.148126Z","iopub.status.idle":"2025-02-25T17:04:01.284204Z","shell.execute_reply.started":"2025-02-25T17:04:01.148105Z","shell.execute_reply":"2025-02-25T17:04:01.283129Z"},"trusted":true},"outputs":[{"name":"stdout","text":"2D_CNN_LSTM_folds.pkl\t3DYoga90.csv\t      criterion_fold_1.pkl  skf.pkl\n2D_CNN_LSTM_skf.pkl\t3DYoga90.json\t      folds.pkl\n3DYoga90_corrected.csv\tcriterion_fold_0.pkl  pose-index.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\n\n# Define paths\npaths_to_check = {\n    \"dataset root\": dataset_root,\n    \"video directory\": video_dir,\n    \"metadata directory\": metadata_dir,\n    \"csv file\": csv_path,\n    \"metadata info path\": meta_info_path,\n    \"valid sequences log\": sequence_path,\n    \"corrupted sequences log\": corrupted_path,\n    \"preprocessed data directory\": preprocessed_dir\n}\n\n# Check existence\nfor name, path in paths_to_check.items():\n    if os.path.exists(path):\n        if os.path.isfile(path):\n            print(f\"[✔] {name} exists and is a file: {path}\")\n        elif os.path.isdir(path):\n            print(f\"[✔] {name} exists and is a directory: {path}\")\n    else:\n        print(f\"[✘] {name} does NOT exist: {path}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:01.285297Z","iopub.execute_input":"2025-02-25T17:04:01.285694Z","iopub.status.idle":"2025-02-25T17:04:01.314471Z","shell.execute_reply.started":"2025-02-25T17:04:01.285640Z","shell.execute_reply":"2025-02-25T17:04:01.313755Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[✔] dataset root exists and is a directory: /kaggle/input/yoga-dataset/\n[✔] video directory exists and is a directory: /kaggle/input/yoga-dataset/short/short/\n[✔] metadata directory exists and is a directory: /kaggle/input/yoga-dataset/data/data/\n[✔] csv file exists and is a file: /kaggle/input/yoga-dataset/data/data/3DYoga90_corrected.csv\n[✔] metadata info path exists and is a directory: /kaggle/working/fold_save\n[✔] valid sequences log exists and is a file: /kaggle/input/yoga-dataset/short/short/downloaded_log.txt\n[✘] corrupted sequences log does NOT exist: /kaggle/working/corrupted_log.txt\n[✔] preprocessed data directory exists and is a directory: /kaggle/input/yoga-dataset/RESIZED_DATA\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"file_path = os.path.join(video_dir, \"missing_videos_log.txt\")\n\n# Count lines in the file\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    line_count = sum(1 for _ in f)\n\nprint(f\"Number of lines in missing_videos.txt: {line_count}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-25T17:04:01.315245Z","iopub.execute_input":"2025-02-25T17:04:01.315466Z","iopub.status.idle":"2025-02-25T17:04:01.333574Z","shell.execute_reply.started":"2025-02-25T17:04:01.315448Z","shell.execute_reply":"2025-02-25T17:04:01.332913Z"},"id":"P5auu6m6oAwT","outputId":"4f1c6bd5-c548-41c8-fa0e-295edc7cbe20","trusted":true},"outputs":[{"name":"stdout","text":"Number of lines in missing_videos.txt: 303\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"file_path = os.path.join(video_dir, \"downloaded_log.txt\")\n\n# Count lines in the file\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    line_count = sum(1 for _ in f)\n\nprint(f\"Number of lines in missing_videos.txt: {line_count}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-25T17:04:01.334315Z","iopub.execute_input":"2025-02-25T17:04:01.334529Z","iopub.status.idle":"2025-02-25T17:04:01.345222Z","shell.execute_reply.started":"2025-02-25T17:04:01.334510Z","shell.execute_reply":"2025-02-25T17:04:01.344478Z"},"id":"AikpWHK1oNj4","outputId":"3dec5be8-8ddc-4d4d-f11c-91dc7533aa79","trusted":true},"outputs":[{"name":"stdout","text":"Number of lines in missing_videos.txt: 1944\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!ls /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:04:01.345949Z","iopub.execute_input":"2025-02-25T17:04:01.346170Z","iopub.status.idle":"2025-02-25T17:04:01.471124Z","shell.execute_reply.started":"2025-02-25T17:04:01.346151Z","shell.execute_reply":"2025-02-25T17:04:01.470024Z"}},"outputs":[{"name":"stdout","text":"fold_save\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\n# today = datetime.today().strftime('%Y-%m-%d')\n# SAVE_PATH = os.path.join(\n#     '/kaggle/working/',\n#     f'BS{BATCH_SIZE}_KAGGLE_CROSS_FOLD_{ARCHITECTURE}_{today}_LR{LEARNING_RATE}_LHS{LSTM_HIDDEN_SIZE}_LL{LSTM_LAYERS}_CT{CNN_TYPE}'\n# )\n# os.makedirs(SAVE_PATH, exist_ok=True)\n# print(SAVE_PATH)\n\npose_list = ['downward-dog','standing-forward-bend','half-way-lift',\n             'mountain','chair','cobra','cockerel','extended-triangle',\n             'extended-side-angle','corpse','staff','wind-relieving','fish'\n            ]\n\nsubset_of_poses = pose_list\nNUM_CLASSES = len(pose_list)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-25T17:04:01.475164Z","iopub.execute_input":"2025-02-25T17:04:01.475409Z","iopub.status.idle":"2025-02-25T17:04:01.480179Z","shell.execute_reply.started":"2025-02-25T17:04:01.475389Z","shell.execute_reply":"2025-02-25T17:04:01.479330Z"},"id":"1QiOQi7BCZC5","outputId":"0a9df3fd-c15a-44c6-9f87-61cb0853fcff","trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"SAVE_PATH = '/kaggle/working/BS32_KAGGLE_CROSS_FOLD_2D_CNN_LSTM_2025-02-23_LR0.001_LHS512_LL1_CTresnet18'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:04:01.481960Z","iopub.execute_input":"2025-02-25T17:04:01.482190Z","iopub.status.idle":"2025-02-25T17:04:01.495886Z","shell.execute_reply.started":"2025-02-25T17:04:01.482171Z","shell.execute_reply":"2025-02-25T17:04:01.495139Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"print(torch.__version__)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-02-25T17:04:01.496689Z","iopub.execute_input":"2025-02-25T17:04:01.496982Z","iopub.status.idle":"2025-02-25T17:04:01.560442Z","shell.execute_reply.started":"2025-02-25T17:04:01.496952Z","shell.execute_reply":"2025-02-25T17:04:01.559602Z"},"id":"H_mVAnHmCZC6","outputId":"cdfb009e-8b2f-4beb-c4df-9d77fa77cb97","trusted":true},"outputs":[{"name":"stdout","text":"2.5.1+cu121\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# meta_info_path = os.path.join(base_path, 'data')\n# pose_index = pd.read_csv(f'{meta_info_path}/pose-index.csv')\n# sequence_index = pd.read_csv(f'{meta_info_path}/3DYoga90_corrected.csv')","metadata":{"id":"SCqudip1CZC6","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:04:01.561342Z","iopub.execute_input":"2025-02-25T17:04:01.561706Z","iopub.status.idle":"2025-02-25T17:04:01.584904Z","shell.execute_reply.started":"2025-02-25T17:04:01.561674Z","shell.execute_reply":"2025-02-25T17:04:01.584009Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"What does each file tell?\n\n1.) pose-index.csv -> Shows Heirarchical organization (THEN NOTHING MORE)\n\n2.) 3DYoga90.csv -> Total Main Info(i.e. along with RGB stream){\n    SequneceID: Parquet_FILE_NAME,\n    URL,\n    Frame Start and Frame Stop,\n    Pose Name, Training Test Split\n} `Difference between train and test? where to get the validation set from? How to do data augmentation?\n\n3.) Parquet Files -> {\n    Frame Number {\n        33 Landmarks\n    },\n    row-id: FrameNumber-TYPE-Landmark_index,\n    Coordinates: {x, y, z}\n}\n\n`PLEASE NOTE: The landmark coordinates are all normalized`","metadata":{"id":"TpLOGWwACZC7"}},{"cell_type":"markdown","source":"# Getting the data ready","metadata":{"id":"1y0RRgDxCZC7"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport torchvision.transforms.v2 as T\nimport random\n\nclass VideoAugmentationPipeline:\n    \"\"\"Video augmentation pipeline using batch transforms from torchvision.transforms.v2\"\"\"\n    def __init__(self, spatial_aug_config=None, temporal_aug_config=None):\n        # Default config with all augmentations enabled\n        default_spatial_config = {\n            'random_resized_crop': {'enabled': False, 'scale': (0.9, 1.0)},\n            'random_horizontal_flip': {'enabled': True, 'p': 0.5},\n            'color_jitter': {'enabled': True, 'brightness': 0.1, 'contrast': 0.1, 'saturation': 0.1, 'p': 0.5},\n            'gaussian_blur': {'enabled': True, 'p': 0.5},\n            'random_rotation': {'enabled': True, 'degrees': (-5, 5),'p':0.5},\n        }\n\n        default_temporal_config = {\n            'temporal_crop': {'enabled': True, 'crop_size': 0.9},\n            'temporal_mask': {'enabled': True, 'n_masks': 1, 'mask_size': 0.1},\n        }\n\n        # Update default config with user-provided config\n        self.spatial_aug_config = self._update_config(default_spatial_config, spatial_aug_config)\n        self.temporal_aug_config = self._update_config(default_temporal_config, temporal_aug_config)\n\n        # Build transforms that can handle batch inputs\n        self.spatial_transforms = self._build_spatial_transforms()\n\n    def _update_config(self, default_config, user_config):\n        \"\"\"Update default config with user config, disabling augmentations not in user config\"\"\"\n        if user_config is None:\n            return default_config\n\n        updated_config = default_config.copy()\n        for aug_name in updated_config:\n            if aug_name in user_config:\n                # # Update probability if provided\n                # if isinstance(user_config[aug_name], dict):\n                #     updated_config[aug_name].update(user_config[aug_name])\n                # else:\n                    updated_config[aug_name]['p'] = user_config[aug_name]\n            else:\n                # Disable augmentation if not in user config\n                updated_config[aug_name]['enabled'] = False\n        return updated_config\n\n    def _build_spatial_transforms(self):\n        \"\"\"Build composition of spatial transforms that support batch processing\"\"\"\n        transform_list = []\n\n        if self.spatial_aug_config['random_resized_crop']['enabled']:\n            transform_list.append(\n                T.RandomResizedCrop(\n                    size=(224, 224),\n                    scale=self.spatial_aug_config['random_resized_crop']['scale'],\n                    antialias=True\n                )\n            )\n\n        if self.spatial_aug_config['random_horizontal_flip']['enabled']:\n            transform_list.append(\n                T.RandomHorizontalFlip(p=self.spatial_aug_config['random_horizontal_flip']['p'])\n            )\n\n        if self.spatial_aug_config['color_jitter']['enabled']:\n            transform_list.append(\n                T.ColorJitter(\n                    brightness=self.spatial_aug_config['color_jitter']['brightness'],\n                    contrast=self.spatial_aug_config['color_jitter']['contrast'],\n                    saturation=self.spatial_aug_config['color_jitter']['saturation']\n                )\n            )\n\n        if self.spatial_aug_config['gaussian_blur']['enabled']:\n            transform_list.append(\n                T.GaussianBlur(\n                    kernel_size=(5, 5),\n                    sigma=(0.1, 1.0)\n                )\n            )\n\n        if self.spatial_aug_config['random_rotation']['enabled']:\n            if random.random() < self.spatial_aug_config['random_rotation']['p']:\n                transform_list.append(\n                    T.RandomRotation(\n                        degrees=self.spatial_aug_config['random_rotation']['degrees'],\n                        interpolation=T.InterpolationMode.BILINEAR\n                    )\n                )\n\n        return T.Compose(transform_list)\n\n    def apply_temporal_augmentation(self, video_tensor):\n        \"\"\"Apply temporal augmentations to video tensor\"\"\"\n        if not any(cfg['enabled'] for cfg in self.temporal_aug_config.values()):\n            return video_tensor\n\n        T, C, H, W = video_tensor.shape\n\n        # Temporal crop\n        if self.temporal_aug_config['temporal_crop']['enabled']:\n            crop_size = int(T * self.temporal_aug_config['temporal_crop']['crop_size'])\n            start_idx = random.randint(0, T - crop_size)\n            video_tensor = video_tensor[start_idx:start_idx + crop_size]\n\n        # Temporal masking\n        if self.temporal_aug_config['temporal_mask']['enabled']:\n            T = len(video_tensor)\n            mask_size = int(T * self.temporal_aug_config['temporal_mask']['mask_size'])\n            for _ in range(self.temporal_aug_config['temporal_mask']['n_masks']):\n                if random.random() < 0.5:\n                    start_idx = random.randint(0, T - mask_size)\n                    video_tensor[start_idx:start_idx + mask_size] = 0\n\n        return video_tensor\n\n    def __call__(self, video_tensor):\n        \"\"\"Apply transforms to entire video tensor at once\"\"\"\n        # Input shape: [T, C, H, W]\n        # Reshape to [T, C, H, W] -> [1, T, C, H, W] for batch processing\n        video_tensor = video_tensor.unsqueeze(0)\n\n        # Apply spatial transforms to entire video tensor at once\n        # transforms.v2 will maintain temporal consistency automatically\n        video_tensor = self.spatial_transforms(video_tensor)\n\n        # Remove batch dimension\n        video_tensor = video_tensor.squeeze(0)\n\n        # Apply temporal augmentations\n        # video_tensor = self.apply_temporal_augmentation(video_tensor)\n\n        return video_tensor","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:01.585970Z","iopub.execute_input":"2025-02-25T17:04:01.586305Z","iopub.status.idle":"2025-02-25T17:04:04.092665Z","shell.execute_reply.started":"2025-02-25T17:04:01.586265Z","shell.execute_reply":"2025-02-25T17:04:04.091775Z"},"id":"Kg3fH4QhCZC8","trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport imageio\nimport random\n\nclass YogaVideoDataset(Dataset):\n    def __init__(self, csv_path, sequence_path, pose_list, video_dir, preprocessed_dir,\n                 spatial_aug_config=None, temporal_aug_config=None, use_augmentation=True, aug_ratio = 0.5):\n        with open(sequence_path) as f:\n            sequence_list = f.read().splitlines()\n            sequence_list = [int(x) for x in sequence_list]\n\n        self.df = pd.read_csv(csv_path)\n        self.df = self.df[self.df['sequence_id'].isin(sequence_list)]\n        self.df = self.df[self.df['l3_pose'].isin(pose_list)]\n\n        self.pose_to_label = {pose: idx for idx, pose in enumerate(pose_list)}\n        self.length_of_dataset = len(self.df)\n        self.idx_to_label = {}\n\n        self.video_dir = video_dir\n        self.preprocessed_dir = preprocessed_dir\n        os.makedirs(self.preprocessed_dir, exist_ok=True)\n\n        # Initialize augmentation pipeline\n        self.augmentation_pipeline = VideoAugmentationPipeline(\n            spatial_aug_config=spatial_aug_config,\n            temporal_aug_config=temporal_aug_config\n        )\n\n        self.cache = dict()\n        self.augmentation_ratio = aug_ratio\n        self.use_augmentation = use_augmentation\n\n        self.transforms = transforms.Compose([\n            transforms.Resize((FRAME_HEIGHT, FRAME_WIDTH)),\n            transforms.ToTensor(),\n            # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n            #                      std=[0.229, 0.224, 0.225])\n        ])\n        self.normalization = transforms.Compose([\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])\n    def __len__(self):\n        return self.length_of_dataset\n\n    def __getitem__(self, i):\n        sequence_id = self.df.iloc[i]['sequence_id']\n        pose = self.df.iloc[i]['l3_pose']\n        label = self.pose_to_label[pose]\n        self.idx_to_label[i] = sequence_id\n        # Load preprocessed frames\n        if sequence_id in self.cache:\n            frames = self.cache[sequence_id]\n        else:\n            file_path = os.path.join(self.preprocessed_dir, f\"{sequence_id}.pt\")\n            if not os.path.exists(file_path):\n                video_path = os.path.join(self.video_dir, f\"{sequence_id}.mp4\")\n                frames = self._get_frames(video_path)\n                torch.save(frames, file_path)\n            else:\n                frames = torch.load(file_path, weights_only=True)\n            # self.cache[sequence_id] = frames\n\n        # Choose whether to use augmented or original data\n        use_augmented_data = self.use_augmentation and self.augmentation_ratio < random.random()\n        if use_augmented_data:\n            frames = self.augmentation_pipeline(frames)\n\n        frames = self.normalization(frames)\n        return frames, label\n\n    # def _get_frames(self, video_path, sequence_length = SEQUENCE_LENGTH):\n    #     reader = imageio.get_reader(video_path, 'ffmpeg')\n    #     fps = imageio.get_reader(video_path, 'ffmpeg').get_meta_data()['fps'] // 1\n    #     total_frames = reader.count_frames()\n    #     print(f\"Processing video: {video_path}\")\n    #     print(f\"Video FPS: {fps}\")\n    #     indices = np.arange(0, fps - 1, fps // SEQUENCE_LENGTH, dtype=int)\n\n    #     # print(indices)\n    #     frames = []\n    #     for i, frame in enumerate(reader):\n    #         if i % fps in indices:\n    #             frame = Image.fromarray(frame)\n    #             frame = self.transforms(frame)\n    #             frames.append(frame)\n    #             # print('Frame number', i % fps, 'Frame', i)\n\n    #     reader.close()\n    #     # print('Frame Length', len(frames))\n    #     frames = torch.stack(frames)\n    #     return frames\n    # def _get_frames(self, video_path, sequence_length = SEQUENCE_LENGTH):\n    #     reader = imageio.get_reader(video_path, 'ffmpeg')\n    #     fps = imageio.get_reader(video_path, 'ffmpeg').get_meta_data()['fps'] // 1\n    #     total_frames = reader.count_frames()\n\n    #     print(f\"Processing video: {video_path}\")\n    #     print(f\"Total frames: {total_frames}\")\n\n    #     # Calculate frame indices to sample\n    #     step = max(1, total_frames // sequence_length)\n    #     indices = np.linspace(0, total_frames - 1, num=sequence_length, dtype=int)\n\n    #     frames = []\n    #     for i, frame in enumerate(reader):\n    #         if i in indices:\n    #             frame = Image.fromarray(frame)\n    #             frame = self.transforms(frame)\n    #             frames.append(frame)\n    #             # print('Frame number', i % fps, 'Frame', i)\n\n    #     reader.close()\n    #     print('Frame Length', len(frames))\n    #     frames = torch.stack(frames)\n    #     return frames\n    def _get_frames(self, video_path, sequence_length=SEQUENCE_LENGTH, corrupted_log_path = corrupted_path):\n        reader = imageio.get_reader(video_path, 'ffmpeg')\n        fps = imageio.get_reader(video_path, 'ffmpeg').get_meta_data()['fps'] // 1\n        total_frames = reader.count_frames()\n\n\n\n        # Calculate frame indices to sample\n        indices = np.linspace(0, total_frames - 1, num=sequence_length, dtype=int)\n        t_indices =[]\n        frames = []\n        for i, frame in enumerate(reader):\n            if i in indices:\n                frame = Image.fromarray(frame)\n                frame = self.transforms(frame)\n                frames.append(frame)\n                t_indices.append(i)\n                # print('Frame number', i % fps, 'Frame', i)\n\n        reader.close()\n\n        # If extracted frames do not match sequence_length, log to corrupted_log_path\n        if len(frames) != sequence_length:\n            with open(corrupted_log_path, \"a\") as f:\n                f.write(f\"Corrupted video: {video_path}\\n\")\n                f.write(f\"Expected {sequence_length} frames, got {len(frames)}\\n\")\n                f.write(f\"Selected frame indices: {t_indices}\\n\\n\")\n            print(f\"Processing video: {video_path}\")\n            print(f\"Total frames: {total_frames}\")\n            print(f\"Total frames: {t_indices}\")\n            print(f\"Final frame count: {len(frames)}\")\n\n\n        # Ensure frames tensor has correct shape\n        if frames:\n            frames = torch.stack(frames)\n        else:\n            frames = torch.empty(0)  # Return an empty tensor if no frames are found\n\n        return frames","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:04.093516Z","iopub.execute_input":"2025-02-25T17:04:04.093993Z","iopub.status.idle":"2025-02-25T17:04:04.155298Z","shell.execute_reply.started":"2025-02-25T17:04:04.093959Z","shell.execute_reply":"2025-02-25T17:04:04.154402Z"},"id":"PP9lZQ6YmGn8","trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\nimport torch.nn.utils.rnn as rnn_utils\n\nclass AttentionLayer(nn.Module):\n    \"\"\"Basic attention mechanism for sequence processing\"\"\"\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.W = nn.Linear(input_size, hidden_size)\n        self.V = nn.Linear(hidden_size, 1)\n\n    def forward(self, lstm_output, mask=None):\n        att_scores = self.V(torch.tanh(self.W(lstm_output))).squeeze(-1)\n\n        if mask is not None:\n            att_scores = att_scores.masked_fill(mask == 0, -1e9)\n\n        att_weights = F.softmax(att_scores, dim=1)\n        context = (lstm_output * att_weights.unsqueeze(-1)).sum(1)\n        return context, att_weights\n\nclass CNNLSTM(nn.Module):\n    \"\"\"Modular video action classifier with various configuration options\"\"\"\n    def __init__(self, num_classes,\n                 lstm_hidden_size=512,\n                 lstm_layers=1,\n                 dropout=0.5,\n                 freeze_cnn=True,\n                 use_attention=False,\n                 cnn_model='resnet18'):\n\n        super().__init__()\n        self.use_attention = use_attention\n        self.cnn_model = cnn_model\n\n        # CNN Feature Extractor\n        self.cnn, self.cnn_feature_size = self._build_cnn(cnn_model)\n        self._set_cnn_freeze(freeze_cnn)\n\n        # Adjust LSTM input size based on CNN feature size\n        lstm_input_size = self.cnn_feature_size\n\n        # Sequence Processing\n        self.lstm = nn.LSTM(\n            input_size=lstm_input_size,\n            hidden_size=lstm_hidden_size,\n            num_layers=lstm_layers,\n            batch_first=True,\n            dropout=dropout if lstm_layers > 1 else 0\n        )\n\n        # Attention Mechanism\n        if self.use_attention:\n            self.attention = AttentionLayer(\n                input_size=lstm_hidden_size,\n                hidden_size=lstm_hidden_size\n            )\n\n        # Classification Head\n        self.classifier = self._build_classifier(\n            lstm_hidden_size,\n            num_classes,\n            dropout\n        )\n\n        # Initialize weights\n        self._initialize_weights()\n\n    def _build_cnn(self, model_name):\n        \"\"\"Initialize CNN feature extractor with proper feature sizes\"\"\"\n        if model_name == 'resnet18':\n            cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n            return nn.Sequential(*list(cnn.children())[:-2]), 512  # Remove avgpool and fc\n        elif model_name == 'mobilenet_v3':\n            cnn = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n            # Remove classifier and avgpool layers\n            return nn.Sequential(*list(cnn.children())[:-2]), 576  # Feature size for mobilenet_v3_small\n        else:\n            raise ValueError(f\"Unsupported CNN model: {model_name}\")\n\n    def _build_classifier(self, input_size, num_classes, dropout):\n        \"\"\"Build modular classification head\"\"\"\n        return nn.Sequential(\n            nn.Linear(input_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n\n            nn.Linear(256, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(dropout/2),\n\n            nn.Linear(128, num_classes)\n        )\n\n    def _initialize_weights(self):\n        \"\"\"Initialize weights for classification layers\"\"\"\n        for m in self.classifier.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _set_cnn_freeze(self, freeze):\n        \"\"\"Freeze/unfreeze CNN parameters\"\"\"\n        for param in self.cnn.parameters():\n            param.requires_grad = not freeze\n\n    def forward(self, x, lengths):\n        # Process frames through CNN\n        batch_size, seq_len = x.size(0), x.size(1)\n        x = x.view(batch_size*seq_len, *x.size()[2:])\n        x = self.cnn(x)\n        \n        # Adaptive pooling to handle different feature sizes\n        if self.cnn_model == 'mobilenet_v3':\n            x = F.adaptive_avg_pool2d(x, (1, 1))  # For 576 features\n        else:\n            x = F.adaptive_avg_pool2d(x, (1, 1))  # For 512 features\n            \n        x = x.view(batch_size, seq_len, -1)\n\n        # Process sequence through LSTM\n        packed_x = rnn_utils.pack_padded_sequence(\n            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n        )\n        packed_out, _ = self.lstm(packed_x)\n        lstm_out, _ = rnn_utils.pad_packed_sequence(packed_out, batch_first=True)\n\n        # Get final representation\n        if self.use_attention:\n            mask = self._create_attention_mask(lstm_out.size(1), lengths)\n            context, _ = self.attention(lstm_out, mask)\n        else:\n            # Get last valid time step output\n            context = lstm_out[torch.arange(batch_size), lengths-1, :]\n\n        return self.classifier(context)\n\n    def _create_attention_mask(self, max_len, lengths):\n        \"\"\"Create attention mask from sequence lengths\"\"\"\n        device = lengths.device\n        return torch.arange(max_len, device=device).expand(len(lengths), max_len) < lengths.unsqueeze(1)\n\n    def unfreeze_cnn_layers(self, num_layers=3, start_from_end=True):\n        \"\"\"Gradually unfreeze CNN layers for fine-tuning\"\"\"\n        conv_layers = [m for m in self.cnn.modules() if isinstance(m, nn.Conv2d)]\n        num_total = len(conv_layers)\n\n        if start_from_end:\n            layers_to_unfreeze = conv_layers[-num_layers:]\n        else:\n            layers_to_unfreeze = conv_layers[:num_layers]\n\n        for layer in layers_to_unfreeze:\n            for param in layer.parameters():\n                param.requires_grad = True\n\n    def count_parameters(self):\n        total_params = 0\n        for name, parameter in self.named_parameters():\n            if parameter.requires_grad:\n                params = parameter.numel()\n                print(f\"{name}: {params}\")\n                total_params += params\n        print(f\"Total Trainable Params: {total_params}\")","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:04.156177Z","iopub.execute_input":"2025-02-25T17:04:04.156458Z","iopub.status.idle":"2025-02-25T17:04:04.172825Z","shell.execute_reply.started":"2025-02-25T17:04:04.156423Z","shell.execute_reply":"2025-02-25T17:04:04.171981Z"},"id":"p8m8Y6qls6qv","trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import os\nimport torch\ndef save_checkpoint(model, optimizer, epoch, history, save_path = SAVE_PATH, best_path=None):\n    if best_path is not None:\n        # chk_path = os.path.join(save_path, f'best_model.pth')\n        chk_path = best_path\n        print(f\"Saving checkpoint to {chk_path}\")\n    else:\n        chk_path = os.path.join(save_path, f'checkpath_model.pth')\n        print(f\"Saving checkpoint to {chk_path}\")\n\n    # Combine model, optimizer, and history into one dictionary\n    checkpoint = {\n        'epoch': epoch + 1,  # Save the next epoch number for resuming\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'history': history  # Save history along with the model and optimizer\n    }\n\n    # Save everything in a single file using torch.save\n    torch.save(checkpoint, chk_path)\n    print(f\"Checkpoint saved at epoch {epoch + 1}\")","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:04.173615Z","iopub.execute_input":"2025-02-25T17:04:04.173880Z","iopub.status.idle":"2025-02-25T17:04:04.192811Z","shell.execute_reply.started":"2025-02-25T17:04:04.173860Z","shell.execute_reply":"2025-02-25T17:04:04.191841Z"},"id":"GFgBfmOSCZC9","trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def load_checkpoint(model, optimizer, checkpoint_path):\n    \"\"\"\n    Load model and training state from a checkpoint\n    \"\"\"\n    print(f\"Loading checkpoint from {checkpoint_path}\")\n    checkpoint = torch.load(checkpoint_path, weights_only = False)\n\n    # Load model and optimizer states\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    # Get the epoch number to resume from\n    start_epoch = checkpoint['epoch']\n\n    # Load training history with new metrics\n    history = checkpoint.get('history', {\n        'train_loss': [], 'val_loss': [],\n        'train_acc': [], 'val_acc': [],\n        'train_precision': [], 'train_recall': [], 'train_f1': [],\n        'val_precision': [], 'val_recall': [], 'val_f1': [],\n        'learning_rates': []\n    })\n\n    return model, optimizer, start_epoch, history\n","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:04.193556Z","iopub.execute_input":"2025-02-25T17:04:04.193875Z","iopub.status.idle":"2025-02-25T17:04:04.208131Z","shell.execute_reply.started":"2025-02-25T17:04:04.193846Z","shell.execute_reply":"2025-02-25T17:04:04.207436Z"},"id":"mZprkJQvCZC9","trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class EarlyStopping:\n    \"\"\"Early stopping to prevent overfitting\"\"\"\n    def __init__(self, patience=7, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        # on default = 7 successive val_loss increase stop\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:04.208977Z","iopub.execute_input":"2025-02-25T17:04:04.209263Z","iopub.status.idle":"2025-02-25T17:04:04.227793Z","shell.execute_reply.started":"2025-02-25T17:04:04.209235Z","shell.execute_reply":"2025-02-25T17:04:04.227056Z"},"id":"dkDP9PAlCZC9","trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\nimport os\nimport matplotlib.pyplot as plt\n\ndef plot_per_class_metric(precision, recall, f1, epoch=0, pose_list=None, master_save_path=SAVE_PATH):\n    \"\"\"\n    Plots and saves per-class metrics (Precision, Recall, F1-score) for the given epoch.\n\n    Args:\n        precision (list): Per-class precision values.\n        recall (list): Per-class recall values.\n        f1 (list): Per-class F1-score values.\n        epoch (int): Current epoch number.\n        pose_list (list): List of class names (strings) corresponding to class indices.\n        master_save_path (str): Directory to save the plot.\n    \"\"\"\n    if pose_list is None:\n        pose_list = [str(i) for i in range(len(precision))]  # Default to numeric labels\n\n    save_dir = os.path.join(master_save_path, 'per_class_metric')\n    os.makedirs(save_dir, exist_ok=True)\n\n    save_file_path = os.path.join(save_dir, f'per_class_metric_epoch_{epoch}.png')\n\n    # Adjust x-axis positions for grouped bars\n    x = range(len(pose_list))\n\n    plt.bar([i - 0.2 for i in x], precision, width=0.2, label='Precision', align='center')\n    plt.bar(x, recall, width=0.2, label='Recall', align='center')\n    plt.bar([i + 0.2 for i in x], f1, width=0.2, label='F1-Score', align='center')\n\n    plt.xlabel('Class')\n    plt.ylabel('Score')\n    plt.title(f'Per-Class Metrics - Epoch {epoch}')\n    plt.xticks(x, pose_list, rotation=45, ha='right')  # Use pose_list for x-axis labels\n    plt.legend()\n    plt.tight_layout()  # Adjust layout to fit rotated labels\n    plt.savefig(save_file_path)\n    plt.show()\n\ndef per_class_metric(true, pred, epoch, pose_list=pose_list, average=None):\n    \"\"\"\n    Computes and logs per-class metrics (Precision, Recall, F1-score).\n\n    Args:\n        true (list): Ground-truth labels.\n        pred (list): Predicted labels.\n        epoch (int): Current epoch number.\n        pose_list (list): List of class names (strings) corresponding to class indices.\n        average (str or None): Averaging method for sklearn metrics (None for per-class).\n    \"\"\"\n    per_class_precision = precision_score(true, pred, average=average, zero_division=0)\n    per_class_recall = recall_score(true, pred, average=average, zero_division=0)\n    per_class_f1 = f1_score(true, pred, average=average, zero_division=0)\n\n    print(f\"Per-Class Metrics for Epoch {epoch}:\")\n    for i, (prec, rec, f1) in enumerate(zip(per_class_precision, per_class_recall, per_class_f1)):\n        class_name = pose_list[i] if pose_list else f\"Class {i}\"\n        print(f\"{class_name}: Precision={prec:.2f}, Recall={rec:.2f}, F1-Score={f1:.2f}\")\n\n    plot_per_class_metric(per_class_precision, per_class_recall, per_class_f1, epoch, pose_list)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:04.228765Z","iopub.execute_input":"2025-02-25T17:04:04.229020Z","iopub.status.idle":"2025-02-25T17:04:04.923529Z","shell.execute_reply.started":"2025-02-25T17:04:04.228999Z","shell.execute_reply":"2025-02-25T17:04:04.922697Z"},"id":"g8OLd0ZqOHVS","trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.optim import lr_scheduler\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tqdm import tqdm\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, dataset,fold, num_epochs=50, patience=18, log_interval=10, checkpoint_path=None, unfreeze_epoch=5, num_layers_unfreeze=3):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    print(\"Using device:\", device)\n\n    # Initialize scheduler\n    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n    # Initialize early stopping\n    early_stopping = EarlyStopping(patience=patience, min_delta=1e-4)\n\n    start_epoch = 0\n    best_val_loss = float('inf')\n\n    # Initialize history for loss, accuracy, precision, recall, and F1\n    history = {\n        'train_loss': [], 'val_loss': [],\n        'train_acc': [], 'val_acc': [],\n        'train_precision': [], 'train_recall': [], 'train_f1': [],\n        'val_precision': [], 'val_recall': [], 'val_f1': [],\n        'learning_rates': []\n    }\n\n    # Check for checkpoint and load if available\n    if checkpoint_path and os.path.exists(checkpoint_path):\n        model, optimizer, start_epoch, history = load_checkpoint(model, optimizer, checkpoint_path)\n        print(f\"Resuming training from epoch {start_epoch}\")\n        torch.cuda.empty_cache()\n\n    for epoch in range(start_epoch, num_epochs):\n        if epoch == 14:\n            model.unfreeze_cnn_layers(num_layers=3)\n        dataset.use_augmentation = True #############\n        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n\n        current_lr = optimizer.param_groups[0]['lr']\n        history['learning_rates'].append(current_lr)\n        print(f\"Current Learning Rate: {current_lr}\")\n\n        # Training phase\n        model.train()\n        train_loss, train_correct, train_total = 0.0, 0, 0\n        train_true, train_pred = [], []\n\n        train_loader_tqdm = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n        for batch_idx, (inputs, labels, lengths) in train_loader_tqdm:\n            inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs, lengths)\n            loss = criterion(outputs, labels)\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            train_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            train_correct += (predicted == labels).sum().item()\n            train_total += labels.size(0)\n\n            # Collect true and predicted labels for precision/recall\n            train_true.extend(labels.cpu().numpy())\n            train_pred.extend(predicted.cpu().numpy())\n\n            # Log batch-level updates\n            if batch_idx % log_interval == 0:\n                train_loader_tqdm.set_postfix({\n                    'loss': train_loss / (BATCH_SIZE * (batch_idx + 1)),\n                    'accuracy': 100.0 * train_correct / train_total\n                })\n\n        # Calculate training metrics\n        train_loss /= len(train_loader.dataset)\n        train_acc = 100.0 * train_correct / train_total\n        train_precision = precision_score(train_true, train_pred, average='macro')\n        train_recall = recall_score(train_true, train_pred, average='macro')\n        train_f1 = f1_score(train_true, train_pred, average='macro')\n\n        per_class_metric(train_true, train_pred, epoch)\n\n        dataset.use_augmentation = False\n        # Validation phase\n        model.eval()\n        val_loss, val_correct, val_total = 0.0, 0, 0\n        val_true, val_pred = [], []\n\n        val_loader_tqdm = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\")\n        with torch.no_grad():\n            for batch_idx, (inputs, labels, lengths) in val_loader_tqdm:\n                inputs, labels, lengths = inputs.to(device), labels.to(device), lengths.to(device)\n                outputs = model(inputs, lengths)  # Pass sequence lengths to model forward function\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item() * inputs.size(0)\n                _, predicted = torch.max(outputs, 1)\n                val_correct += (predicted == labels).sum().item()\n                val_total += labels.size(0)\n\n                # Collect true and predicted labels for precision/recall\n                val_true.extend(labels.cpu().numpy())\n                val_pred.extend(predicted.cpu().numpy())\n\n                # Log batch-level updates for validation\n                if batch_idx % log_interval == 0:\n                    val_loader_tqdm.set_postfix({\n                        'loss': val_loss / (BATCH_SIZE * (batch_idx + 1)),\n                        'accuracy': 100.0 * val_correct / val_total\n                    })\n\n        # Calculate validation metrics\n        val_loss /= len(val_loader.dataset)\n        val_acc = 100.0 * val_correct / val_total\n        val_precision = precision_score(val_true, val_pred, average='macro')\n        val_recall = recall_score(val_true, val_pred, average='macro')\n        val_f1 = f1_score(val_true, val_pred, average='macro')\n        # Per-class metrics for validation\n        per_class_metric(val_true, val_pred, epoch)\n        # Update history\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n        history['train_precision'].append(train_precision)\n        history['train_recall'].append(train_recall)\n        history['train_f1'].append(train_f1)\n        history['val_precision'].append(val_precision)\n        history['val_recall'].append(val_recall)\n        history['val_f1'].append(val_f1)\n\n        # Print metrics at the end of the epoch\n        print(f'\\nEpoch {epoch + 1}/{num_epochs} Summary:')\n        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Train Precision: {train_precision:.2f} | Train Recall: {train_recall:.2f} | Train F1: {train_f1:.2f}')\n        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Val Precision: {val_precision:.2f} | Val Recall: {val_recall:.2f} | Val F1: {val_f1:.2f}')\n\n        # Save the best model checkpoint\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_path = os.path.join(SAVE_PATH, f'best_model_fold_{fold}.pth')\n            if best_model_path is not None:\n                print(best_model_path)\n            save_checkpoint(model, optimizer, epoch, history, SAVE_PATH, best_model_path)\n            print(f\"New best model saved! Validation Loss: {best_val_loss:.4f}\")\n\n        # Adjust learning rate based on validation loss\n        scheduler.step(val_loss)\n\n        # Check for early stopping\n        # early_stopping(val_loss)\n        # if early_stopping.early_stop:\n        #     print(\"Early stopping triggered\")\n        #     break\n\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:04.924595Z","iopub.execute_input":"2025-02-25T17:04:04.925078Z","iopub.status.idle":"2025-02-25T17:04:04.951899Z","shell.execute_reply.started":"2025-02-25T17:04:04.925048Z","shell.execute_reply":"2025-02-25T17:04:04.951041Z"},"id":"nNTAFuncCZC9","trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nimport seaborn as sns\nimport torch\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\ndef evaluate_model(model, test_loader, criterion, class_names, spath=SAVE_PATH, fsave='confusion_matrix_accuracy.png', fold = None):\n    \"\"\"\n    Evaluate model on test set\n\n    Args:\n        model: PyTorch model\n        test_loader: DataLoader for test data\n        criterion: Loss function\n        class_names: List of class names\n        spath: Directory to save the plot\n        fsave: Filename for confusion matrix plot\n\n    Returns:\n        test_loss: Average test loss\n        accuracy: Test accuracy\n        precision: Macro precision score\n        recall: Macro recall score\n        f1: Macro F1 score\n    \"\"\"\n    ffsave = f'fold_{fold}_{fsave}'\n    csave = os.path.join(spath, ffsave)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    all_predictions = []\n    all_labels = []\n\n    test_loader_tqdm = tqdm(test_loader, desc=\"Testing\")\n\n    with torch.no_grad():\n        for inputs, labels, lengths in test_loader_tqdm:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            lengths = lengths.to(device)\n            outputs = model(inputs, lengths)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item() * inputs.size(0)\n\n            _, predicted = torch.max(outputs, 1)\n            test_correct += (predicted == labels).sum().item()\n            test_total += labels.size(0)\n\n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    test_loss = test_loss / len(test_loader.dataset)\n    accuracy = 100 * test_correct / test_total\n\n    precision = precision_score(all_labels, all_predictions, average='weighted')\n    recall = recall_score(all_labels, all_predictions, average='weighted')\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n    \n    plt.figure(figsize=(12, 10))  # Increased figure size\n    plt.subplot(111, position=[0.1, 0.2, 0.8, 0.7])  # Adjust main plot position to leave room for text\n    cm = confusion_matrix(all_labels, all_predictions)\n    \n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    \n    # Prepare metrics text\n    metrics_text = (f'Test Loss: {test_loss:.4f}\\n'\n                    f'Test Accuracy: {accuracy:.2f}%\\n'\n                    f'Precision (Weighted): {precision:.4f}\\n'\n                    f'Recall (Weighted): {recall:.4f}\\n'\n                    f'F1 Score (Weighted): {f1:.4f}')\n    \n    # Add metrics text to the figure\n    plt.gcf().text(0, 0.01, metrics_text, fontsize=12, ha='left', va='bottom')\n    \n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.2)  # Leave space at bottom for metrics\n    plt.savefig(csave, bbox_inches='tight')\n    plt.show()\n    plt.close()\n    \n    print(f'Test Loss: {test_loss:.4f}')\n    print(f'Test Accuracy: {accuracy:.2f}%')\n    print(f'Precision (Weighted): {precision:.4f}')\n    print(f'Recall (Weighted): {recall:.4f}')\n    print(f'F1 Score (Weighted): {f1:.4f}')\n    \n    # Store metrics in a dictionary\n    metrics = {\n        'loss': test_loss,       # Test loss value\n        'accuracy': accuracy,    # Test accuracy percentage\n        'precision': precision,  # Weighted precision\n        'recall': recall,        # Weighted recall\n        'f1': f1                 # Weighted F1 score\n    }\n    \n    return metrics\n\n\n    # Calculate metrics with macro averaging\n    # precision = precision_score(all_labels, all_predictions, average='macro')\n    # recall = recall_score(all_labels, all_predictions, average='macro')\n    # f1 = f1_score(all_labels, all_predictions, average='macro')\n    \n    # plt.figure(figsize=(12, 10))  # Increased figure size\n    # plt.subplot(111, position=[0.1, 0.2, 0.8, 0.7])  # Adjust main plot position to leave room for text\n    # cm = confusion_matrix(all_labels, all_predictions)\n\n    # # plt.figure(figsize=(10, 8))\n    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n    #             xticklabels=class_names, yticklabels=class_names)\n    # plt.title('Confusion Matrix')\n    # plt.xlabel('Predicted')\n    # plt.ylabel('True')\n\n    # # Prepare metrics text\n    # metrics_text = (f'Test Loss: {test_loss:.4f}\\n'\n    #                 f'Test Accuracy: {accuracy:.2f}%\\n'\n    #                 f'Precision (Macro): {precision:.4f}\\n'\n    #                 f'Recall (Macro): {recall:.4f}\\n'\n    #                 f'F1 Score (Macro): {f1:.4f}')\n\n    # # Add metrics text to the figure\n    # plt.gcf().text(0, 0.01, metrics_text, fontsize=12, ha='left', va='bottom')\n\n    # plt.tight_layout()\n    # plt.subplots_adjust(bottom=0.2)  # Leave space at bottom for metrics\n    # # Save with tight layout to include all elements\n    # plt.savefig(csave, bbox_inches='tight')\n    # plt.show()\n    # plt.close()\n\n    # print(f'Test Loss: {test_loss:.4f}')\n    # print(f'Test Accuracy: {accuracy:.2f}%')\n    # print(f'Precision (Macro): {precision:.4f}')\n    # print(f'Recall (Macro): {recall:.4f}')\n    # print(f'F1 Score (Macro): {f1:.4f}')\n\n    # # return test_loss, accuracy, precision, recall, f1\n    # metrics = {\n    # 'loss': test_loss,       # Test loss value\n    # 'accuracy': accuracy,    # Test accuracy percentage\n    # 'precision': precision,  # Macro-averaged precision\n    # 'recall': recall,        # Macro-averaged recall\n    # 'f1': f1                 # Macro-averaged F1 score\n    # }\n\n    # # return test_loss, accuracy, precision, recall, f1\n    # return metrics","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:04.952829Z","iopub.execute_input":"2025-02-25T17:04:04.953081Z","iopub.status.idle":"2025-02-25T17:04:05.319572Z","shell.execute_reply.started":"2025-02-25T17:04:04.953051Z","shell.execute_reply":"2025-02-25T17:04:05.318708Z"},"id":"8MayBbiF0gtk","trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\n\ndef evaluate_model_normalized(model, test_loader, criterion, class_names, writer=None, global_step=0, spath=SAVE_PATH, fsave='confusion_matrix_normalized.png', fold=None):\n    \"\"\"\n    Evaluate model on test set and log results to TensorBoard.\n\n    Args:\n        model: PyTorch model\n        test_loader: DataLoader for test data\n        criterion: Loss function\n        class_names: List of class names\n        writer: TensorBoard SummaryWriter (optional)\n        global_step: Step for logging test metrics (e.g., epoch number)\n        spath: Directory to save the plot\n        fsave: Filename for confusion matrix plot\n\n    Returns:\n        test_loss: Average test loss\n        accuracy: Test accuracy\n        precision: Macro precision score\n        recall: Macro recall score\n        f1: Macro F1 score\n    \"\"\"\n    # csave = os.path.join(spath, fsave)\n    ffsave = f'fold_{fold}_{fsave}'\n    # csave = os.path.join(spath, fsave)\n    csave = os.path.join(spath, ffsave)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    all_predictions = []\n    all_labels = []\n\n    test_loader_tqdm = tqdm(test_loader, desc=\"Testing\")\n\n    with torch.no_grad():\n        for inputs, labels, lengths in test_loader_tqdm:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            lengths = lengths.to(device)\n            outputs = model(inputs, lengths)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item() * inputs.size(0)\n\n            _, predicted = torch.max(outputs, 1)\n            test_correct += (predicted == labels).sum().item()\n            test_total += labels.size(0)\n\n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    test_loss = test_loss / len(test_loader.dataset)\n    accuracy = 100 * test_correct / test_total\n\n    # Calculate metrics with macro averaging\n    precision = precision_score(all_labels, all_predictions, average='macro')\n    recall = recall_score(all_labels, all_predictions, average='macro')\n    f1 = f1_score(all_labels, all_predictions, average='macro')\n\n    plt.figure(figsize=(12, 10))  # Increased figure size\n    plt.subplot(111, position=[0.1, 0.2, 0.8, 0.7])  # Adjust main plot position to leave room for text\n\n    # Calculate confusion matrix and normalize it\n    cm = confusion_matrix(all_labels, all_predictions)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    # Replace NaN values (from division by zero) with 0\n    cm_normalized = np.nan_to_num(cm_normalized)\n\n    # plt.figure(figsize=(10, 8))\n    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Normalized Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n\n    # Prepare metrics text\n    metrics_text = (f'Test Loss: {test_loss:.4f}\\n'\n                    f'Test Accuracy: {accuracy:.2f}%\\n'\n                    f'Precision (Macro): {precision:.4f}\\n'\n                    f'Recall (Macro): {recall:.4f}\\n'\n                    f'F1 Score (Macro): {f1:.4f}')\n\n    # Add metrics text to the figure\n    plt.gcf().text(0, 0, metrics_text, fontsize=12, ha='left', va='bottom')\n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.2)  # Leave space at bottom for metrics\n\n    # Save with tight layout to include all elements\n    plt.savefig(csave, bbox_inches='tight')\n    plt.show()\n    plt.close()\n\n    print(f'Test Loss: {test_loss:.4f}')\n    print(f'Test Accuracy: {accuracy:.2f}%')\n    print(f'Precision (Macro): {precision:.4f}')\n    print(f'Recall (Macro): {recall:.4f}')\n    print(f'F1 Score (Macro): {f1:.4f}')\n\n    metrics = {\n    'loss': test_loss,       # Test loss value\n    'accuracy': accuracy,    # Test accuracy percentage\n    'precision': precision,  # Macro-averaged precision\n    'recall': recall,        # Macro-averaged recall\n    'f1': f1                 # Macro-averaged F1 score\n    }\n\n    \n    return metrics","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:05.320436Z","iopub.execute_input":"2025-02-25T17:04:05.320794Z","iopub.status.idle":"2025-02-25T17:04:17.809353Z","shell.execute_reply.started":"2025-02-25T17:04:05.320764Z","shell.execute_reply":"2025-02-25T17:04:17.808578Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    frames, labels = zip(*batch)  # Separate frames and labels\n    # FRAMES == DATA same thing shape (N, T, C, H, W)\n    # Pad the sequences of frames for each video in the batch along the sequence dimension\n    frames_padded = pad_sequence(frames, batch_first=True, padding_value=0)  # Shape: [batch_size, max_seq_len, 3, 224, 224]\n    lengths = torch.tensor([len(seq) for seq in frames])  # Record original sequence lengths\n\n    labels = torch.tensor(labels)\n\n    return frames_padded, labels, lengths  # Return lengths for packing","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:17.810169Z","iopub.execute_input":"2025-02-25T17:04:17.810691Z","iopub.status.idle":"2025-02-25T17:04:17.815487Z","shell.execute_reply.started":"2025-02-25T17:04:17.810667Z","shell.execute_reply":"2025-02-25T17:04:17.814506Z"},"id":"YagSnOqwmGn9","trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from torch.utils.data import random_split, DataLoader\n\ndef train_test_split(dataset, test_split = TEST_SPLIT):\n    total_size = len(dataset)\n    test_size = int(test_split * total_size)\n    train_size = total_size - test_size\n\n    train_dataset, test_dataset = random_split(\n        dataset,\n        [train_size, test_size],\n        generator=torch.Generator().manual_seed(42)  # For reproducibility\n    )\n\n    return train_dataset, test_dataset\n\ndef create_data_loaders(train_dataset, test_dataset, batch_size = BATCH_SIZE):\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True,\n        collate_fn=collate_fn\n    )\n\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        # num_workers=,\n        pin_memory=True,\n        collate_fn=collate_fn\n    )\n\n    return train_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:17.816357Z","iopub.execute_input":"2025-02-25T17:04:17.816641Z","iopub.status.idle":"2025-02-25T17:04:17.862924Z","shell.execute_reply.started":"2025-02-25T17:04:17.816605Z","shell.execute_reply":"2025-02-25T17:04:17.862018Z"},"id":"7EWkYX8sCZC-","trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"\ndef plot_training_curves(history, fold):\n    fsave=f'training_curves_fold_{fold}.png'\n    tsave = os.path.join(SAVE_PATH, fsave)\n    # plt.style.use('seaborn')\n    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n\n    # Loss curves\n    axs[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n    axs[0, 0].plot(history['val_loss'], label='Validation Loss', marker='o')\n    axs[0, 0].set_title('Loss')\n    axs[0, 0].legend()\n\n    # Accuracy curves\n    axs[0, 1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n    axs[0, 1].plot(history['val_acc'], label='Validation Accuracy', marker='o')\n    axs[0, 1].set_title('Accuracy')\n    axs[0, 1].legend()\n\n    # Learning rate\n    axs[0, 2].plot(history['learning_rates'], label='Learning Rate', marker='o')\n    axs[0, 2].set_title('Learning Rate')\n    axs[0, 2].set_yscale('log')\n    axs[0, 2].legend()\n\n    # Precision\n    axs[1, 0].plot(history['train_precision'], label='Train Precision', marker='o')\n    axs[1, 0].plot(history['val_precision'], label='Validation Precision', marker='o')\n    axs[1, 0].set_title('Precision')\n    axs[1, 0].legend()\n\n    # Recall\n    axs[1, 1].plot(history['train_recall'], label='Train Recall', marker='o')\n    axs[1, 1].plot(history['val_recall'], label='Validation Recall', marker='o')\n    axs[1, 1].set_title('Recall')\n    axs[1, 1].legend()\n\n    # F1 Score\n    axs[1, 2].plot(history['train_f1'], label='Train F1', marker='o')\n    axs[1, 2].plot(history['val_f1'], label='Validation F1', marker='o')\n    axs[1, 2].set_title('F1 Score')\n    axs[1, 2].legend()\n\n    for ax in axs.flat:\n        ax.set_xlabel('Epoch')\n        ax.grid(True)\n\n    plt.tight_layout()\n    plt.savefig(tsave)\n    plt.show()\n    plt.close()\n","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:17.863782Z","iopub.execute_input":"2025-02-25T17:04:17.864066Z","iopub.status.idle":"2025-02-25T17:04:17.875493Z","shell.execute_reply.started":"2025-02-25T17:04:17.864039Z","shell.execute_reply":"2025-02-25T17:04:17.874557Z"},"id":"-nVQoc_DCZC-","trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom collections import Counter\nfrom sklearn.utils.class_weight import compute_class_weight\n\ndef get_class_weights(pose_list, num_classes, strategy='balanced'):\n    \"\"\"\n    Calculate class weights with proper class index alignment.\n\n    Args:\n        pose_list: List of labels/poses in the dataset\n        num_classes: Total number of classes\n        strategy: Weighting strategy ('balanced', 'inverse', 'effective_samples', 'sqrt_inverse')\n\n    Returns:\n        torch.Tensor: Class weights tensor aligned with class indices\n    \"\"\"\n    # Count samples per class\n    class_counts = Counter(pose_list)\n    total_samples = len(pose_list)\n\n    # Initialize weights array with zeros for all possible classes\n    weights = np.zeros(num_classes)\n\n    if strategy == 'balanced':\n        # Use sklearn's balanced weighting\n        unique_classes = sorted(class_counts.keys())\n        sklearn_weights = compute_class_weight(\n            class_weight='balanced',\n            classes=np.array(unique_classes),\n            y=pose_list\n        )\n        # Map weights to correct indices\n        for idx, class_label in enumerate(unique_classes):\n            weights[class_label] = sklearn_weights[idx]\n\n    elif strategy == 'inverse':\n        # Inverse of sample frequency\n        for class_label, count in class_counts.items():\n            weights[class_label] = total_samples / (num_classes * count)\n\n    elif strategy == 'effective_samples':\n        # Effective number of samples weighting\n        beta = 0.9999\n        for class_label, count in class_counts.items():\n            weights[class_label] = (1 - beta) / (1 - beta ** count)\n\n    elif strategy == 'sqrt_inverse':\n        # Square root of inverse frequency\n        for class_label, count in class_counts.items():\n            weights[class_label] = np.sqrt(total_samples / (num_classes * count))\n\n    else:\n        raise ValueError(f\"Unknown weighting strategy: {strategy}\")\n\n    # Convert to tensor and normalize\n    weights = torch.FloatTensor(weights)\n    weights = weights / weights.sum() * len(weights)\n\n    return weights\n\ndef create_weighted_criterion(pose_list, num_classes, strategy='balanced'):\n    \"\"\"\n    Create a weighted CrossEntropyLoss criterion.\n\n    Args:\n        pose_list: List of labels/poses in the dataset\n        num_classes: Total number of classes\n        strategy: Weighting strategy for calculating class weights\n\n    Returns:\n        nn.CrossEntropyLoss: Weighted loss criterion\n    \"\"\"\n    weights = get_class_weights(pose_list, num_classes, strategy)\n    print(\"Class weights aligned with indices:\", weights)\n    if torch.cuda.is_available():\n        weights = weights.cuda()\n    return nn.CrossEntropyLoss(weight=weights)\n\ndef analyze_class_distribution(pose_list):\n    \"\"\"\n    Analyze and print class distribution information.\n\n    Args:\n        pose_list: List of labels/poses in the dataset\n    \"\"\"\n    class_counts = Counter(pose_list)\n    total_samples = len(pose_list)\n\n    print(\"\\nClass Distribution Analysis:\")\n    print(\"-\" * 50)\n    for class_idx, count in sorted(class_counts.items()):\n        percentage = (count / total_samples) * 100\n        print(f\"Class {class_idx}: {count} samples ({percentage:.2f}%)\")\n\n    # Calculate imbalance metrics\n    max_count = max(class_counts.values())\n    min_count = min(class_counts.values())\n    imbalance_ratio = max_count / min_count\n\n    print(\"\\nImbalance Statistics:\")\n    print(f\"Imbalance Ratio (max/min): {imbalance_ratio:.2f}\")\n    print(f\"Maximum class size: {max_count}\")\n    print(f\"Minimum class size: {min_count}\")\n    print(f\"Average class size: {total_samples/len(class_counts):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:17.876269Z","iopub.execute_input":"2025-02-25T17:04:17.876478Z","iopub.status.idle":"2025-02-25T17:04:17.890150Z","shell.execute_reply.started":"2025-02-25T17:04:17.876460Z","shell.execute_reply":"2025-02-25T17:04:17.889319Z"},"id":"7LXXcX28u5Ar","trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"from torch.utils.data import Subset\n\ndef create_data_loaders(train_idx, val_idx, dataset, batch_size=BATCH_SIZE):\n    # Create subsets for this fold\n    train_subset = Subset(dataset, train_idx)\n    val_subset = Subset(dataset, val_idx)\n\n    # Create data loaders with optional collate_fn\n    train_loader = DataLoader(\n        train_subset,\n        batch_size=batch_size,\n        shuffle=True,\n        pin_memory=True,\n        collate_fn=collate_fn\n    )\n    val_loader = DataLoader(\n        val_subset,\n        batch_size=batch_size,\n        shuffle=False,\n        pin_memory=True,\n        collate_fn=collate_fn\n    )\n\n    return train_loader, val_loader\n","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:17.890904Z","iopub.execute_input":"2025-02-25T17:04:17.891144Z","iopub.status.idle":"2025-02-25T17:04:17.907450Z","shell.execute_reply.started":"2025-02-25T17:04:17.891124Z","shell.execute_reply":"2025-02-25T17:04:17.906723Z"},"id":"fqt2Ho_GOHVT","trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def compute_average_history(histories):\n    \"\"\"\n    Compute the average history across multiple folds.\n\n    Args:\n        histories (list of dict): List of history dictionaries from all folds. Each dictionary contains\n                                  metrics like 'train_loss', 'val_loss', 'train_accuracy', etc., as lists.\n\n    Returns:\n        dict: Averaged history containing the same keys as the input histories.\n    \"\"\"\n    avg_history = {}\n    num_folds = len(histories)\n\n    for key in histories[0]:  # Iterate over metric names\n        # Initialize a list for each metric\n        avg_history[key] = [0.0] * len(histories[0][key])  # Assume all folds have same length histories\n\n        # Sum across all folds\n        for fold_history in histories:\n            for i, value in enumerate(fold_history[key]):\n                avg_history[key][i] += value\n\n        # Divide by the number of folds to compute the average\n        avg_history[key] = [val / num_folds for val in avg_history[key]]\n\n    return avg_history","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:17.908351Z","iopub.execute_input":"2025-02-25T17:04:17.908679Z","iopub.status.idle":"2025-02-25T17:04:17.926008Z","shell.execute_reply.started":"2025-02-25T17:04:17.908631Z","shell.execute_reply":"2025-02-25T17:04:17.925176Z"},"id":"RcWrvy4ZNiYj","trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import os\nimport pickle\nfrom sklearn.model_selection import StratifiedKFold\n\ndef save_folds(meta_info_path, skf, folds, architecture=ARCHITECTURE):\n    if architecture == '2D_CNN_LSTM':\n        folds_path = os.path.join(meta_info_path, f\"{architecture}_folds.pkl\")\n        skf_path = os.path.join(meta_info_path, f\"{architecture}_skf.pkl\")\n    else:\n        folds_path = os.path.join(meta_info_path, \"folds.pkl\")\n        skf_path = os.path.join(meta_info_path, \"skf.pkl\")\n\n    with open(folds_path, \"wb\") as f:\n        pickle.dump(folds, f)\n\n    with open(skf_path, \"wb\") as f:\n        pickle.dump(skf, f)\n\ndef load_folds(meta_info_path, architecture=ARCHITECTURE):\n    if architecture == '2D_CNN_LSTM':\n        folds_path = os.path.join(meta_info_path, f\"{architecture}_folds.pkl\")\n        skf_path = os.path.join(meta_info_path, f\"{architecture}_skf.pkl\")\n    else:\n        folds_path = os.path.join(meta_info_path, \"folds.pkl\")\n        skf_path = os.path.join(meta_info_path, \"skf.pkl\")\n\n    if os.path.exists(folds_path) and os.path.exists(skf_path):\n        with open(folds_path, \"rb\") as f:\n            folds = pickle.load(f)\n        with open(skf_path, \"rb\") as f:\n            skf = pickle.load(f)\n        return skf, folds\n    return None, None","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:17.926891Z","iopub.execute_input":"2025-02-25T17:04:17.927175Z","iopub.status.idle":"2025-02-25T17:04:17.952041Z","shell.execute_reply.started":"2025-02-25T17:04:17.927147Z","shell.execute_reply":"2025-02-25T17:04:17.951427Z"},"id":"gX9YM1zJNj3A","trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import torch\nimport os\nimport json\nimport pickle\n\ndef get_weighted_loss_criterion(train_dataset, train_indices=None, meta_info_path=None, fold=None):\n    \"\"\"\n    Get or compute weighted loss criterion, with caching support.\n\n    Args:\n        train_dataset: The training dataset\n        train_indices: Optional indices for cross-validation fold\n        meta_info_path: Path to save/load cached criteria\n        fold: Current fold number (required if using caching)\n\n    Returns:\n        torch.nn.CrossEntropyLoss with computed weights\n    \"\"\"\n    if meta_info_path and fold is not None:\n        criterion_cache_path = os.path.join(meta_info_path, f'criterion_fold_{fold}.pkl')\n\n        # Try to load cached criterion\n        if os.path.exists(criterion_cache_path):\n            try:\n                with open(criterion_cache_path, 'rb') as f:\n                    cached_data = pickle.load(f)\n\n                # Verify the cached criterion matches current data\n                if verify_criterion_cache(cached_data, train_indices):\n                    print(f\"Loading cached criterion for fold {fold}\")\n                    return cached_data['criterion']\n                else:\n                    print(f\"Cached criterion for fold {fold} is invalid, recomputing...\")\n            except Exception as e:\n                print(f\"Error loading cached criterion: {e}, recomputing...\")\n\n    # Compute criterion if no cache exists or verification failed\n    if train_indices is not None:\n        labels = [train_dataset[i][1] for i in train_indices]\n    else:\n        labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n\n    analyze_class_distribution(labels)\n    criterion = create_weighted_criterion(\n        labels,\n        num_classes=NUM_CLASSES,\n        strategy='effective_samples'\n    )\n\n    # Cache the computed criterion if path is provided\n    if meta_info_path and fold is not None:\n        os.makedirs(meta_info_path, exist_ok=True)\n        cache_data = {\n            'criterion': criterion,\n            'train_indices': train_indices,\n            'fold': fold\n        }\n        with open(criterion_cache_path, 'wb') as f:\n            pickle.dump(cache_data, f)\n        print(f\"Cached criterion for fold {fold}\")\n\n    return criterion\n\ndef verify_criterion_cache(cached_data, current_train_indices):\n    \"\"\"\n    Verify that cached criterion matches current training indices.\n\n    Args:\n        cached_data: Dictionary containing cached criterion and metadata\n        current_train_indices: Current training indices to verify against\n\n    Returns:\n        bool: True if cache is valid, False otherwise\n    \"\"\"\n    cached_indices = cached_data['train_indices']\n    if cached_indices is None and current_train_indices is None:\n        return True\n    if cached_indices is None or current_train_indices is None:\n        return False\n    return len(cached_indices) == len(current_train_indices) and all(\n        a == b for a, b in zip(sorted(cached_indices), sorted(current_train_indices))\n    )","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:17.952819Z","iopub.execute_input":"2025-02-25T17:04:17.953017Z","iopub.status.idle":"2025-02-25T17:04:17.960748Z","shell.execute_reply.started":"2025-02-25T17:04:17.953000Z","shell.execute_reply":"2025-02-25T17:04:17.960064Z"},"id":"Vn5XpTqG6_6J","trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.optim import lr_scheduler\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tqdm import tqdm\n\ndef train_model_final(model, train_loader, criterion, optimizer, dataset, fold, num_epochs, log_interval=10, writer=None, save_path=None):\n    \"\"\"\n    Train the final model on the full dataset without a validation set.\n    Logs training metrics to TensorBoard and saves the model.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    print(\"Using device:\", device)\n\n    # Initialize scheduler (optional, based on training loss since no validation)\n    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n\n    # Initialize history for training metrics only\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'train_precision': [],\n        'train_recall': [],\n        'train_f1': [],\n        'learning_rates': []\n    }\n\n    for epoch in range(num_epochs):\n        dataset.use_augmentation = True  # Enable augmentation\n        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n\n        current_lr = optimizer.param_groups[0]['lr']\n        history['learning_rates'].append(current_lr)\n        print(f\"Current Learning Rate: {current_lr}\")\n\n        # Training phase\n        model.train()\n        train_loss, train_correct, train_total = 0.0, 0, 0\n        train_true, train_pred = [], []\n\n        train_loader_tqdm = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n        for batch_idx, (inputs, labels, lengths) in train_loader_tqdm:\n            inputs, labels = inputs.to(device), labels.to(device)\n            lengths = lengths.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs, lengths)\n            loss = criterion(outputs, labels)\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            train_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            train_correct += (predicted == labels).sum().item()\n            train_total += labels.size(0)\n\n            # Collect true and predicted labels for precision/recall\n            train_true.extend(labels.cpu().numpy())\n            train_pred.extend(predicted.cpu().numpy())\n\n            if batch_idx % log_interval == 0:\n                train_loader_tqdm.set_postfix({\n                    'loss': train_loss / (BATCH_SIZE * (batch_idx + 1)),\n                    'accuracy': 100.0 * train_correct / train_total\n                })\n\n        # Calculate training metrics\n        train_loss /= len(train_loader.dataset)\n        train_acc = 100.0 * train_correct / train_total\n        train_precision = precision_score(train_true, train_pred, average='macro')\n        train_recall = recall_score(train_true, train_pred, average='macro')\n        train_f1 = f1_score(train_true, train_pred, average='macro')\n\n        # Store metrics in history\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['train_precision'].append(train_precision)\n        history['train_recall'].append(train_recall)\n        history['train_f1'].append(train_f1)\n\n        # Log metrics to TensorBoard if writer is provided\n        if writer:\n            writer.add_scalar(f'Final/train_loss', train_loss, epoch)\n            writer.add_scalar(f'Final/train_acc', train_acc, epoch)\n            writer.add_scalar(f'Final/train_precision', train_precision, epoch)\n            writer.add_scalar(f'Final/train_recall', train_recall, epoch)\n            writer.add_scalar(f'Final/train_f1', train_f1, epoch)\n            writer.add_scalar(f'Final/learning_rate', current_lr, epoch)\n\n        # Print metrics\n        print(f'\\nEpoch {epoch + 1}/{num_epochs} Summary:')\n        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Train Precision: {train_precision:.2f} | Train Recall: {train_recall:.2f} | Train F1: {train_f1:.2f}')\n\n        # Adjust learning rate based on training loss (since no validation)\n        scheduler.step(train_loss)\n\n    # Save the final model\n    if save_path:\n        model_save_path = os.path.join(save_path, f'final_model_fold_{fold}.pth')\n        torch.save(model.state_dict(), model_save_path)\n        print(f\"Final model saved at {model_save_path}\")\n\n    return model, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:04:17.961550Z","iopub.execute_input":"2025-02-25T17:04:17.961825Z","iopub.status.idle":"2025-02-25T17:04:17.978791Z","shell.execute_reply.started":"2025-02-25T17:04:17.961805Z","shell.execute_reply":"2025-02-25T17:04:17.978070Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# from sklearn.model_selection import StratifiedKFold\n# from torch.utils.tensorboard import SummaryWriter\n\n# def main(lstm_hidden_size= LSTM_HIDDEN_SIZE,lstm_layers = LSTM_LAYERS, num_classes=NUM_CLASSES,LR=LEARNING_RATE, Epochs=50):\n    \n#     # Create TensorBoard log directory\n#     tensorboard_dir = os.path.join(SAVE_PATH, 'tensorboard_logs')\n#     os.makedirs(tensorboard_dir, exist_ok=True)\n#     writer = SummaryWriter(log_dir=tensorboard_dir)\n\n#     spatial_aug_config = {\n#         'gaussian_blur': 0.5,\n#         'random_horizontal_flip':  0.5,\n#         'color_jitter': 0.5,\n#         'random_rotation': 0.5\n#     }\n\n#     # temporal_aug_config = {\n#     #     'temporal_crop': {'enabled': True, 'crop_size': 0.8},\n#     #     'temporal_mask': {'enabled': True, 'mask_size': 0.2, 'n_masks': 2}\n#     # }\n#     dataset, train_dataset, test_dataset = prepare_dataset(spatial_aug_config=spatial_aug_config, temporal_aug_config=None)\n#     skf, folds = load_folds(meta_info_path, architecture=ARCHITECTURE)\n\n#     if folds is None:\n#         k_folds = 5\n#         skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n#         folds = list(skf.split(np.arange(len(train_dataset)), get_all_the_labels(train_dataset)))\n#         save_folds(meta_info_path, skf, folds,architecture=ARCHITECTURE)\n\n#     best_model = None\n#     best_val_loss = float('inf')\n#     best_fold = 0\n#     all_metrics = []\n\n#     print('Starting cross-validation...')\n\n#     # Create checkpoint directory if it doesn't exist\n#     os.makedirs(FOLD_CHECKPOINT_PATH, exist_ok=True)\n\n#     for fold, (train_idx, val_idx) in enumerate(folds):\n#         # Check if checkpoint exists for this fold\n#         checkpoint_path = os.path.join(FOLD_CHECKPOINT_PATH, f'fold_{fold}_checkpoint.pth')\n#         if os.path.exists(checkpoint_path):\n#             print(f\"Loading checkpoint for fold {fold + 1}\")\n#             checkpoint = torch.load(checkpoint_path)\n#             model = checkpoint['model']\n#             optimizer = checkpoint['optimizer']\n#             history = checkpoint['history']\n#             all_metrics = checkpoint['all_metrics']\n#             best_model = checkpoint['best_model']\n#             best_val_loss = checkpoint['best_val_loss']\n#             best_fold = checkpoint['best_fold']\n#             continue\n\n#         print(f\"Fold {fold + 1}/{len(folds)}\")\n#         train_loader, val_loader = create_data_loaders(train_idx, val_idx, train_dataset)\n\n#         criterion = get_weighted_loss_criterion(\n#             train_dataset,\n#             train_idx,\n#             meta_info_path=meta_info_path,\n#             fold=fold\n#         ).to(device)\n#         model = CNNLSTM(num_classes=num_classes, lstm_hidden_size=lstm_hidden_size, lstm_layers=lstm_layers, dropout=DROPOUT,cnn_model=CNN_TYPE)\n#         print(model)\n#         print()\n#         optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.01)\n\n#         model, history = train_model(\n#             model,\n#             train_loader,\n#             val_loader,\n#             criterion,\n#             optimizer,\n#             dataset,\n#             fold=fold,\n#             num_epochs=Epochs,\n#             patience=10,\n#             log_interval=1,\n#             checkpoint_path=None\n#         )\n        \n#         # Log metrics to TensorBoard from history\n#         for epoch in range(len(history['train_loss'])):\n#             writer.add_scalar(f'Fold_{fold}/train_loss', history['train_loss'][epoch], epoch)\n#             writer.add_scalar(f'Fold_{fold}/val_loss', history['val_loss'][epoch], epoch)\n#             writer.add_scalar(f'Fold_{fold}/train_acc', history['train_acc'][epoch], epoch)\n#             writer.add_scalar(f'Fold_{fold}/val_acc', history['val_acc'][epoch], epoch)\n#             writer.add_scalar(f'Fold_{fold}/train_precision', history['train_precision'][epoch], epoch)\n#             writer.add_scalar(f'Fold_{fold}/train_recall', history['train_recall'][epoch], epoch)\n#             writer.add_scalar(f'Fold_{fold}/train_f1', history['train_f1'][epoch], epoch)\n#             writer.add_scalar(f'Fold_{fold}/val_precision', history['val_precision'][epoch], epoch)\n#             writer.add_scalar(f'Fold_{fold}/val_recall', history['val_recall'][epoch], epoch)\n#             writer.add_scalar(f'Fold_{fold}/val_f1', history['val_f1'][epoch], epoch)\n\n#         plot_training_curves(history, fold)\n#         all_metrics.append(history)\n\n#         if history['val_loss'][-1] < best_val_loss:\n#             best_val_loss = history['val_loss'][-1]\n#             best_model = model\n#             best_fold = fold\n\n#         # Save checkpoint after each successful fold\n#         checkpoint = {\n#             'model': model,\n#             'optimizer': optimizer,\n#             'history': history,\n#             'all_metrics': all_metrics,\n#             'best_model': best_model,\n#             'best_val_loss': best_val_loss,\n#             'best_fold': best_fold,\n#             'fold': fold\n#         }\n#         torch.save(checkpoint, checkpoint_path)\n#         print(f\"Saved checkpoint for fold {fold + 1}\")\n\n#     # Log average metrics\n#     avg_history = compute_average_history(all_metrics)\n#     for epoch in range(len(avg_history['train_loss'])):\n#         writer.add_scalar('Average/train_loss', avg_history['train_loss'][epoch], epoch)\n#         writer.add_scalar('Average/val_loss', avg_history['val_loss'][epoch], epoch)\n#         writer.add_scalar('Average/train_acc', avg_history['train_acc'][epoch], epoch)\n#         writer.add_scalar('Average/val_acc', avg_history['val_acc'][epoch], epoch)\n#         writer.add_scalar('Average/train_precision', avg_history['train_precision'][epoch], epoch)\n#         writer.add_scalar('Average/train_recall', avg_history['train_recall'][epoch], epoch)\n#         writer.add_scalar('Average/train_f1', avg_history['train_f1'][epoch], epoch)\n#         writer.add_scalar('Average/val_precision', avg_history['val_precision'][epoch], epoch)\n#         writer.add_scalar('Average/val_recall', avg_history['val_recall'][epoch], epoch)\n#         writer.add_scalar('Average/val_f1', avg_history['val_f1'][epoch], epoch)\n\n#     plot_training_curves(avg_history, 'average')\n\n#     test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn = collate_fn)\n#     # Directly use the best fold's training indices to create the correct criterion\n#     train_idx, _ = folds[best_fold]  # Extract train indices for best fold\n    \n#     criterion = get_weighted_loss_criterion(\n#         train_dataset,\n#         train_idx,\n#         meta_info_path=meta_info_path,\n#         fold=best_fold\n#     ).to(device)\n#     evaluate_model(best_model, test_loader, criterion, pose_list)\n#     evaluate_model_normalized(best_model, test_loader, criterion, pose_list, writer=writer, global_step=best_fold)\n\n#     model_save_path = os.path.join(SAVE_PATH, f'my_model_{best_fold}.pth')\n#     torch.save(best_model.state_dict(), model_save_path)\n    \n#     # Close the TensorBoard writer\n#     writer.close()\n\n# def prepare_dataset(spatial_aug_config=None, temporal_aug_config=None):\n#     dataset = YogaVideoDataset(csv_path, sequence_path, pose_list, video_dir, preprocessed_dir, spatial_aug_config=spatial_aug_config, temporal_aug_config=temporal_aug_config)\n#     print(len(dataset))\n#     train_dataset, test_dataset = train_test_split(dataset)\n#     return dataset,train_dataset,test_dataset\n\n# def get_all_the_labels(dataset):\n#     return [label for _, label in dataset]","metadata":{"execution":{"iopub.status.busy":"2025-02-25T17:04:17.979843Z","iopub.execute_input":"2025-02-25T17:04:17.980044Z","iopub.status.idle":"2025-02-25T17:04:17.996972Z","shell.execute_reply.started":"2025-02-25T17:04:17.980027Z","shell.execute_reply":"2025-02-25T17:04:17.996256Z"},"id":"_G8_YTgfNq3R","trusted":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\n\ndef plot_test_metrics(avg_test_loss, avg_test_acc, avg_test_precision, avg_test_recall, avg_test_f1, save_path=SAVE_PATH, filename='test_metrics_average.png'):\n    \"\"\"\n    Plot average test metrics as text and save to a PNG file.\n\n    Args:\n        avg_test_loss (float): Average test loss across folds\n        avg_test_acc (float): Average test accuracy across folds (percentage)\n        avg_test_precision (float): Average macro precision across folds\n        avg_test_recall (float): Average macro recall across folds\n        avg_test_f1 (float): Average macro F1 score across folds\n        save_path (str): Directory to save the plot\n        filename (str): Filename for the plot (default: 'test_metrics_average.png')\n    \"\"\"\n    # Create figure without axes\n    fig, ax = plt.subplots(figsize=(8, 4))  # Adjust size as needed\n    ax.axis('off')  # Hide axes\n\n    # Define the text to display\n    metrics_text = (\n        f'Average Test Metrics Across Folds:\\n\\n'\n        f'Test Loss: {avg_test_loss:.4f}\\n'\n        f'Test Accuracy: {avg_test_acc:.2f}%\\n'\n        f'Precision (Macro): {avg_test_precision:.4f}\\n'\n        f'Recall (Macro): {avg_test_recall:.4f}\\n'\n        f'F1 Score (Macro): {avg_test_f1:.4f}'\n    )\n\n    # Place text in the center of the figure\n    ax.text(0.5, 0.5, metrics_text, ha='center', va='center', fontsize=12, fontfamily='monospace')\n\n    # Construct the full save path\n    save_file = os.path.join(save_path, filename)\n\n    # Ensure the directory exists\n    os.makedirs(save_path, exist_ok=True)\n\n    # Save the plot\n    plt.savefig(save_file, bbox_inches='tight', dpi=300)\n    print(f\"Saved test metrics plot to {save_file}\")\n    plt.show()\n    # Close the figure to free memory\n    plt.close(fig)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:04:17.997703Z","iopub.execute_input":"2025-02-25T17:04:17.997922Z","iopub.status.idle":"2025-02-25T17:04:18.013414Z","shell.execute_reply.started":"2025-02-25T17:04:17.997892Z","shell.execute_reply":"2025-02-25T17:04:18.012549Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, train_test_split\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch\nimport os\nimport numpy as np\n\ndef main(lstm_hidden_size=LSTM_HIDDEN_SIZE, lstm_layers=LSTM_LAYERS, num_classes=NUM_CLASSES, LR=LEARNING_RATE, Epochs=50):\n    \"\"\"\n    Main function implementing the new action plan with 5-fold CV on the entire dataset,\n    per-fold validation splits, and a final model trained on all data.\n    \"\"\"\n    # Create TensorBoard log directory\n    tensorboard_dir = os.path.join(SAVE_PATH, 'tensorboard_logs')\n    os.makedirs(tensorboard_dir, exist_ok=True)\n    writer = SummaryWriter(log_dir=tensorboard_dir)\n\n    # Define augmentation configurations\n    spatial_aug_config = {\n        'gaussian_blur': 0.5,\n        'random_horizontal_flip': 0.5,\n        'color_jitter': 0.5,\n        'random_rotation': 0.5\n    }\n\n    # Load the entire dataset without initial train-test split\n    dataset = prepare_dataset(spatial_aug_config=spatial_aug_config, temporal_aug_config=None)\n    labels = get_all_the_labels(dataset)\n\n    # Set up stratified 5-fold cross-validation on the entire dataset\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    folds = list(skf.split(np.arange(len(dataset)), labels))\n\n    all_metrics = []  # Store training history for each fold\n    all_test_metrics = []  # Store test metrics for each fold\n\n    print('Starting cross-validation...')\n\n    # Ensure checkpoint directory exists\n    os.makedirs(FOLD_CHECKPOINT_PATH, exist_ok=True)\n\n    for fold, (train_idx, test_idx) in enumerate(folds):\n        checkpoint_path = os.path.join(FOLD_CHECKPOINT_PATH, f'fold_{fold}_checkpoint.pth')\n        if os.path.exists(checkpoint_path):\n            print(f\"Loading checkpoint for fold {fold + 1}\")\n            checkpoint = torch.load(checkpoint_path)\n            history = checkpoint['history']\n            test_metrics = checkpoint['test_metrics']\n            all_metrics.append(history)\n            all_test_metrics.append(test_metrics)\n            continue\n\n        print(f\"Fold {fold + 1}/{len(folds)}\")\n\n        # Further split train_idx into actual_train_idx and val_idx (90-10 split)\n        train_labels = [labels[i] for i in train_idx]\n        actual_train_idx, val_idx = train_test_split(\n            train_idx,\n            test_size=0.1,\n            stratify=train_labels,\n            random_state=42\n        )\n\n        # Create data loaders for actual training, validation, and test sets\n        train_sub_dataset = Subset(dataset, actual_train_idx)\n        val_sub_dataset = Subset(dataset, val_idx)\n        test_sub_dataset = Subset(dataset, test_idx)\n\n        train_loader = DataLoader(train_sub_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n        val_loader = DataLoader(val_sub_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n        test_loader = DataLoader(test_sub_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n        # Define loss criterion based on actual training indices\n        criterion = get_weighted_loss_criterion(\n            dataset,\n            actual_train_idx,\n            meta_info_path=meta_info_path,\n            fold=fold\n        ).to(device)\n\n        # Initialize the model\n        model = CNNLSTM(\n            num_classes=num_classes,\n            lstm_hidden_size=lstm_hidden_size,\n            lstm_layers=lstm_layers,\n            dropout=DROPOUT,\n            cnn_model=CNN_TYPE\n        ).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.01)\n\n        # Train the model\n        model, history = train_model(\n            model,\n            train_loader,\n            val_loader,\n            criterion,\n            optimizer,\n            dataset,\n            fold=fold,\n            num_epochs=Epochs,\n            patience=10,\n            log_interval=1,\n            checkpoint_path=None\n        )\n\n        # Log training and validation metrics to TensorBoard\n        for epoch in range(len(history['train_loss'])):\n            writer.add_scalar(f'Fold_{fold}/train_loss', history['train_loss'][epoch], epoch)\n            writer.add_scalar(f'Fold_{fold}/val_loss', history['val_loss'][epoch], epoch)\n            writer.add_scalar(f'Fold_{fold}/train_acc', history['train_acc'][epoch], epoch)\n            writer.add_scalar(f'Fold_{fold}/val_acc', history['val_acc'][epoch], epoch)\n            writer.add_scalar(f'Fold_{fold}/train_precision', history['train_precision'][epoch], epoch)\n            writer.add_scalar(f'Fold_{fold}/train_recall', history['train_recall'][epoch], epoch)\n            writer.add_scalar(f'Fold_{fold}/train_f1', history['train_f1'][epoch], epoch)\n            writer.add_scalar(f'Fold_{fold}/val_precision', history['val_precision'][epoch], epoch)\n            writer.add_scalar(f'Fold_{fold}/val_recall', history['val_recall'][epoch], epoch)\n            writer.add_scalar(f'Fold_{fold}/val_f1', history['val_f1'][epoch], epoch)\n\n        plot_training_curves(history, fold)\n        all_metrics.append(history)\n\n       # Evaluate on the fold's test set\n        evaluate_model(model, test_loader, criterion, pose_list, fold=fold)\n        test_metrics = evaluate_model_normalized(model, test_loader, criterion, pose_list, fold=fold)\n\n        all_test_metrics.append(test_metrics)\n\n        # Log test metrics to TensorBoard\n        writer.add_scalar(f'Fold_{fold}/test_loss', test_metrics['loss'], 0)\n        writer.add_scalar(f'Fold_{fold}/test_acc', test_metrics['accuracy'], 0)\n        writer.add_scalar(f'Fold_{fold}/test_precision', test_metrics['precision'], 0)\n        writer.add_scalar(f'Fold_{fold}/test_recall', test_metrics['recall'], 0)\n        writer.add_scalar(f'Fold_{fold}/test_f1', test_metrics['f1'], 0)\n\n\n        # Save checkpoint\n        checkpoint = {\n            'model': model,\n            'optimizer': optimizer,\n            'history': history,\n            'test_metrics': test_metrics,\n            'fold': fold\n        }\n        torch.save(checkpoint, checkpoint_path)\n        print(f\"Saved checkpoint for fold {fold + 1}\")\n\n    # Compute and log average metrics across folds\n    avg_history = compute_average_history(all_metrics)\n    for epoch in range(len(avg_history['train_loss'])):\n        writer.add_scalar('Average/train_loss', avg_history['train_loss'][epoch], epoch)\n        writer.add_scalar('Average/val_loss', avg_history['val_loss'][epoch], epoch)\n        writer.add_scalar('Average/train_acc', avg_history['train_acc'][epoch], epoch)\n        writer.add_scalar('Average/val_acc', avg_history['val_acc'][epoch], epoch)\n        writer.add_scalar('Average/train_precision', avg_history['train_precision'][epoch], epoch)\n        writer.add_scalar('Average/train_recall', avg_history['train_recall'][epoch], epoch)\n        writer.add_scalar('Average/train_f1', avg_history['train_f1'][epoch], epoch)\n        writer.add_scalar('Average/val_precision', avg_history['val_precision'][epoch], epoch)\n        writer.add_scalar('Average/val_recall', avg_history['val_recall'][epoch], epoch)\n        writer.add_scalar('Average/val_f1', avg_history['val_f1'][epoch], epoch)\n\n    plot_training_curves(avg_history, 'average')\n\n    # # Compute average test metrics\n    # avg_test_loss = np.mean([m['loss'] for m in all_test_metrics])\n    # avg_test_acc = np.mean([m['accuracy'] for m in all_test_metrics])\n    # writer.add_scalar('Average/test_loss', avg_test_loss, 0)\n    # writer.add_scalar('Average/test_acc', avg_test_acc, 0)\n    # # Add other average test metrics as needed\n    avg_test_loss = np.mean([m['loss'] for m in all_test_metrics])\n    avg_test_acc = np.mean([m['accuracy'] for m in all_test_metrics])\n    avg_test_precision = np.mean([m['precision'] for m in all_test_metrics])\n    avg_test_recall = np.mean([m['recall'] for m in all_test_metrics])\n    avg_test_f1 = np.mean([m['f1'] for m in all_test_metrics])\n\n    # Log to TensorBoard\n    writer.add_scalar('Average/test_loss', avg_test_loss, 0)\n    writer.add_scalar('Average/test_acc', avg_test_acc, 0)\n    writer.add_scalar('Average/test_precision', avg_test_precision, 0)\n    writer.add_scalar('Average/test_recall', avg_test_recall, 0)\n    writer.add_scalar('Average/test_f1', avg_test_f1, 0)\n    \n    plot_test_metrics(\n      avg_test_loss,\n      avg_test_acc,\n      avg_test_precision,\n      avg_test_recall,\n      avg_test_f1,\n      save_path=SAVE_PATH,\n      filename='test_metrics_average.png'\n    )\n\n    # Train final model on the entire dataset\n    final_train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    final_criterion = get_weighted_loss_criterion(\n        dataset,\n        np.arange(len(dataset)),\n        meta_info_path=meta_info_path,\n        fold='final'\n    ).to(device)\n    final_model = CNNLSTM(\n        num_classes=num_classes,\n        lstm_hidden_size=lstm_hidden_size,\n        lstm_layers=lstm_layers,\n        dropout=DROPOUT,\n        cnn_model=CNN_TYPE\n    ).to(device)\n    final_optimizer = torch.optim.Adam(final_model.parameters(), lr=LR, weight_decay=0.01)\n\n    # Use average number of epochs from cross-validation\n    avg_epochs = int(np.mean([len(history['train_loss']) for history in all_metrics]))\n\n    final_model, final_history = train_model_final(\n        final_model,\n        final_train_loader,\n        final_criterion,\n        final_optimizer,\n        dataset,\n        fold='final',\n        num_epochs=avg_epochs,\n        log_interval=1,\n        writer=writer,  # Pass the TensorBoard writer\n        save_path=SAVE_PATH\n    )\n\n    # Save the final model\n    model_save_path = os.path.join(SAVE_PATH, 'final_model.pth')\n    torch.save(final_model.state_dict(), model_save_path)\n    print(f\"Final model saved at {model_save_path}\")\n\n    # Close TensorBoard writer\n    writer.close()\ndef prepare_dataset(spatial_aug_config=None, temporal_aug_config=None):\n    dataset = YogaVideoDataset(\n        csv_path,\n        sequence_path,\n        pose_list,\n        video_dir,\n        preprocessed_dir,\n        spatial_aug_config=spatial_aug_config,\n        temporal_aug_config=temporal_aug_config\n    )\n    print(f\"Dataset size: {len(dataset)}\")\n    return dataset  # Return only the full dataset\ndef get_all_the_labels(dataset):\n    return [label for _, label in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:04:18.014295Z","iopub.execute_input":"2025-02-25T17:04:18.014590Z","iopub.status.idle":"2025-02-25T17:04:18.037333Z","shell.execute_reply.started":"2025-02-25T17:04:18.014562Z","shell.execute_reply":"2025-02-25T17:04:18.036533Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-02-25T17:04:18.038274Z","iopub.execute_input":"2025-02-25T17:04:18.038539Z"},"id":"GpaMPADECZC-","outputId":"fb2305d9-5d28-4215-fb6f-58d135a893c3","trusted":true},"outputs":[{"name":"stdout","text":"Dataset size: 1575\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/RESNET_18_E15_UNFREEZE.zip /kaggle/working\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport torch\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport csv\n\ndef find_sus(model, test_loader, criterion, class_names, spath=SAVE_PATH, fsave='confusion_matrix_all.png'):\n    \"\"\"\n    Evaluate model on test set\n\n    Args:\n        model: PyTorch model\n        test_loader: DataLoader for test data\n        criterion: Loss function\n        class_names: List of class names\n        save_path: Directory to save the plot\n        fsave: Filename for confusion matrix plot\n    \"\"\"\n    csave = os.path.join(spath, fsave)\n    sussave = os.path.join(spath, 'sus.csv')\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n\n    test_loss = 0.0\n    test_correct = 0\n    test_total = 0\n    all_predictions = []\n    all_labels = []\n    misclassified_data = []\n\n    # Create progress bar\n    test_loader_tqdm = tqdm(test_loader, desc=\"Testing\")\n\n    with torch.no_grad():\n        for batch_idx, (inputs, labels, lengths) in enumerate(test_loader_tqdm):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            lengths = lengths.to(device)\n            # Forward pass\n            outputs = model(inputs, lengths)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item() * inputs.size(0)\n\n            # Get predictions\n            _, predicted = torch.max(outputs, 1)\n            # Calculate accuracy\n            test_correct += (predicted == labels).sum().item()\n            test_total += labels.size(0)\n\n            # Store predictions and labels for confusion matrix\n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            # Check if the sample is misclassified\n            misclassified_indices = (predicted != labels).nonzero(as_tuple=True)[0]\n            for misclassified_idx in misclassified_indices:\n                global_idx = batch_idx * inputs.size(0) + misclassified_idx.item()\n                sequence_id = test_loader.dataset.idx_to_label[global_idx]\n                correct_label = class_names[labels[misclassified_idx].item()]\n                prediction = class_names[predicted[misclassified_idx].item()]\n                misclassified_data.append([sequence_id, correct_label, prediction])\n\n    # Calculate metrics\n    test_loss = test_loss / len(test_loader.dataset)\n    accuracy = 100 * test_correct / test_total\n\n    # Create confusion matrix\n    cm = confusion_matrix(all_labels, all_predictions)\n\n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=pose_list, yticklabels=pose_list)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n\n    # Save the plot\n    plt.savefig(csave)\n    plt.show()\n    plt.close()\n\n    # Save misclassified data to CSV\n    with open(sussave, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['sequence_id', 'correct_label', 'prediction'])\n        writer.writerows(misclassified_data)\n\n    print(f'Test Loss: {test_loss:.4f}')\n    print(f'Test Accuracy: {accuracy:.2f}%')\n\n    return test_loss, accuracy","metadata":{"id":"zNhSc5nau5Ar","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset, train_dataset, test_dataset = prepare_dataset()\nlabel_to_pose = {v:k for k,v in dataset.pose_to_label.items()}\n\nmodel = CNNLSTM(num_classes=NUM_CLASSES, lstm_hidden_size=LSTM_HIDDEN_SIZE, lstm_layers=LSTM_LAYERS, dropout=DROPOUT, cnn_model = CNN_TYPE)\n# criterion = nn.CrossEntropyLoss()\nall_labels = [dataset[i][1] for i in range(len(dataset))]\ncriterion = create_weighted_criterion(\n        all_labels,\n        num_classes= NUM_CLASSES,\n        strategy='inverse'  # Try different strategies\n    )\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\ncheckpoint_path = os.path.join(SAVE_PATH, 'best_model.pth')\n# Plot the training curves\nif checkpoint_path and os.path.exists(checkpoint_path):\n        model, optimizer, start_epoch, history = load_checkpoint(\n            model, optimizer, checkpoint_path\n        )\n        print(f\"Resuming training from epoch {start_epoch}\")\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n# plot_training_curves(history)\nfind_sus(model, loader, criterion, label_to_pose)","metadata":{"id":"60euC5Fmu5As","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}