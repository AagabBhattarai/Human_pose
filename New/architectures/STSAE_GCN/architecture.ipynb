{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "JOINT_COUNT = 33\n",
    "\n",
    "\n",
    "def edge2mat(link, num_node):\n",
    "    A = np.zeros((num_node, num_node))\n",
    "    for i, j in link:\n",
    "        A[j, i] = 1\n",
    "    return A\n",
    "\n",
    "\n",
    "def normalize_digraph(A, dim=0):\n",
    "    # A is a 2D square array\n",
    "    Dl = np.sum(A, dim)\n",
    "    h, w = A.shape\n",
    "    Dn = np.zeros((w, w))\n",
    "\n",
    "    for i in range(w):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i] ** (-1)\n",
    "\n",
    "    AD = np.dot(A, Dn)\n",
    "    return AD\n",
    "\n",
    "\n",
    "class Graph:\n",
    "    \"\"\"The Graph to model the skeletons.\n",
    "\n",
    "    Args:\n",
    "        layout (str): must be one of the following candidates: 'mediapipe'. Default: 'mediapipe'.\n",
    "        mode (str): must be one of the following candidates: 'stgcn_spatial', 'spatial'. Default: 'spatial'.\n",
    "        max_hop (int): the maximal distance between two connected nodes.\n",
    "            Default: 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layout=\"mediapipe\",\n",
    "        mode=\"spatial\",\n",
    "        max_hop=1,\n",
    "        nx_node=1,\n",
    "        num_filter=3,\n",
    "        init_std=0.02,\n",
    "        init_off=0.04,\n",
    "    ):\n",
    "\n",
    "        self.max_hop = max_hop\n",
    "        self.layout = layout\n",
    "        self.mode = mode\n",
    "        self.num_filter = num_filter\n",
    "        self.init_std = init_std\n",
    "        self.init_off = init_off\n",
    "        self.nx_node = nx_node\n",
    "\n",
    "        assert (\n",
    "            nx_node == 1 or mode == \"random\"\n",
    "        ), \"nx_node can be > 1 only if mode is 'random'\"\n",
    "        assert layout in [\"mediapipe\"]\n",
    "\n",
    "        self.get_layout(layout)\n",
    "        #  self.hop_dis = get_hop_distance(self.num_node, self.inward, max_hop)\n",
    "\n",
    "        assert hasattr(self, mode), f\"Do Not Exist This Mode: {mode}\"\n",
    "        self.A = getattr(self, mode)()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.A\n",
    "\n",
    "    def get_layout(self, layout):\n",
    "        if layout == \"mediapipe\":\n",
    "            self.num_node = 33\n",
    "            self.inward = [\n",
    "                # Torso\n",
    "                (12, 11),\n",
    "                (24, 12),\n",
    "                (24, 23),\n",
    "                (23, 11),  # shoulders to hips\n",
    "                # Right arm\n",
    "                (14, 12),\n",
    "                (16, 14),\n",
    "                (18, 16),\n",
    "                (20, 18),\n",
    "                (20, 22),  # shoulder to fingertip\n",
    "                # Left arm\n",
    "                (13, 11),\n",
    "                (15, 13),\n",
    "                (17, 15),\n",
    "                (19, 17),\n",
    "                (21, 19),  # shoulder to fingertip\n",
    "                # Right leg\n",
    "                (26, 24),\n",
    "                (28, 26),\n",
    "                (30, 28),\n",
    "                (32, 30),  # hip to foot\n",
    "                # Left leg\n",
    "                (25, 23),\n",
    "                (27, 25),\n",
    "                (29, 27),\n",
    "                (31, 29),  # hip to foot\n",
    "                # Face\n",
    "                (1, 0),\n",
    "                (2, 1),\n",
    "                (3, 2),\n",
    "                (7, 3),  # right eye\n",
    "                (4, 0),\n",
    "                (5, 4),\n",
    "                (6, 5),\n",
    "                (8, 6),  # left eye\n",
    "                (10, 9),  # mouth\n",
    "                # Add connections to nose (0) from shoulders\n",
    "                # (0, 11), (0, 12)  # do we include this connection dear friend?\n",
    "            ]\n",
    "            self.center = 11  # Left shoulder as the center\n",
    "        else:\n",
    "            raise ValueError(f\"Do Not Exist This Layout: {layout}\")\n",
    "        self.self_link = [(i, i) for i in range(self.num_node)]\n",
    "        self.outward = [(j, i) for (i, j) in self.inward]\n",
    "        self.neighbor = self.inward + self.outward\n",
    "\n",
    "    def spatial(self):\n",
    "        Iden = edge2mat(self.self_link, self.num_node)\n",
    "        In = normalize_digraph(edge2mat(self.inward, self.num_node))\n",
    "        Out = normalize_digraph(edge2mat(self.outward, self.num_node))\n",
    "        A = np.stack((Iden, In, Out))\n",
    "        return A\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AGCN(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, adaptive=\"init\", conv_pos=\"pre\", with_res=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.graph = Graph()\n",
    "        A = torch.tensor(self.graph.A, dtype=torch.float32, requires_grad=False)\n",
    "        self.num_subsets = A.size(0)\n",
    "        # print(\"NUM_SUBSET:\", self.num_subsets)\n",
    "        self.adaptive = adaptive\n",
    "        self.conv_pos = conv_pos\n",
    "        self.with_res = with_res\n",
    "\n",
    "        self.A = nn.Parameter(A.clone())\n",
    "\n",
    "        if self.conv_pos == \"pre\":\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels * self.num_subsets, 1)\n",
    "        elif self.conv_pos == \"post\":\n",
    "            self.conv = nn.Conv2d(self.num_subsets * in_channels, out_channels, 1)\n",
    "\n",
    "        if self.with_res:\n",
    "            if in_channels != out_channels:\n",
    "                self.residual = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, 1),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                )\n",
    "            else:\n",
    "                self.residual = lambda x: x\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, t, v = x.shape\n",
    "        res = self.residual(x) if self.with_res else 0\n",
    "\n",
    "        A = self.A\n",
    "\n",
    "        if self.conv_pos == \"pre\":\n",
    "            x = self.conv(x)\n",
    "            x = x.view(n, self.num_subsets, -1, t, v)\n",
    "            x = torch.einsum(\"nkctv,kvw->nctw\", (x, A)).contiguous()\n",
    "        elif self.conv_pos == \"post\":\n",
    "            x = torch.einsum(\"nctv,kvw->nkctw\", (x, A)).contiguous()\n",
    "            x = x.view(n, -1, t, v)\n",
    "            x = self.conv(x)\n",
    "\n",
    "        return F.relu(self.batch_norm(x) + res)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        if hasattr(m, \"weight\"):\n",
    "            # He initialization for convolutional layers\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        if hasattr(m, \"weight\") and m.weight is not None:\n",
    "            m.weight.data.normal_(1.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        pad = (kernel_size + (kernel_size - 1) * (dilation - 1) - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(kernel_size, 1),\n",
    "            padding=(pad, 0),\n",
    "            stride=(stride, 1),\n",
    "            dilation=(dilation, 1),\n",
    "        )\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MTCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        dilations=[1, 2, 3, 4],\n",
    "        residual=True,\n",
    "        residual_kernel_size=1,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            out_channels % (len(dilations) + 2) == 0\n",
    "        ), \"# out channels should be multiples of # branches\"\n",
    "\n",
    "        # Multiple branches of temporal convolution\n",
    "        self.num_branches = len(dilations) + 2\n",
    "        branch_channels = out_channels // self.num_branches\n",
    "        if type(kernel_size) == list:\n",
    "            assert len(kernel_size) == len(dilations)\n",
    "        else:\n",
    "            kernel_size = [kernel_size] * len(dilations)\n",
    "        # Temporal Convolution branches\n",
    "        self.branches = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n",
    "                    nn.BatchNorm2d(branch_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    TemporalConv(\n",
    "                        branch_channels,\n",
    "                        branch_channels,\n",
    "                        kernel_size=ks,\n",
    "                        stride=stride,\n",
    "                        dilation=dilation,\n",
    "                    ),\n",
    "                )\n",
    "                for ks, dilation in zip(kernel_size, dilations)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Additional Max & 1x1 branch\n",
    "        self.branches.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=(3, 1), stride=(stride, 1), padding=(1, 0)),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.branches.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    branch_channels,\n",
    "                    kernel_size=1,\n",
    "                    padding=0,\n",
    "                    stride=(stride, 1),\n",
    "                ),\n",
    "                nn.BatchNorm2d(branch_channels),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Residual connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=residual_kernel_size,\n",
    "                stride=stride,\n",
    "            )\n",
    "\n",
    "        # initialize\n",
    "        self.apply(weights_init)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input dim: (N,C,T,V)\n",
    "        res = self.residual(x)\n",
    "        branch_outs = []\n",
    "        for tempconv in self.branches:\n",
    "            out = tempconv(x)\n",
    "            branch_outs.append(out)\n",
    "\n",
    "        out = torch.cat(branch_outs, dim=1)\n",
    "        out = self.bn(out)\n",
    "        out += res\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, in_channels, joint_count=JOINT_COUNT):\n",
    "        super(SAM, self).__init__()\n",
    "\n",
    "        # Conv1D over joints (V), capturing spatial dependencies\n",
    "        ks = (\n",
    "            joint_count if joint_count % 2 else joint_count - 1\n",
    "        )  # Ensure odd kernel size\n",
    "        padding = (ks - 1) // 2\n",
    "        self.gs = nn.Conv1d(in_channels, 1, kernel_size=ks, padding=padding)\n",
    "\n",
    "        # Pool over time (T) only, preserve joints (V)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(\n",
    "            (1, None)\n",
    "        )  # Collapses T, keeps V unchanged\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, T, V = x.shape\n",
    "        # Pool along time (T) dimension only\n",
    "        t = self.avg_pool(x)  # (N, C, 1, V)\n",
    "\n",
    "        t = t.squeeze(2)\n",
    "        t = self.gs(t)  # (N, 1, V)\n",
    "        Ms = self.sigmoid(t)\n",
    "        Ms = Ms.unsqueeze(2)\n",
    "        return (x + x * Ms), Ms\n",
    "\n",
    "\n",
    "class TAM(nn.Module):\n",
    "    def __init__(self, in_channels, temporal_size=9):\n",
    "        super(TAM, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(\n",
    "            (None, 1)\n",
    "        )  # Keep temporal dim, pool joints\n",
    "        padding = (temporal_size) // 2\n",
    "        self.gt = nn.Conv1d(in_channels, 1, kernel_size=temporal_size, padding=padding)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pool over joints (V) while keeping temporal dimension\n",
    "        t = self.avg_pool(x).squeeze(-1)  # (N, C, T)\n",
    "\n",
    "        t = self.gt(t)  # (N, 1, T)\n",
    "\n",
    "        Mt = self.sigmoid(t).unsqueeze(-1)  # (N, 1, T, 1)\n",
    "\n",
    "        return x * Mt + x, Mt\n",
    "\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=2):\n",
    "        super(CAM, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        # Combine the operations into a single sequential block\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction),\n",
    "            nn.ReLU(),  # Î´ applied to entire W1 term\n",
    "            nn.Linear(in_channels // reduction, in_channels),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, _, _ = x.shape\n",
    "        t = self.avg_pool(x).view(n, c)\n",
    "        Mc = self.fc(t).view(n, c, 1, 1)\n",
    "        return x * Mc + x, Mc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class STSAE_GCN_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, residual=True, stride=1):\n",
    "        super(STSAE_GCN_Block, self).__init__()\n",
    "\n",
    "        # Graph Convolution\n",
    "        self.agcn = AGCN(in_channels, out_channels)\n",
    "        if USE_ATTENTION:\n",
    "          self.sam = SAM(out_channels)  # Spatial Attention\n",
    "          self.tam = TAM(out_channels)  # Temporal Attention\n",
    "          self.cam = CAM(out_channels)  # Channel Attention\n",
    "\n",
    "        # Multi-Scale Temporal Convolution Network\n",
    "        self.mtcn = MTCN(out_channels, out_channels)\n",
    "\n",
    "        # Residual Connection\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0\n",
    "        elif (in_channels == out_channels) and (stride == 1):\n",
    "            self.residual = lambda x: x\n",
    "        else:\n",
    "            self.residual = TemporalConv(\n",
    "                in_channels, out_channels, kernel_size=1, stride=stride\n",
    "            )\n",
    "\n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Store Attention Maps\n",
    "        self.Ms = None  # Spatial Attention Map\n",
    "        self.Mt = None  # Temporal Attention Map\n",
    "        self.Mc = None  # Channel Attention Map\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.residual(x)\n",
    "\n",
    "        # Apply Graph Convolution\n",
    "        x = self.agcn(x)\n",
    "\n",
    "        # Apply Attention Modules and Store Attention Maps\n",
    "        if USE_ATTENTION:\n",
    "          x, self.Ms = self.sam(x)  # Spatial Attention\n",
    "          x, self.Mt = self.tam(x)  # Temporal Attention\n",
    "          x, self.Mc = self.cam(x)  # Channel Attention\n",
    "\n",
    "        # Apply Multi-Scale Temporal Convolution\n",
    "        x = self.bn(self.mtcn(x)) + res\n",
    "\n",
    "        return self.relu(x)\n",
    "\n",
    "    def get_attention(self):\n",
    "        \"\"\"\n",
    "        Returns the stored attention maps.\n",
    "        \"\"\"\n",
    "        return {\"Ms\": self.Ms, \"Mt\": self.Mt, \"Mc\": self.Mc}\n",
    "\n",
    "\n",
    "class STSAE_GCN(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, hidden_channels, num_classes, num_frames, num_blocks=9\n",
    "    ):\n",
    "        super(STSAE_GCN, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        num_nodes = 33\n",
    "        self.batch_norm = nn.BatchNorm1d(in_channels * num_nodes)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                STSAE_GCN_Block(\n",
    "                    in_channels if i == 0 else hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    residual=False if i == 0 else True,\n",
    "                )\n",
    "                for i in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n",
    "        self.batch_norm_out = nn.BatchNorm1d(hidden_channels)\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_channels, num_classes\n",
    "        )  # Only hidden_channels remain after pooling\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, in_channels, num_frames, num_nodes)\n",
    "\n",
    "        # print(\"SHAPE X:\", x.shape)\n",
    "        N, C, T, V = x.size()\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = x.view(N, V * C, T)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.view(N, V, C, T).permute(0, 2, 3, 1)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            # print(\"X SHAPE:\", x.shape)\n",
    "\n",
    "        # Apply Global Average Pooling\n",
    "        x = (\n",
    "            self.avg_pool(x).squeeze(-1).squeeze(-1)\n",
    "        )  # Reduce to (batch_size, hidden_channels)\n",
    "        \n",
    "        x = self.batch_norm_out(x)\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def count_parameters(self):\n",
    "        total_params = 0\n",
    "        param_str = \"\"\n",
    "        for name, parameter in self.named_parameters():\n",
    "            if parameter.requires_grad:\n",
    "                params = parameter.numel()\n",
    "                param_str += f\"{name}: {params}\\n\"\n",
    "                total_params += params\n",
    "        param_str += f\"Total Trainable Params: {total_params}\"\n",
    "        print(param_str)\n",
    "        return param_str, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.1\n",
    "VAL_SPLIT = 0.1\n",
    "BATCH_SIZE = 16\n",
    "NUM_BLOCKS = 9\n",
    "HIDDEN_CHANNELS = 120\n",
    "LEARNING_RATE = 0.001\n",
    "ATTENTION = \"NO_ATTENTION_NEW\"\n",
    "USE_ATTENTION = True\n",
    "DATA_AUGMENTATION = True\n",
    "WEIGHT_DECAY = 0.005\n",
    "LR_PATIENCE = 5\n",
    "LR_DROP_FACTOR = 0.1\n",
    "NUM_CLASSES = 13\n",
    "model = STSAE_GCN(3, HIDDEN_CHANNELS, num_frames=20, num_classes=NUM_CLASSES,num_blocks=NUM_BLOCKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.61860990524292\n",
      "Logits Mean: -0.0121\n",
      "Logits Std: 0.8552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming model, NUM_CLASSES, and BATCH_SIZE are defined as in the notebook\n",
    "\n",
    "# Generate dummy data\n",
    "dummy_input = torch.randn(16, 3, 20, 33)  # Batch size 10, 3 channels, 20 frames, 17 joints\n",
    "dummy_labels = torch.randint(0, NUM_CLASSES, (16,))  # Batch size 10, random labels\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Forward pass\n",
    "outputs = model(dummy_input)\n",
    "loss = criterion(outputs, dummy_labels)\n",
    "\n",
    "# Print the loss\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# Calculate mean loss (if needed, though CrossEntropyLoss already returns a mean)\n",
    "# mean_loss = loss.mean()\n",
    "# print(\"Mean Loss:\", mean_loss.item())\n",
    "# Calculate mean and std of logits\n",
    "logits_mean = outputs.mean().item()\n",
    "logits_std = outputs.std().item()\n",
    "print(f\"Logits Mean: {logits_mean:.4f}\")\n",
    "print(f\"Logits Std: {logits_std:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
