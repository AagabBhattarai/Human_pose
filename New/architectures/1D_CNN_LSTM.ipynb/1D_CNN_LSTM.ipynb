{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3,\n",
    "                              stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3,\n",
    "                              padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, input_channels,hidden_size=256, num_classes=NUM_CLASSES, num_nodes=33):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(input_channels * num_nodes)\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.blk1 = ResBlock(32, 64, stride=2)\n",
    "        self.blk2 = ResBlock(64, 128, stride=2)\n",
    "        self.blk3 = ResBlock(128, 256, stride=2)\n",
    "        self.blk4 = ResBlock(256, 512, stride=2)\n",
    "\n",
    "        self.V_downsampled = 3\n",
    "        self.lstm = nn.LSTM(512 * self.V_downsampled, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, T, V = x.size()\n",
    "\n",
    "        # BATCH NORM\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = x.view(N, V * C, T)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.view(N, V, C, T).permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "        # CNN LAYER APPLICATION\n",
    "        x = x.view(N*T, C, V)\n",
    "        x = self.conv1(x)\n",
    "        x = self.blk1(x)\n",
    "        x = self.blk2(x)\n",
    "        x = self.blk3(x)\n",
    "        x = self.blk4(x)\n",
    "\n",
    "        # Current shape: (N*T, 512, V_downsampled)\n",
    "        _, C_out, V_down = x.size()\n",
    "        x = x.view(N, T, C_out, V_down)\n",
    "\n",
    "        # no interleaving (x0,y0,z0, x1,y1,z1,..) instead (x0,x1,..x32, y1, y2,...)\n",
    "        x = x.view(N, T, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    def count_parameters(self):\n",
    "        total_params = 0\n",
    "        param_str = \"\"\n",
    "        for name, parameter in self.named_parameters():\n",
    "            if parameter.requires_grad:\n",
    "                params = parameter.numel()\n",
    "                print(f\"{name}: {params}\")\n",
    "                param_str += f\"{name}: {params}\\n\"\n",
    "                total_params += params\n",
    "        param_str += f\"Total Trainable Params: {total_params}\"\n",
    "        return param_str, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7769935131073\n",
      "Output shape: torch.Size([2, 13])\n",
      "Labels shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming the CNN_LSTM class is defined as in the previous cells\n",
    "\n",
    "# Dummy input data\n",
    "N = 2  # Batch size\n",
    "C = 3  # Number of channels\n",
    "T = 20  # Sequence length\n",
    "V = 33  # Number of joints\n",
    "\n",
    "dummy_input = torch.randn(N, C, T, V)\n",
    "dummy_labels = torch.randint(0, NUM_CLASSES, (N,))\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNNLSTM(3)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Forward pass\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(output, dummy_labels)\n",
    "\n",
    "# Print the loss\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# # Optional: Backpropagation (for a complete test)\n",
    "# optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Labels shape:\", dummy_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_norm.weight: 99\n",
      "batch_norm.bias: 99\n",
      "conv1.0.weight: 288\n",
      "conv1.0.bias: 32\n",
      "conv1.1.weight: 32\n",
      "conv1.1.bias: 32\n",
      "blk1.conv1.weight: 6144\n",
      "blk1.conv1.bias: 64\n",
      "blk1.bn1.weight: 64\n",
      "blk1.bn1.bias: 64\n",
      "blk1.conv2.weight: 12288\n",
      "blk1.conv2.bias: 64\n",
      "blk1.bn2.weight: 64\n",
      "blk1.bn2.bias: 64\n",
      "blk1.shortcut.0.weight: 2048\n",
      "blk1.shortcut.0.bias: 64\n",
      "blk1.shortcut.1.weight: 64\n",
      "blk1.shortcut.1.bias: 64\n",
      "blk2.conv1.weight: 24576\n",
      "blk2.conv1.bias: 128\n",
      "blk2.bn1.weight: 128\n",
      "blk2.bn1.bias: 128\n",
      "blk2.conv2.weight: 49152\n",
      "blk2.conv2.bias: 128\n",
      "blk2.bn2.weight: 128\n",
      "blk2.bn2.bias: 128\n",
      "blk2.shortcut.0.weight: 8192\n",
      "blk2.shortcut.0.bias: 128\n",
      "blk2.shortcut.1.weight: 128\n",
      "blk2.shortcut.1.bias: 128\n",
      "blk3.conv1.weight: 98304\n",
      "blk3.conv1.bias: 256\n",
      "blk3.bn1.weight: 256\n",
      "blk3.bn1.bias: 256\n",
      "blk3.conv2.weight: 196608\n",
      "blk3.conv2.bias: 256\n",
      "blk3.bn2.weight: 256\n",
      "blk3.bn2.bias: 256\n",
      "blk3.shortcut.0.weight: 32768\n",
      "blk3.shortcut.0.bias: 256\n",
      "blk3.shortcut.1.weight: 256\n",
      "blk3.shortcut.1.bias: 256\n",
      "blk4.conv1.weight: 393216\n",
      "blk4.conv1.bias: 512\n",
      "blk4.bn1.weight: 512\n",
      "blk4.bn1.bias: 512\n",
      "blk4.conv2.weight: 786432\n",
      "blk4.conv2.bias: 512\n",
      "blk4.bn2.weight: 512\n",
      "blk4.bn2.bias: 512\n",
      "blk4.shortcut.0.weight: 131072\n",
      "blk4.shortcut.0.bias: 512\n",
      "blk4.shortcut.1.weight: 512\n",
      "blk4.shortcut.1.bias: 512\n",
      "lstm.weight_ih_l0: 1572864\n",
      "lstm.weight_hh_l0: 262144\n",
      "lstm.bias_ih_l0: 1024\n",
      "lstm.bias_hh_l0: 1024\n",
      "fc.weight: 3328\n",
      "fc.bias: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('batch_norm.weight: 99\\nbatch_norm.bias: 99\\nconv1.0.weight: 288\\nconv1.0.bias: 32\\nconv1.1.weight: 32\\nconv1.1.bias: 32\\nblk1.conv1.weight: 6144\\nblk1.conv1.bias: 64\\nblk1.bn1.weight: 64\\nblk1.bn1.bias: 64\\nblk1.conv2.weight: 12288\\nblk1.conv2.bias: 64\\nblk1.bn2.weight: 64\\nblk1.bn2.bias: 64\\nblk1.shortcut.0.weight: 2048\\nblk1.shortcut.0.bias: 64\\nblk1.shortcut.1.weight: 64\\nblk1.shortcut.1.bias: 64\\nblk2.conv1.weight: 24576\\nblk2.conv1.bias: 128\\nblk2.bn1.weight: 128\\nblk2.bn1.bias: 128\\nblk2.conv2.weight: 49152\\nblk2.conv2.bias: 128\\nblk2.bn2.weight: 128\\nblk2.bn2.bias: 128\\nblk2.shortcut.0.weight: 8192\\nblk2.shortcut.0.bias: 128\\nblk2.shortcut.1.weight: 128\\nblk2.shortcut.1.bias: 128\\nblk3.conv1.weight: 98304\\nblk3.conv1.bias: 256\\nblk3.bn1.weight: 256\\nblk3.bn1.bias: 256\\nblk3.conv2.weight: 196608\\nblk3.conv2.bias: 256\\nblk3.bn2.weight: 256\\nblk3.bn2.bias: 256\\nblk3.shortcut.0.weight: 32768\\nblk3.shortcut.0.bias: 256\\nblk3.shortcut.1.weight: 256\\nblk3.shortcut.1.bias: 256\\nblk4.conv1.weight: 393216\\nblk4.conv1.bias: 512\\nblk4.bn1.weight: 512\\nblk4.bn1.bias: 512\\nblk4.conv2.weight: 786432\\nblk4.conv2.bias: 512\\nblk4.bn2.weight: 512\\nblk4.bn2.bias: 512\\nblk4.shortcut.0.weight: 131072\\nblk4.shortcut.0.bias: 512\\nblk4.shortcut.1.weight: 512\\nblk4.shortcut.1.bias: 512\\nlstm.weight_ih_l0: 1572864\\nlstm.weight_hh_l0: 262144\\nlstm.bias_ih_l0: 1024\\nlstm.bias_hh_l0: 1024\\nfc.weight: 3328\\nfc.bias: 13\\nTotal Trainable Params: 3590419',\n",
       " 3590419)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNLSTM(3, 128)\n",
    "param_count, trainable_count = model.count_parameters()\n",
    "\n",
    "with open(\"model_parameters_count.txt\", \"w\") as f:\n",
    "    f.write(f\"Total Params: {param_count}\\n\")\n",
    "    f.write(f\"Total Trainable Params: {trainable_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean logits: 0.011346329003572464\n",
      "Std logits: 0.20061618089675903\n"
     ]
    }
   ],
   "source": [
    "mean_logits = output.mean().item()\n",
    "std_logits = output.std().item()\n",
    "\n",
    "print(f\"Mean logits: {mean_logits}\")\n",
    "print(f\"Std logits: {std_logits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
