{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEcYREdIKc3_",
        "outputId": "4017bda3-e114-46f1-e48b-5b19281827ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.10/dist-packages (2.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (10.4.0)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (0.5.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from imageio[ffmpeg]) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg->imageio[ffmpeg]) (75.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "# %pip install opencv-python\n",
        "%pip install imageio[ffmpeg]\n",
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve64pLu4Kc4F",
        "outputId": "613c9499-4b8e-484d-fd99-561211648086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "# Define base folder path\n",
        "base_path = '/content/gdrive/MyDrive/RGB_data_stream'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pgF2d1uVKc4G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "csv_path = os.path.join(base_path,'data/3DYoga90_corrected.csv')\n",
        "sequence_path = os.path.join(base_path, 'short/downloaded_log.txt')\n",
        "save_path = os.path.join(base_path, 'Second_Resnet_nov6')\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "pose_list = ['mountain', 'half-way-lift', 'standing-forward-bend', 'downward-dog']\n",
        "NUM_CLASSES = len(pose_list)\n",
        "video_dir = os.path.join(base_path, 'short')\n",
        "preprocessed_dir = os.path.join(base_path, 'complete_pre_processed')\n",
        "os.makedirs(preprocessed_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v4o0CzWLKc4H"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "FRAME_HEIGHT = 224  # VGG16 input size\n",
        "FRAME_WIDTH = 224\n",
        "SEQUENCE_LENGTH = 16\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 0.0001\n",
        "VALIDATION_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.1\n",
        "NUM_EPOCHS = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxUMJ-OkKc4H"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Work Left\n",
        "1. Data Augmentation\n",
        "2. Expanding to more classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i-weN6y5Kc4J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import imageio\n",
        "\n",
        "class YogaVideoDataset(Dataset):\n",
        "    def __init__(self, csv_path, sequence_path, pose_list, video_dir, preprocessed_dir):\n",
        "        with open(sequence_path) as f:\n",
        "            sequence_list = f.read().splitlines()\n",
        "            sequence_list = [int(x) for x in sequence_list]\n",
        "\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.df = self.df[self.df['sequence_id'].isin(sequence_list)]\n",
        "        self.df = self.df[self.df['l3_pose'].isin(pose_list)]\n",
        "\n",
        "        self.pose_to_idx = {pose: idx for idx, pose in enumerate(pose_list)}\n",
        "        self.length_of_dataset = len(self.df)\n",
        "\n",
        "        self.video_dir = video_dir\n",
        "        self.preprocessed_dir = preprocessed_dir\n",
        "\n",
        "        os.makedirs(self.preprocessed_dir, exist_ok=True)\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize((FRAME_HEIGHT, FRAME_WIDTH)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.cache = dict()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length_of_dataset\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        sequence_id = self.df.iloc[i]['sequence_id']\n",
        "        pose = self.df.iloc[i]['l3_pose']\n",
        "        label = self.pose_to_idx[pose]\n",
        "\n",
        "        # Check cache first\n",
        "        if sequence_id in self.cache:\n",
        "            frames = self.cache[sequence_id]\n",
        "        else:\n",
        "            # Path to preprocessed tensor file\n",
        "            preprocessed_path = os.path.join(self.preprocessed_dir, f\"{sequence_id}.pt\")\n",
        "\n",
        "            if os.path.exists(preprocessed_path):\n",
        "                frames = torch.load(preprocessed_path, weights_only=True)\n",
        "            else:\n",
        "                # Process and save if not already preprocessed\n",
        "                video_path = os.path.join(self.video_dir, f\"{sequence_id}.mp4\")\n",
        "                frames = self._get_frames(video_path)\n",
        "                torch.save(frames, preprocessed_path)\n",
        "\n",
        "            # Add to cache\n",
        "            # self.cache[sequence_id] = frames\n",
        "\n",
        "        return frames, label\n",
        "\n",
        "    def _get_frames(self, video_path):\n",
        "        reader = imageio.get_reader(video_path, 'ffmpeg')\n",
        "        total_frames = reader.count_frames()\n",
        "        indices = np.linspace(0, total_frames - 1, SEQUENCE_LENGTH, dtype=int)\n",
        "\n",
        "        frames = []\n",
        "        for i, frame in enumerate(reader):\n",
        "            # if i in indices:\n",
        "                frame = Image.fromarray(frame)\n",
        "                frame = self.transforms(frame)\n",
        "                frames.append(frame)\n",
        "\n",
        "        reader.close()\n",
        "        frames = torch.stack(frames)\n",
        "        return frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U4u_UQWKc4K"
      },
      "source": [
        "# Model\n",
        "``` (mobile_net to get feature map and LSTM to go through the frame sequences)```\n",
        "Trying out average of LSTM from each time_step\n",
        "Work Left\n",
        "1. Using only last time step output from LSTM to using average value, max value, using attention mechanism\n",
        "2. Using other imagenet model to extract the feature map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "taCxNt4YKc4K"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torchvision.models as models\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class CNNLSTM(nn.Module):\n",
        "#     def __init__(self, num_classes, lstm_hidden_size=512, lstm_layers=1, dropout=0.5):\n",
        "#         super(CNNLSTM, self).__init__()\n",
        "\n",
        "#         # Load pretrained MobileNetV2\n",
        "#         mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "#         self.features = nn.Sequential(*list(mobilenet.features))\n",
        "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "#         # Initially freeze all MobileNetV2 parameters\n",
        "#         self._freeze_features()\n",
        "\n",
        "#         # MobileNetV2 outputs 1280 features after pooling\n",
        "#         self.feature_size = 1280\n",
        "\n",
        "#         # LSTM layer\n",
        "#         self.lstm = nn.LSTM(\n",
        "#             input_size=self.feature_size,\n",
        "#             hidden_size=lstm_hidden_size,\n",
        "#             num_layers=lstm_layers,\n",
        "#             batch_first=True,\n",
        "#             dropout=dropout if lstm_layers > 1 else 0\n",
        "#         )\n",
        "\n",
        "#         # Gradual dimension reduction in classifier with batch normalization\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Linear(lstm_hidden_size, 512),\n",
        "#             nn.BatchNorm1d(512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout),\n",
        "\n",
        "#             nn.Linear(512, 256),\n",
        "#             nn.BatchNorm1d(256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout),\n",
        "\n",
        "#             nn.Linear(256, 128),\n",
        "#             nn.BatchNorm1d(128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout/2),  # Reducing dropout as we get closer to output\n",
        "\n",
        "#             nn.Linear(128, num_classes)\n",
        "#         )\n",
        "\n",
        "#         # Initialize weights for the classifier layers\n",
        "#         self._initialize_weights()\n",
        "\n",
        "#     def _initialize_weights(self):\n",
        "#         for m in self.classifier.modules():\n",
        "#             if isinstance(m, nn.Linear):\n",
        "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "#                 if m.bias is not None:\n",
        "#                     nn.init.constant_(m.bias, 0)\n",
        "#             elif isinstance(m, nn.BatchNorm1d):\n",
        "#                 nn.init.constant_(m.weight, 1)\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "\n",
        "#     def _freeze_features(self):\n",
        "#         for param in self.features.parameters():\n",
        "#             param.requires_grad = False\n",
        "\n",
        "#     def unfreeze_features_gradually(self, num_layers=3, start_from_end=True):\n",
        "#         \"\"\"\n",
        "#         Unfreeze a specified number of convolutional layers from MobileNetV2.\n",
        "#         Args:\n",
        "#             num_layers: Number of layers to unfreeze\n",
        "#             start_from_end: If True, start unfreezing from the end (default)\n",
        "#         \"\"\"\n",
        "#         # First freeze all layers\n",
        "#         self._freeze_features()\n",
        "\n",
        "#         # Get list of all feature layers\n",
        "#         features_list = list(self.features.modules())\n",
        "#         conv_layers = [m for m in features_list if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "#         # Determine which layers to unfreeze\n",
        "#         if start_from_end:\n",
        "#             layers_to_unfreeze = conv_layers[-num_layers:]\n",
        "#         else:\n",
        "#             layers_to_unfreeze = conv_layers[:num_layers]\n",
        "\n",
        "#         # Unfreeze the selected layers\n",
        "#         for layer in layers_to_unfreeze:\n",
        "#             for param in layer.parameters():\n",
        "#                 param.requires_grad = True\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         batch_size, seq_length, c, h, w = x.size()\n",
        "\n",
        "#         # Process each frame through CNN\n",
        "#         x = x.view(batch_size * seq_length, c, h, w)\n",
        "#         x = self.features(x)\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         # Reshape for LSTM\n",
        "#         x = x.view(batch_size, seq_length, self.feature_size)\n",
        "\n",
        "#         # LSTM processing\n",
        "#         lstm_out, _ = self.lstm(x)\n",
        "\n",
        "#         # Try different temporal pooling strategies\n",
        "#         # Strategy 1: Mean pooling (default)\n",
        "#         x = torch.mean(lstm_out, dim=1)\n",
        "#         # Strategy 2: Last state\n",
        "#         # x = lstm_out[:, -1, :]\n",
        "#         # Strategy 3: Max pooling\n",
        "#         # x = torch.max(lstm_out, dim=1)[0]\n",
        "\n",
        "#         # Classification\n",
        "#         x = self.classifier(x)\n",
        "#         return x\n",
        "\n",
        "#     def get_trainable_params(self):\n",
        "#         \"\"\"Return the number of trainable parameters\"\"\"\n",
        "#         return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNLSTM(nn.Module):\n",
        "    def __init__(self, num_classes, lstm_hidden_size=512, lstm_layers=1, dropout=0.5):\n",
        "        super(CNNLSTM, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet18\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        # Extract all layers except the final fully connected layer\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-2])  # Remove last FC layer and avgpool\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Initially freeze all ResNet parameters\n",
        "        self._freeze_features()\n",
        "\n",
        "        # ResNet18 outputs 512 features after pooling (for ResNet50, this would be 2048)\n",
        "        self.feature_size = 512\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.feature_size,\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Gradual dimension reduction in classifier with batch normalization\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden_size, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout / 2),  # Reducing dropout as we get closer to output\n",
        "\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights for the classifier layers\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _freeze_features(self):\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_features_gradually(self, num_layers=3, start_from_end=True):\n",
        "        \"\"\"\n",
        "        Unfreeze a specified number of convolutional layers from ResNet.\n",
        "        Args:\n",
        "            num_layers: Number of layers to unfreeze\n",
        "            start_from_end: If True, start unfreezing from the end (default)\n",
        "        \"\"\"\n",
        "        # First freeze all layers\n",
        "        self._freeze_features()\n",
        "\n",
        "        # Get list of all feature layers\n",
        "        features_list = list(self.features.modules())\n",
        "        conv_layers = [m for m in features_list if isinstance(m, nn.Conv2d)]\n",
        "\n",
        "        # Determine which layers to unfreeze\n",
        "        if start_from_end:\n",
        "            layers_to_unfreeze = conv_layers[-num_layers:]\n",
        "        else:\n",
        "            layers_to_unfreeze = conv_layers[:num_layers]\n",
        "\n",
        "        # Unfreeze the selected layers\n",
        "        for layer in layers_to_unfreeze:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, c, h, w = x.size()\n",
        "\n",
        "        # Process each frame through CNN\n",
        "        x = x.view(batch_size * seq_length, c, h, w)\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Reshape for LSTM\n",
        "        x = x.view(batch_size, seq_length, self.feature_size)\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Try different temporal pooling strategies\n",
        "        # Strategy 1: Mean pooling (default)\n",
        "        x = torch.mean(lstm_out, dim=1)\n",
        "        # Strategy 2: Last state\n",
        "        # x = lstm_out[:, -1, :]\n",
        "        # Strategy 3: Max pooling\n",
        "        # x = torch.max(lstm_out, dim=1)[0]\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def get_trainable_params(self):\n",
        "        \"\"\"Return the number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "NKgHTKgpSDny"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MeGvP73aKc4L"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "def train_val_test_split(dataset):\n",
        "    total_size = len(dataset)\n",
        "    test_size = int(TEST_SPLIT * total_size)\n",
        "    val_size = int(VALIDATION_SPLIT * total_size)\n",
        "    train_size = total_size - val_size - test_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        dataset,\n",
        "        [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def create_data_loaders(train_dataset, val_dataset, test_dataset):\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory = True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory = True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory = True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Yu8Wt-JWKc4M"
      },
      "outputs": [],
      "source": [
        "def plot_training_curves(history, fsave='training_curves.png'):\n",
        "    tsave = os.path.join(save_path, fsave)\n",
        "    # plt.style.use('seaborn')\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Loss curves\n",
        "    axs[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "    axs[0, 0].plot(history['val_loss'], label='Validation Loss', marker='o')\n",
        "    axs[0, 0].set_title('Loss')\n",
        "    axs[0, 0].legend()\n",
        "\n",
        "    # Accuracy curves\n",
        "    axs[0, 1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "    axs[0, 1].plot(history['val_acc'], label='Validation Accuracy', marker='o')\n",
        "    axs[0, 1].set_title('Accuracy')\n",
        "    axs[0, 1].legend()\n",
        "\n",
        "    # Learning rate\n",
        "    axs[0, 2].plot(history['learning_rates'], label='Learning Rate', marker='o')\n",
        "    axs[0, 2].set_title('Learning Rate')\n",
        "    axs[0, 2].set_yscale('log')\n",
        "    axs[0, 2].legend()\n",
        "\n",
        "    # Precision\n",
        "    axs[1, 0].plot(history['train_precision'], label='Train Precision', marker='o')\n",
        "    axs[1, 0].plot(history['val_precision'], label='Validation Precision', marker='o')\n",
        "    axs[1, 0].set_title('Precision')\n",
        "    axs[1, 0].legend()\n",
        "\n",
        "    # Recall\n",
        "    axs[1, 1].plot(history['train_recall'], label='Train Recall', marker='o')\n",
        "    axs[1, 1].plot(history['val_recall'], label='Validation Recall', marker='o')\n",
        "    axs[1, 1].set_title('Recall')\n",
        "    axs[1, 1].legend()\n",
        "\n",
        "    # F1 Score\n",
        "    axs[1, 2].plot(history['train_f1'], label='Train F1', marker='o')\n",
        "    axs[1, 2].plot(history['val_f1'], label='Validation F1', marker='o')\n",
        "    axs[1, 2].set_title('F1 Score')\n",
        "    axs[1, 2].legend()\n",
        "\n",
        "    for ax in axs.flat:\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(tsave)\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C9hI_YPeKc4M"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        # on default = 7 successive val_loss increase stop\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ym8hfKK7YpbC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "def save_checkpoint(model, optimizer, epoch, history, best_path=None):\n",
        "    if best_path is not None:\n",
        "        chk_path = os.path.join(save_path, f'best_model.pth')\n",
        "        print(f\"Saving checkpoint to {chk_path}\")\n",
        "    else:\n",
        "        chk_path = os.path.join(save_path, f'checkpath_model.pth')\n",
        "        print(f\"Saving checkpoint to {chk_path}\")\n",
        "\n",
        "    # Combine model, optimizer, and history into one dictionary\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,  # Save the next epoch number for resuming\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'history': history  # Save history along with the model and optimizer\n",
        "    }\n",
        "\n",
        "    # Save everything in a single file using torch.save\n",
        "    torch.save(checkpoint, chk_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gTWiSzTLPAGj"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Load model and training state from a checkpoint\n",
        "    \"\"\"\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Load model and optimizer states\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Get the epoch number to resume from\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "    # Load training history with new metrics\n",
        "    history = checkpoint.get('history', {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': [],\n",
        "        'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
        "        'val_precision': [], 'val_recall': [], 'val_f1': [],\n",
        "        'learning_rates': []\n",
        "    })\n",
        "\n",
        "    return model, optimizer, start_epoch, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LWy0HheUKc4M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                num_epochs=50, patience=7, log_interval=10, checkpoint_path=None, unfreeze_epoch = 5, num_layers_unfreeze =3):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Initialize scheduler\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4)\n",
        "\n",
        "    # Initialize early stopping (example implementation; you may need to define your own)\n",
        "    early_stopping = EarlyStopping(patience=patience, min_delta=1e-4)\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Initialize history for loss, accuracy, precision, recall, and F1\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': [],\n",
        "        'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
        "        'val_precision': [], 'val_recall': [], 'val_f1': [],\n",
        "        'learning_rates': []\n",
        "    }\n",
        "\n",
        "    # Check for checkpoint and load if available\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        model, optimizer, start_epoch, history = load_checkpoint(\n",
        "            model, optimizer, checkpoint_path\n",
        "        )\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "        if epoch == unfreeze_epoch:\n",
        "            print(f\"Unfreezing last {num_layers_unfreeze} layers of MobileNetV2\")\n",
        "            model.unfreeze_features_gradually(num_layers=num_layers_unfreeze)\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        history['learning_rates'].append(current_lr)\n",
        "        print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "        train_true, train_pred = [], []\n",
        "\n",
        "        train_loader_tqdm = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n",
        "        for batch_idx, (inputs, labels) in train_loader_tqdm:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "            # Collect true and predicted labels for precision/recall\n",
        "            train_true.extend(labels.cpu().numpy())\n",
        "            train_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "            # Log batch-level updates\n",
        "            if batch_idx % log_interval == 0:\n",
        "                train_loader_tqdm.set_postfix({\n",
        "                    'loss': train_loss / (batch_idx + 1),\n",
        "                    'accuracy': 100.0 * train_correct / train_total\n",
        "                })\n",
        "\n",
        "        # Calculate training metrics\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = 100.0 * train_correct / train_total\n",
        "        train_precision = precision_score(train_true, train_pred, average='weighted')\n",
        "        train_recall = recall_score(train_true, train_pred, average='weighted')\n",
        "        train_f1 = f1_score(train_true, train_pred, average='weighted')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        val_true, val_pred = [], []\n",
        "\n",
        "        val_loader_tqdm = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, labels) in val_loader_tqdm:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "                # Collect true and predicted labels for precision/recall\n",
        "                val_true.extend(labels.cpu().numpy())\n",
        "                val_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "                # Log batch-level updates for validation\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    val_loader_tqdm.set_postfix({\n",
        "                        'loss': val_loss / (batch_idx + 1),\n",
        "                        'accuracy': 100.0 * val_correct / val_total\n",
        "                    })\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100.0 * val_correct / val_total\n",
        "        val_precision = precision_score(val_true, val_pred, average='weighted')\n",
        "        val_recall = recall_score(val_true, val_pred, average='weighted')\n",
        "        val_f1 = f1_score(val_true, val_pred, average='weighted')\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['train_precision'].append(train_precision)\n",
        "        history['train_recall'].append(train_recall)\n",
        "        history['train_f1'].append(train_f1)\n",
        "        history['val_precision'].append(val_precision)\n",
        "        history['val_recall'].append(val_recall)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Print metrics at the end of the epoch\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs} Summary:')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Train Precision: {train_precision:.2f} | Train Recall: {train_recall:.2f} | Train F1: {train_f1:.2f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Val Precision: {val_precision:.2f} | Val Recall: {val_recall:.2f} | Val F1: {val_f1:.2f}')\n",
        "\n",
        "        # Save the best model checkpoint\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_path = os.path.join(save_path, 'best_model.pth')\n",
        "            save_checkpoint(model, optimizer, epoch, history, best_model_path)\n",
        "            print(f\"New best model saved! Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "        # Adjust learning rate based on validation loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Check for early stopping\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0Qt54or3XzJE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, class_names,spath=save_path, fsave='confusion_matrix.png'):\n",
        "    \"\"\"\n",
        "    Evaluate model on test set\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        test_loader: DataLoader for test data\n",
        "        criterion: Loss function\n",
        "        class_names: List of class names\n",
        "        save_path: Directory to save the plot\n",
        "        fsave: Filename for confusion matrix plot\n",
        "    \"\"\"\n",
        "    csave = os.path.join(spath, fsave)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Create progress bar\n",
        "    test_loader_tqdm = tqdm(test_loader, desc=\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_tqdm:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            # Calculate accuracy\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            test_total += labels.size(0)\n",
        "\n",
        "            # Store predictions and labels for confusion matrix\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "    accuracy = 100 * test_correct / test_total\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(csave)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    return test_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2DtznLJKc4N",
        "outputId": "ebeec4dc-c7b3-44d5-9cd6-b2727888346f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Data\n",
            "Finished Loading Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 182MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model\n",
            "Using device: cuda\n",
            "\n",
            "Epoch 1/50\n",
            "Current Learning Rate: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/31 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "print(\"Loading Data\")\n",
        "dataset = YogaVideoDataset(csv_path, sequence_path, pose_list, video_dir, preprocessed_dir)\n",
        "train_dataset, val_dataset, test_dataset = train_val_test_split(dataset)\n",
        "train_loader, val_loader, test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset)\n",
        "print(\"Finished Loading Data\")\n",
        "\n",
        "model = CNNLSTM(num_classes=NUM_CLASSES)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Training the model\")\n",
        "checkpoint_path = os.path.join(save_path, 'checkpath_model.pth')\n",
        "model, history = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    NUM_EPOCHS,\n",
        "    patience=7,\n",
        "    log_interval=10,\n",
        "    checkpoint_path=checkpoint_path\n",
        ")\n",
        "# Plot the training curves\n",
        "plot_training_curves(history)\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        model, optimizer, start_epoch, history = load_checkpoint(\n",
        "            model, optimizer, checkpoint_path\n",
        "        )\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "evaluate_model(model, test_loader, criterion, pose_list)\n",
        "\n",
        "model_save_path = os.path.join(save_path, 'my_model.pth')\n",
        "torch.save(model.state_dict(), model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_curves(history)"
      ],
      "metadata": {
        "id": "Yyk-IIoKoYW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, test_loader, criterion, pose_list)"
      ],
      "metadata": {
        "id": "38XMmvjPodfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = os.path.join(save_path, 'best_model.pth')\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        model, optimizer, start_epoch, history = load_checkpoint(\n",
        "            model, optimizer, checkpoint_path\n",
        "        )\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "evaluate_model(model, test_loader, criterion, pose_list)"
      ],
      "metadata": {
        "id": "i2SzQwOvollu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TO EXAMINE THE DATASET FOR FAULTS\n"
      ],
      "metadata": {
        "id": "CNlzK-nG9Kzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def find_potential_label_errors(model, dataset, dataloader, threshold=0.9, device='cuda'):\n",
        "    # Move model to GPU\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    potential_errors = {\n",
        "        'sequence_id': [],\n",
        "        'predicted_pose': [],\n",
        "        'given_pose': [],\n",
        "        'confidence': []\n",
        "    }\n",
        "\n",
        "    # Reverse mapping from index to pose name\n",
        "    idx_to_pose = {v: k for k, v in dataset.pose_to_idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (sequences, labels) in enumerate(tqdm(dataloader, desc=\"Analyzing sequences\")):\n",
        "            # Move tensors to GPU\n",
        "            sequences = sequences.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            logits = model(sequences)  # Your raw logits\n",
        "            probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "            predictions = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            # Find high confidence disagreements\n",
        "            for idx, (pred, true_label, prob) in enumerate(zip(predictions, labels, probabilities)):\n",
        "                confidence = prob[pred].item()\n",
        "\n",
        "                if pred != true_label and confidence > threshold:\n",
        "                    # Get global index and sequence_id\n",
        "                    global_idx = batch_idx * dataloader.batch_size + idx\n",
        "                    sequence_id = dataset.df.iloc[global_idx]['sequence_id']\n",
        "\n",
        "                    potential_errors['sequence_id'].append(sequence_id)\n",
        "                    potential_errors['predicted_pose'].append(idx_to_pose[pred.item()])\n",
        "                    potential_errors['given_pose'].append(idx_to_pose[true_label.item()])\n",
        "                    potential_errors['confidence'].append(confidence)\n",
        "\n",
        "    return potential_errors\n",
        "\n",
        "def analyze_yoga_errors(potential_errors):\n",
        "    \"\"\"Analyze patterns in potential errors for yoga poses\"\"\"\n",
        "    error_df = pd.DataFrame(potential_errors)\n",
        "\n",
        "    print(f\"Found {len(error_df)} potential errors\")\n",
        "\n",
        "    # Analyze pose-wise errors\n",
        "    print(\"\\nMost common incorrect labelings:\")\n",
        "    error_counts = error_df.groupby(['given_pose', 'predicted_pose']).size().sort_values(ascending=False)\n",
        "    print(error_counts.head(10))\n",
        "\n",
        "    # High confidence errors\n",
        "    print(\"\\nHighest confidence disagreements:\")\n",
        "    high_conf_errors = error_df.sort_values('confidence', ascending=False)\n",
        "    print(high_conf_errors.head(10))\n",
        "\n",
        "    return error_df\n",
        "\n",
        "def validate_yoga_dataset(model, dataset, batch_size=32, threshold=0.9):\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Find potential errors\n",
        "    errors = find_potential_label_errors(model, dataset, dataloader, threshold)\n",
        "\n",
        "    # Analyze and get DataFrame of errors\n",
        "    error_df = analyze_yoga_errors(errors)\n",
        "\n",
        "    return error_df\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Define de-normalization transform\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "denormalize = transforms.Normalize(\n",
        "    mean=[-m/s for m, s in zip(imagenet_mean, imagenet_std)],\n",
        "    std=[1/s for s in imagenet_std]\n",
        ")\n",
        "\n",
        "def save_suspicious_sequences(error_df, dataset, save_dir, max_samples=5):\n",
        "    \"\"\"Save frames from suspicious sequences for manual review\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Group by pose pairs to save examples of each type of error\n",
        "    grouped_errors = error_df.groupby(['given_pose', 'predicted_pose'])\n",
        "\n",
        "    for (given_pose, pred_pose), group in grouped_errors:\n",
        "        # Take up to max_samples highest confidence examples for each error type\n",
        "        samples = group.nlargest(max_samples, 'confidence')\n",
        "\n",
        "        for idx, row in samples.iterrows():\n",
        "            sequence_id = row['sequence_id']\n",
        "            confidence = row['confidence']\n",
        "\n",
        "            # Create directory for this error type\n",
        "            error_dir = os.path.join(save_dir, f\"{given_pose}_to_{pred_pose}\")\n",
        "            os.makedirs(error_dir, exist_ok=True)\n",
        "\n",
        "            try:\n",
        "                # Find the index in dataset.df that matches this sequence_id\n",
        "                dataset_idx = dataset.df[dataset.df['sequence_id'] == sequence_id].index\n",
        "\n",
        "                if len(dataset_idx) > 0:\n",
        "                    # Get the sequence frames\n",
        "                    frames, _ = dataset[dataset_idx[0]]\n",
        "\n",
        "                    # Move frames to CPU if they're on GPU\n",
        "                    if frames.is_cuda:\n",
        "                        frames = frames.cpu()\n",
        "\n",
        "                    # Save middle frame as representative image\n",
        "                    middle_frame = frames[len(frames)//2]\n",
        "\n",
        "                    # De-normalize the frame\n",
        "                    middle_frame = denormalize(middle_frame)\n",
        "\n",
        "                    # Clip the values to the valid range [0, 1] for image saving\n",
        "                    middle_frame = torch.clamp(middle_frame, 0, 1)\n",
        "\n",
        "                    save_path = os.path.join(error_dir, f'seq_{sequence_id}_conf_{confidence:.2f}.png')\n",
        "\n",
        "                    # Convert tensor to PIL Image and save\n",
        "                    middle_frame = transforms.ToPILImage()(middle_frame)\n",
        "                    middle_frame.save(save_path)\n",
        "                else:\n",
        "                    print(f\"Warning: Sequence ID {sequence_id} not found in dataset\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sequence {sequence_id}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "# Usage example:\n",
        "\n",
        "model = CNNLSTM(num_classes=NUM_CLASSES)\n",
        "dataset = YogaVideoDataset(csv_path, sequence_path, pose_list, video_dir, preprocessed_dir)\n",
        "checkpoint_path = os.path.join(save_path, 'best_model.pth')\n",
        "\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "  model, optimizer, start_epoch, history = load_checkpoint(\n",
        "            model, optimizer, checkpoint_path\n",
        "  )\n",
        "  print(f\"Resuming training from epoch {start_epoch}\")\n",
        "    # Find and analyze errors\n",
        "error_df = validate_yoga_dataset(model, dataset, batch_size=32, threshold=0.9)\n",
        "\n",
        "    # Save suspicious sequences for manual review\n",
        "save_dir=os.path.join(save_path,'suspicious_sequences')\n",
        "save_suspicious_sequences(error_df, dataset, save_dir)\n",
        "\n",
        "csv_path_file = os.path.join(save_path, 'potential_label_errors.csv')\n",
        "    # Export results to CSV for further analysis\n",
        "error_df.to_csv(csv_path_file, index=False)\n",
        "\n",
        "    # Print some statistics\n",
        "print(\"\\nMost frequently mislabeled sequences:\")\n",
        "sequence_counts = error_df['sequence_id'].value_counts()\n",
        "print(sequence_counts.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "rFuA0ugooxk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3lvFeFK1W-96"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}