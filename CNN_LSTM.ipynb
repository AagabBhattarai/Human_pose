{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\aagab\\anaconda3\\lib\\site-packages (2.33.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio[ffmpeg]) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio[ffmpeg]) (10.3.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio[ffmpeg]) (0.5.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio[ffmpeg]) (5.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio-ffmpeg->imageio[ffmpeg]) (69.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\users\\aagab\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install opencv-python\n",
    "%pip install imageio[ffmpeg]\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_path = 'data/3DYoga90.csv'\n",
    "sequence_path = 'short/downloaded_log.txt'\n",
    "pose_list = ['mountain', 'half-way-lift', 'standing-forward-bend', 'downward-dog']\n",
    "NUM_CLASSES = len(pose_list)\n",
    "video_dir = 'short'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "FRAME_HEIGHT = 224  # VGG16 input size\n",
    "FRAME_WIDTH = 224\n",
    "SEQUENCE_LENGTH = 16 \n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Work Left\n",
    "1. Data Augmentation\n",
    "2. Expanding to more classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "class YogaVideoDataset(Dataset):\n",
    "    def __init__(self, csv_path, sequence_path, pose_list, video_dir):\n",
    "        with open(sequence_path) as f:\n",
    "            sequence_list = f.read().splitlines()\n",
    "            sequence_list = [int(x) for x in sequence_list]\n",
    "            \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        # Keep only downloaded sequences\n",
    "        self.df = self.df[self.df['sequence_id'].isin(sequence_list)]\n",
    "        # Keep only required classes\n",
    "        self.df = self.df[self.df['l3_pose'].isin(pose_list)]\n",
    "\n",
    "        self.pose_to_idx = {pose: idx for idx, pose in enumerate(pose_list)}\n",
    "\n",
    "        self.length_of_dataset = len(self.df)\n",
    "\n",
    "        self.video_dir = video_dir\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize((FRAME_HEIGHT, FRAME_WIDTH)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_of_dataset\n",
    "\n",
    "    def print(self):\n",
    "        print(len(self.df))\n",
    "        print(self.pose_to_idx)\n",
    "        print(len(self))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sequence_id = self.df.iloc[i]['sequence_id']\n",
    "        # print(sequence_id)\n",
    "        video_path = os.path.join(self.video_dir, f\"{sequence_id}.mp4\")\n",
    "        pose = self.df.iloc[i]['l3_pose']\n",
    "\n",
    "        label = torch.zeros(NUM_CLASSES)\n",
    "        label[self.pose_to_idx[pose]] = 1\n",
    "\n",
    "        frames = self._get_frames(video_path)\n",
    "        # print(frames.shape)\n",
    "        \n",
    "        return frames, label\n",
    "    \n",
    "    def _get_frames(self, video_path):\n",
    "        reader = imageio.get_reader(video_path, 'ffmpeg')\n",
    "        total_frames = reader.count_frames()\n",
    "        # print(total_frames)\n",
    "        indices = np.linspace(0, total_frames-1, SEQUENCE_LENGTH, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        for i, frame in enumerate(reader):\n",
    "            if i in indices:\n",
    "                frame = Image.fromarray(frame)\n",
    "                frame = self.transforms(frame)\n",
    "                frames.append(frame)\n",
    "        \n",
    "        reader.close()\n",
    "        frames = torch.stack([torch.tensor(np.array(f)) for f in frames])\n",
    "        return frames  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "``` (VGG16 to get feature map and LSTM to go through the frame sequences)```\n",
    "\n",
    "Work Left\n",
    "1. Using only last time step output from LSTM to using average value, max value, using attention mechanism\n",
    "2. Using other imagenet model to extract the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        # Load pretrained VGG16\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        # Remove the last classifier layer\n",
    "        self.features = nn.Sequential(*list(vgg.features.children()))\n",
    "        \n",
    "        # Freeze VGG16 parameters\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # # LSTM configuration\n",
    "        # self.lstm = nn.LSTM(\n",
    "        #     input_size=512*7*7,  # VGG16 output size\n",
    "        #     hidden_size=512,\n",
    "        #     num_layers=2,\n",
    "        #     batch_first=True\n",
    "        # )\n",
    "                # LSTM \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512*7*7,  # VGG16 output size\n",
    "            hidden_size=512,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.size()\n",
    "        \n",
    "        # Combine batch and sequence dimensions\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        \n",
    "        # Extract CNN features\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Flatten the CNN output\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Use the last time step output\n",
    "        x = lstm_out[:, -1, :] \n",
    "        \n",
    "        # Classify\n",
    "        x = self.classifier(x)\n",
    "        return F.log_softmax(x, dim=1)  # for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def train_val_test_split(dataset):\n",
    "    total_size = len(dataset)\n",
    "    test_size = int(TEST_SPLIT * total_size)\n",
    "    val_size = int(VALIDATION_SPLIT * total_size)\n",
    "    train_size = total_size - val_size - test_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def create_data_loaders(train_dataset, val_dataset, test_dataset):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, train_accs, val_accs, save_dir, timestamp):\n",
    "    \"\"\"Plot and save training curves\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Training Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title('Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'training_curves_{timestamp}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs=50, save_dir='checkpoints'):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create save directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Initialize scheduler\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_acc = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc='Training')\n",
    "        \n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # For one-hot encoded labels\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels_idx = torch.max(labels, 1)\n",
    "            running_corrects += torch.sum(predicted == labels_idx).item()\n",
    "            total_samples += inputs.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.0 * running_corrects / total_samples:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = 100.0 * running_corrects / total_samples\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Progress bar for validation\n",
    "        val_pbar = tqdm(val_loader, desc='Validation')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels_idx = torch.max(labels, 1)\n",
    "                running_corrects += torch.sum(predicted == labels_idx).item()\n",
    "                total_samples += inputs.size(0)\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{100.0 * running_corrects / total_samples:.2f}%'\n",
    "                })\n",
    "        \n",
    "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = 100.0 * running_corrects / total_samples\n",
    "        \n",
    "        # Store statistics\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'\\nTraining Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.2f}%')\n",
    "        print(f'Validation Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.2f}%')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            best_epoch = epoch\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': epoch_val_acc,\n",
    "                'val_loss': epoch_val_loss,\n",
    "            }, os.path.join(save_dir, f'best_model_{timestamp}.pt'))\n",
    "        \n",
    "        # Save training stats\n",
    "        np.save(os.path.join(save_dir, f'training_stats_{timestamp}.npy'), {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_val_acc': best_val_acc\n",
    "        })\n",
    "        \n",
    "        # Plot and save training curves\n",
    "        plot_training_curves(\n",
    "            train_losses, val_losses, \n",
    "            train_accuracies, val_accuracies,\n",
    "            save_dir, timestamp\n",
    "        )\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        # on default = 7 successive val_loss increase stop\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YogaVideoDataset(csv_path, sequence_path, pose_list, video_dir)\n",
    "train_loader, val_loader, test_loader = train_val_test_split(dataset)\n",
    "\n",
    "model = CNNLSTM(num_classes=NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "trained_model, training_stats = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=50,\n",
    "    save_dir='checkpoints'\n",
    ")\n",
    "\n",
    "# Load best model if needed\n",
    "checkpoint = torch.load('checkpoints/best_model_[timestamp].pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, class_names):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels.argmax(axis=1), all_predictions.argmax(axis=1))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (all_predictions == all_labels).all(axis=1).mean()\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5056, 0.7598, 0.0992],\n",
      "        [0.2523, 0.2669, 0.5835],\n",
      "        [0.5865, 0.1670, 0.2410]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,3)\n",
    "print(a)\n",
    "_, pred = torch.max(a, 1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.Tensor([[1,0,0],[0,1,0],[1,0,0]])\n",
    "_, li = torch.max(label, dim =1)\n",
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 2.2489,  2.2489,  2.2489,  ...,  0.1426,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.1254,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.0056,  0.4337,  0.6049],\n",
       "           ...,\n",
       "           [-0.0458, -0.0287, -0.0116,  ..., -0.7993, -0.8335, -0.8678],\n",
       "           [-0.0801, -0.0629, -0.0458,  ..., -0.8164, -0.8507, -0.8849],\n",
       "           [-0.0972, -0.0801, -0.0629,  ..., -0.8164, -0.8507, -0.8849]],\n",
       " \n",
       "          [[ 2.4286,  2.4286,  2.4286,  ...,  0.1001,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ...,  0.0826,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ..., -0.0574,  0.4153,  0.5903],\n",
       "           ...,\n",
       "           [-0.1800, -0.1625, -0.1450,  ..., -1.1429, -1.1779, -1.2129],\n",
       "           [-0.2150, -0.1975, -0.1800,  ..., -1.1604, -1.1954, -1.2304],\n",
       "           [-0.2325, -0.2150, -0.1975,  ..., -1.1604, -1.1954, -1.2304]],\n",
       " \n",
       "          [[ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.2010,  0.2696,  0.4439],\n",
       "           ...,\n",
       "           [-0.2010, -0.1835, -0.1661,  ..., -1.3164, -1.3513, -1.3861],\n",
       "           [-0.2358, -0.2184, -0.2010,  ..., -1.3339, -1.3687, -1.4036],\n",
       "           [-0.2532, -0.2358, -0.2184,  ..., -1.3339, -1.3687, -1.4036]]],\n",
       " \n",
       " \n",
       "         [[[ 2.2489,  2.2489,  2.2489,  ...,  0.1426,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.1254,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.0056,  0.4337,  0.6049],\n",
       "           ...,\n",
       "           [-0.0458, -0.0287, -0.0116,  ..., -0.7993, -0.8335, -0.8678],\n",
       "           [-0.0801, -0.0629, -0.0458,  ..., -0.8164, -0.8507, -0.8849],\n",
       "           [-0.0972, -0.0801, -0.0629,  ..., -0.8164, -0.8507, -0.8849]],\n",
       " \n",
       "          [[ 2.4286,  2.4286,  2.4286,  ...,  0.1001,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ...,  0.0826,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ..., -0.0574,  0.4153,  0.5903],\n",
       "           ...,\n",
       "           [-0.1800, -0.1625, -0.1450,  ..., -1.1429, -1.1779, -1.2129],\n",
       "           [-0.2150, -0.1975, -0.1800,  ..., -1.1604, -1.1954, -1.2304],\n",
       "           [-0.2325, -0.2150, -0.1975,  ..., -1.1604, -1.1954, -1.2304]],\n",
       " \n",
       "          [[ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.2010,  0.2696,  0.4439],\n",
       "           ...,\n",
       "           [-0.2010, -0.1835, -0.1661,  ..., -1.3164, -1.3513, -1.3861],\n",
       "           [-0.2358, -0.2184, -0.2010,  ..., -1.3339, -1.3687, -1.4036],\n",
       "           [-0.2532, -0.2358, -0.2184,  ..., -1.3339, -1.3687, -1.4036]]],\n",
       " \n",
       " \n",
       "         [[[ 2.2489,  2.2489,  2.2489,  ...,  0.1426,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.1254,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.0056,  0.4337,  0.6049],\n",
       "           ...,\n",
       "           [-0.0458, -0.0287, -0.0116,  ..., -0.7993, -0.8335, -0.8678],\n",
       "           [-0.0801, -0.0629, -0.0458,  ..., -0.8164, -0.8507, -0.8849],\n",
       "           [-0.0972, -0.0801, -0.0629,  ..., -0.8164, -0.8507, -0.8849]],\n",
       " \n",
       "          [[ 2.4286,  2.4286,  2.4286,  ...,  0.1001,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ...,  0.0826,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ..., -0.0574,  0.4153,  0.5903],\n",
       "           ...,\n",
       "           [-0.1800, -0.1625, -0.1450,  ..., -1.1429, -1.1779, -1.2129],\n",
       "           [-0.2150, -0.1975, -0.1800,  ..., -1.1604, -1.1954, -1.2304],\n",
       "           [-0.2325, -0.2150, -0.1975,  ..., -1.1604, -1.1954, -1.2304]],\n",
       " \n",
       "          [[ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.2010,  0.2696,  0.4439],\n",
       "           ...,\n",
       "           [-0.2010, -0.1835, -0.1661,  ..., -1.3164, -1.3513, -1.3861],\n",
       "           [-0.2358, -0.2184, -0.2010,  ..., -1.3339, -1.3687, -1.4036],\n",
       "           [-0.2532, -0.2358, -0.2184,  ..., -1.3339, -1.3687, -1.4036]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 2.2489,  2.2489,  2.2489,  ...,  0.1426,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.1254,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.0056,  0.4337,  0.6049],\n",
       "           ...,\n",
       "           [-0.0458, -0.0287, -0.0116,  ..., -0.8164, -0.8507, -0.8678],\n",
       "           [-0.0801, -0.0629, -0.0458,  ..., -0.8335, -0.8678, -0.8849],\n",
       "           [-0.0972, -0.0801, -0.0629,  ..., -0.8335, -0.8678, -0.8849]],\n",
       " \n",
       "          [[ 2.4286,  2.4286,  2.4286,  ...,  0.1001,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ...,  0.0826,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ..., -0.0574,  0.4153,  0.5903],\n",
       "           ...,\n",
       "           [-0.1800, -0.1625, -0.1450,  ..., -1.1604, -1.1954, -1.2129],\n",
       "           [-0.2150, -0.1975, -0.1800,  ..., -1.1779, -1.2129, -1.2304],\n",
       "           [-0.2325, -0.2150, -0.1975,  ..., -1.1779, -1.2129, -1.2304]],\n",
       " \n",
       "          [[ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.2010,  0.2696,  0.4439],\n",
       "           ...,\n",
       "           [-0.2010, -0.1835, -0.1661,  ..., -1.3339, -1.3687, -1.3861],\n",
       "           [-0.2358, -0.2184, -0.2010,  ..., -1.3513, -1.3861, -1.4036],\n",
       "           [-0.2532, -0.2358, -0.2184,  ..., -1.3513, -1.3861, -1.4036]]],\n",
       " \n",
       " \n",
       "         [[[ 2.2489,  2.2489,  2.2489,  ...,  0.1426,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.1254,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.0056,  0.4337,  0.6049],\n",
       "           ...,\n",
       "           [-0.0629, -0.0458, -0.0287,  ..., -0.8164, -0.8507, -0.8678],\n",
       "           [-0.0801, -0.0629, -0.0629,  ..., -0.8335, -0.8678, -0.8849],\n",
       "           [-0.0972, -0.0801, -0.0801,  ..., -0.8335, -0.8678, -0.8849]],\n",
       " \n",
       "          [[ 2.4286,  2.4286,  2.4286,  ...,  0.1001,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ...,  0.0826,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ..., -0.0574,  0.4153,  0.5903],\n",
       "           ...,\n",
       "           [-0.1975, -0.1800, -0.1625,  ..., -1.1604, -1.1954, -1.2129],\n",
       "           [-0.2150, -0.1975, -0.1975,  ..., -1.1779, -1.2129, -1.2304],\n",
       "           [-0.2325, -0.2150, -0.2150,  ..., -1.1779, -1.2129, -1.2304]],\n",
       " \n",
       "          [[ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.2010,  0.2696,  0.4439],\n",
       "           ...,\n",
       "           [-0.2184, -0.2010, -0.1835,  ..., -1.3339, -1.3687, -1.3861],\n",
       "           [-0.2358, -0.2184, -0.2184,  ..., -1.3513, -1.3861, -1.4036],\n",
       "           [-0.2532, -0.2358, -0.2358,  ..., -1.3513, -1.3861, -1.4036]]],\n",
       " \n",
       " \n",
       "         [[[ 2.2489,  2.2489,  2.2489,  ...,  0.1426,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.1254,  0.4851,  0.6049],\n",
       "           [ 2.2489,  2.2489,  2.2489,  ...,  0.0056,  0.4337,  0.6049],\n",
       "           ...,\n",
       "           [-0.0629, -0.0458, -0.0287,  ..., -0.8164, -0.8507, -0.8678],\n",
       "           [-0.0801, -0.0629, -0.0629,  ..., -0.8335, -0.8678, -0.8849],\n",
       "           [-0.0972, -0.0801, -0.0801,  ..., -0.8335, -0.8678, -0.8849]],\n",
       " \n",
       "          [[ 2.4286,  2.4286,  2.4286,  ...,  0.1001,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ...,  0.0826,  0.4678,  0.5903],\n",
       "           [ 2.4286,  2.4286,  2.4286,  ..., -0.0574,  0.4153,  0.5903],\n",
       "           ...,\n",
       "           [-0.1975, -0.1800, -0.1625,  ..., -1.1604, -1.1954, -1.2129],\n",
       "           [-0.2150, -0.1975, -0.1975,  ..., -1.1779, -1.2129, -1.2304],\n",
       "           [-0.2325, -0.2150, -0.2150,  ..., -1.1779, -1.2129, -1.2304]],\n",
       " \n",
       "          [[ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.0615,  0.3219,  0.4439],\n",
       "           [ 2.6400,  2.6400,  2.6400,  ..., -0.2010,  0.2696,  0.4439],\n",
       "           ...,\n",
       "           [-0.2184, -0.2010, -0.1835,  ..., -1.3339, -1.3687, -1.3861],\n",
       "           [-0.2358, -0.2184, -0.2184,  ..., -1.3513, -1.3861, -1.4036],\n",
       "           [-0.2532, -0.2358, -0.2358,  ..., -1.3513, -1.3861, -1.4036]]]]),\n",
       " tensor([0., 1., 0., 0.]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = YogaVideoDataset(csv_path, sequence_path, pose_list, video_dir)\n",
    "td, vd, td = train_val_test_split(dataset)\n",
    "dataset[333]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
