{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio[ffmpeg] in c:\\users\\aagab\\anaconda3\\lib\\site-packages (2.33.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio[ffmpeg]) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio[ffmpeg]) (10.3.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio[ffmpeg]) (0.5.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio[ffmpeg]) (5.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from imageio-ffmpeg->imageio[ffmpeg]) (69.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in c:\\users\\aagab\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\aagab\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install opencv-python\n",
    "%pip install imageio[ffmpeg]\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "csv_path = os.path.join(base_path,'data/3DYoga90.csv')\n",
    "sequence_path = os.path.join(base_path, 'short/downloaded_log.txt')\n",
    "pose_list = ['mountain', 'half-way-lift', 'standing-forward-bend', 'downward-dog']\n",
    "NUM_CLASSES = len(pose_list)\n",
    "video_dir = os.path.join(base_path, 'short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "FRAME_HEIGHT = 224  # VGG16 input size\n",
    "FRAME_WIDTH = 224\n",
    "SEQUENCE_LENGTH = 16 \n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Work Left\n",
    "1. Data Augmentation\n",
    "2. Expanding to more classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "class YogaVideoDataset(Dataset):\n",
    "    def __init__(self, csv_path, sequence_path, pose_list, video_dir):\n",
    "        with open(sequence_path) as f:\n",
    "            sequence_list = f.read().splitlines()\n",
    "            sequence_list = [int(x) for x in sequence_list]\n",
    "            \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        # Keep only downloaded sequences\n",
    "        self.df = self.df[self.df['sequence_id'].isin(sequence_list)]\n",
    "        # Keep only required classes\n",
    "        self.df = self.df[self.df['l3_pose'].isin(pose_list)]\n",
    "\n",
    "        self.pose_to_idx = {pose: idx for idx, pose in enumerate(pose_list)}\n",
    "\n",
    "        self.length_of_dataset = len(self.df)\n",
    "\n",
    "        self.video_dir = video_dir\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize((FRAME_HEIGHT, FRAME_WIDTH)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_of_dataset\n",
    "\n",
    "    def print(self):\n",
    "        print(len(self.df))\n",
    "        print(self.pose_to_idx)\n",
    "        print(len(self))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sequence_id = self.df.iloc[i]['sequence_id']\n",
    "        # print(sequence_id)\n",
    "        video_path = os.path.join(self.video_dir, f\"{sequence_id}.mp4\")\n",
    "        pose = self.df.iloc[i]['l3_pose']\n",
    "\n",
    "        label = torch.zeros(NUM_CLASSES)\n",
    "        label[self.pose_to_idx[pose]] = 1\n",
    "\n",
    "        frames = self._get_frames(video_path)\n",
    "        # print(frames.shape)\n",
    "        \n",
    "        return frames, label\n",
    "    \n",
    "    def _get_frames(self, video_path):\n",
    "        reader = imageio.get_reader(video_path, 'ffmpeg')\n",
    "        total_frames = reader.count_frames()\n",
    "        # print(total_frames)\n",
    "        indices = np.linspace(0, total_frames-1, SEQUENCE_LENGTH, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        for i, frame in enumerate(reader):\n",
    "            if i in indices:\n",
    "                frame = Image.fromarray(frame)\n",
    "                frame = self.transforms(frame)\n",
    "                frames.append(frame)\n",
    "        \n",
    "        reader.close()\n",
    "        frames = torch.stack([torch.tensor(np.array(f)) for f in frames])\n",
    "        return frames  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "``` (VGG16 to get feature map and LSTM to go through the frame sequences)```\n",
    "\n",
    "Work Left\n",
    "1. Using only last time step output from LSTM to using average value, max value, using attention mechanism\n",
    "2. Using other imagenet model to extract the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        # Load pretrained VGG16\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        # Remove the last classifier layer\n",
    "        self.features = nn.Sequential(*list(vgg.features.children()))\n",
    "        \n",
    "        # Freeze VGG16 parameters\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # # LSTM configuration\n",
    "        # self.lstm = nn.LSTM(\n",
    "        #     input_size=512*7*7,  # VGG16 output size\n",
    "        #     hidden_size=512,\n",
    "        #     num_layers=2,\n",
    "        #     batch_first=True\n",
    "        # )\n",
    "                # LSTM \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512*7*7,  # VGG16 output size\n",
    "            hidden_size=512,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.5\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.size()\n",
    "        \n",
    "        # Combine batch and sequence dimensions\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        \n",
    "        # Extract CNN features\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Flatten the CNN output\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Use the last time step output\n",
    "        x = lstm_out[:, -1, :] \n",
    "        \n",
    "        # Classify\n",
    "        x = self.classifier(x)\n",
    "        return F.log_softmax(x, dim=1)  # for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def train_val_test_split(dataset):\n",
    "    total_size = len(dataset)\n",
    "    test_size = int(TEST_SPLIT * total_size)\n",
    "    val_size = int(VALIDATION_SPLIT * total_size)\n",
    "    train_size = total_size - val_size - test_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def create_data_loaders(train_dataset, val_dataset, test_dataset):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"Plot training curves including learning rate\"\"\"\n",
    "    plt.style.use('seaborn')\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot loss curves\n",
    "    ax1.plot(history['train_loss'], label='Training Loss', marker='o')\n",
    "    ax1.plot(history['val_loss'], label='Validation Loss', marker='o')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy curves\n",
    "    ax2.plot(history['train_acc'], label='Training Accuracy', marker='o')\n",
    "    ax2.plot(history['val_acc'], label='Validation Accuracy', marker='o')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot learning rate\n",
    "    ax3.plot(history['learning_rates'], label='Learning Rate', marker='o')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_title('Learning Rate over Time')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        # on default = 7 successive val_loss increase stop\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs=50, patience=7):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    # Initialize scheduler\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "    \n",
    "    # Initialize early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    \n",
    "    # Initialize history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Store current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, labels_idx = torch.max(labels, 1)  # For one-hot encoded labels\n",
    "            train_correct += (predicted == labels_idx).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                _, labels_idx = torch.max(labels, 1)\n",
    "                val_correct += (predicted == labels_idx).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%')\n",
    "        print(f'Learning Rate: {current_lr}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, class_names):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels.argmax(axis=1), all_predictions.argmax(axis=1))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (all_predictions == all_labels).all(axis=1).mean()\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Finished Loading Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aagab\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\aagab\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model\n",
      "Using device: cpu\n",
      "\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Data\")\n",
    "dataset = YogaVideoDataset(csv_path, sequence_path, pose_list, video_dir)\n",
    "train_dataset, val_dataset, test_dataset = train_val_test_split(dataset)\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset)\n",
    "print(\"Finished Loading Data\")\n",
    "\n",
    "model = CNNLSTM(num_classes=NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Training the model\")\n",
    "model, history = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    optimizer,\n",
    "    NUM_EPOCHS\n",
    ")\n",
    "# Plot the training curves\n",
    "plot_training_curves(history)\n",
    "evaluate_model(model, test_loader, criterion, pose_list)\n",
    "torch.save(model.state_dict(), 'my_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
