{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cu0Fn1p_CZC3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "BATCH_SIZE = 16\n",
        "# VALIDATION_SPLIT = 0.2\n",
        "TEST_SPLIT = 0.1\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "# Constants\n",
        "FRAME_HEIGHT = 224\n",
        "FRAME_WIDTH = 224\n",
        "SEQUENCE_LENGTH = 16\n",
        "\n",
        "ARCHITECTURE = '2D_CNN_LSTM'\n",
        "CNN_TYPE = 'Resnet'\n",
        "LSTM_HIDDEN_SIZE=512\n",
        "LSTM_LAYERS = 1\n",
        "DROPOUT = 0.5\n",
        "LEARNING_RATE = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "# Define base folder path\n",
        "base_path = '/content/gdrive/MyDrive/yoga_proj'\n",
        "!ls /content/gdrive/MyDrive/yoga_proj\n",
        "# base_path = '../'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Gets label\n",
        "csv_path = os.path.join(base_path,'data/3DYoga90_corrected.csv')\n",
        "meta_info_path = os.path.join(base_path, 'data')\n",
        "# Gets valid samples\n",
        "sequence_path = os.path.join(base_path, 'short/downloaded_log.txt')\n",
        "\n",
        "# Original Downloaded Seqeuence\n",
        "video_dir = os.path.join(base_path, 'short')\n",
        "\n",
        "# Pre_processed to tensor to make training fast # Makes video_dir redundant BUT NOT WITH KAGGLE IDK\n",
        "preprocessed_dir = os.path.join(base_path, 'RESIZED_SHORTS')\n",
        "os.makedirs(preprocessed_dir, exist_ok=True)\n",
        "assert os.path.isdir(preprocessed_dir), f\"Directory '{preprocessed_dir}' does not exist.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QiOQi7BCZC5",
        "outputId": "dae0e208-4ca2-4230-8259-88c630f9711b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "SAVE_PATH = os.path.join(\n",
        "    base_path,\n",
        "    f'CROSS_FOLD_{ARCHITECTURE}_{today}_LR{LEARNING_RATE}_LHS{LSTM_HIDDEN_SIZE}_LL{LSTM_LAYERS}_CT{CNN_TYPE}'\n",
        ")\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "print(SAVE_PATH)\n",
        "\n",
        "pose_list = ['downward-dog','standing-forward-bend','half-way-lift',\n",
        "             'mountain','chair','cobra','cockerel','extended-triangle',\n",
        "             'extended-side-angle','corpse','staff','wind-relieving','fish'\n",
        "            ]\n",
        "subset_of_poses = pose_list\n",
        "NUM_CLASSES = len(pose_list)\n",
        "\n",
        "# dataset_dir = os.path.join(base_path, 'official_dataset')\n",
        "# assert os.path.isdir(dataset_dir), f\"Directory '{dataset_dir}' does not exist.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_mVAnHmCZC6",
        "outputId": "1d12234a-00e4-4764-f025-d479b03949f0"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SCqudip1CZC6"
      },
      "outputs": [],
      "source": [
        "# meta_info_path = os.path.join(base_path, 'data')\n",
        "# pose_index = pd.read_csv(f'{meta_info_path}/pose-index.csv')\n",
        "# sequence_index = pd.read_csv(f'{meta_info_path}/3DYoga90_corrected.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpLOGWwACZC7"
      },
      "source": [
        "What does each file tell?\n",
        "\n",
        "1.) pose-index.csv -> Shows Heirarchical organization (THEN NOTHING MORE)\n",
        "\n",
        "2.) 3DYoga90.csv -> Total Main Info(i.e. along with RGB stream){\n",
        "    SequneceID: Parquet_FILE_NAME,\n",
        "    URL,\n",
        "    Frame Start and Frame Stop,\n",
        "    Pose Name, Training Test Split\n",
        "} `Difference between train and test? where to get the validation set from? How to do data augmentation?\n",
        "\n",
        "3.) Parquet Files -> {\n",
        "    Frame Number {\n",
        "        33 Landmarks\n",
        "    },\n",
        "    row-id: FrameNumber-TYPE-Landmark_index,\n",
        "    Coordinates: {x, y, z}\n",
        "}\n",
        "\n",
        "`PLEASE NOTE: The landmark coordinates are all normalized`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y0RRgDxCZC7"
      },
      "source": [
        "# Getting the data ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Kg3fH4QhCZC8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms.v2 as T\n",
        "import random\n",
        "\n",
        "class VideoAugmentationPipeline:\n",
        "    \"\"\"Video augmentation pipeline using batch transforms from torchvision.transforms.v2\"\"\"\n",
        "    def __init__(self, spatial_aug_config=None, temporal_aug_config=None):\n",
        "        # Default config with all augmentations enabled\n",
        "        default_spatial_config = {\n",
        "            'random_resized_crop': {'enabled': False, 'scale': (0.9, 1.0)},\n",
        "            'random_horizontal_flip': {'enabled': True, 'p': 0.5},\n",
        "            'color_jitter': {'enabled': True, 'brightness': 0.1, 'contrast': 0.1, 'saturation': 0.1, 'p': 0.5},\n",
        "            'gaussian_blur': {'enabled': True, 'p': 0.5},\n",
        "            'random_rotation': {'enabled': True, 'degrees': (-5, 5),'p':0.5},\n",
        "        }\n",
        "\n",
        "        default_temporal_config = {\n",
        "            'temporal_crop': {'enabled': True, 'crop_size': 0.9},\n",
        "            'temporal_mask': {'enabled': True, 'n_masks': 1, 'mask_size': 0.1},\n",
        "        }\n",
        "\n",
        "        # Update default config with user-provided config\n",
        "        self.spatial_aug_config = self._update_config(default_spatial_config, spatial_aug_config)\n",
        "        self.temporal_aug_config = self._update_config(default_temporal_config, temporal_aug_config)\n",
        "\n",
        "        # Build transforms that can handle batch inputs\n",
        "        self.spatial_transforms = self._build_spatial_transforms()\n",
        "\n",
        "    def _update_config(self, default_config, user_config):\n",
        "        \"\"\"Update default config with user config, disabling augmentations not in user config\"\"\"\n",
        "        if user_config is None:\n",
        "            return default_config\n",
        "            \n",
        "        updated_config = default_config.copy()\n",
        "        for aug_name in updated_config:\n",
        "            if aug_name in user_config:\n",
        "                # # Update probability if provided\n",
        "                # if isinstance(user_config[aug_name], dict):\n",
        "                #     updated_config[aug_name].update(user_config[aug_name])\n",
        "                # else:\n",
        "                    updated_config[aug_name]['p'] = user_config[aug_name]\n",
        "            else:\n",
        "                # Disable augmentation if not in user config\n",
        "                updated_config[aug_name]['enabled'] = False\n",
        "        return updated_config\n",
        "\n",
        "    def _build_spatial_transforms(self):\n",
        "        \"\"\"Build composition of spatial transforms that support batch processing\"\"\"\n",
        "        transform_list = []\n",
        "\n",
        "        if self.spatial_aug_config['random_resized_crop']['enabled']:\n",
        "            transform_list.append(\n",
        "                T.RandomResizedCrop(\n",
        "                    size=(224, 224),\n",
        "                    scale=self.spatial_aug_config['random_resized_crop']['scale'],\n",
        "                    antialias=True\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if self.spatial_aug_config['random_horizontal_flip']['enabled']:\n",
        "            transform_list.append(\n",
        "                T.RandomHorizontalFlip(p=self.spatial_aug_config['random_horizontal_flip']['p'])\n",
        "            )\n",
        "\n",
        "        if self.spatial_aug_config['color_jitter']['enabled']:\n",
        "            transform_list.append(\n",
        "                T.ColorJitter(\n",
        "                    brightness=self.spatial_aug_config['color_jitter']['brightness'],\n",
        "                    contrast=self.spatial_aug_config['color_jitter']['contrast'],\n",
        "                    saturation=self.spatial_aug_config['color_jitter']['saturation']\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if self.spatial_aug_config['gaussian_blur']['enabled']:\n",
        "            transform_list.append(\n",
        "                T.GaussianBlur(\n",
        "                    kernel_size=(5, 5),\n",
        "                    sigma=(0.1, 1.0)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if self.spatial_aug_config['random_rotation']['enabled']:\n",
        "            if random.random() < self.spatial_aug_config['random_rotation']['p']:\n",
        "                transform_list.append(\n",
        "                    T.RandomRotation(\n",
        "                        degrees=self.spatial_aug_config['random_rotation']['degrees'],\n",
        "                        interpolation=T.InterpolationMode.BILINEAR\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        return T.Compose(transform_list)\n",
        "\n",
        "    def apply_temporal_augmentation(self, video_tensor):\n",
        "        \"\"\"Apply temporal augmentations to video tensor\"\"\"\n",
        "        if not any(cfg['enabled'] for cfg in self.temporal_aug_config.values()):\n",
        "            return video_tensor\n",
        "\n",
        "        T, C, H, W = video_tensor.shape\n",
        "\n",
        "        # Temporal crop\n",
        "        if self.temporal_aug_config['temporal_crop']['enabled']:\n",
        "            crop_size = int(T * self.temporal_aug_config['temporal_crop']['crop_size'])\n",
        "            start_idx = random.randint(0, T - crop_size)\n",
        "            video_tensor = video_tensor[start_idx:start_idx + crop_size]\n",
        "\n",
        "        # Temporal masking\n",
        "        if self.temporal_aug_config['temporal_mask']['enabled']:\n",
        "            T = len(video_tensor)\n",
        "            mask_size = int(T * self.temporal_aug_config['temporal_mask']['mask_size'])\n",
        "            for _ in range(self.temporal_aug_config['temporal_mask']['n_masks']):\n",
        "                if random.random() < 0.5:\n",
        "                    start_idx = random.randint(0, T - mask_size)\n",
        "                    video_tensor[start_idx:start_idx + mask_size] = 0\n",
        "\n",
        "        return video_tensor\n",
        "\n",
        "    def __call__(self, video_tensor):\n",
        "        \"\"\"Apply transforms to entire video tensor at once\"\"\"\n",
        "        # Input shape: [T, C, H, W]\n",
        "        # Reshape to [T, C, H, W] -> [1, T, C, H, W] for batch processing\n",
        "        video_tensor = video_tensor.unsqueeze(0)\n",
        "\n",
        "        # Apply spatial transforms to entire video tensor at once\n",
        "        # transforms.v2 will maintain temporal consistency automatically\n",
        "        video_tensor = self.spatial_transforms(video_tensor)\n",
        "\n",
        "        # Remove batch dimension\n",
        "        video_tensor = video_tensor.squeeze(0)\n",
        "\n",
        "        # Apply temporal augmentations\n",
        "        # video_tensor = self.apply_temporal_augmentation(video_tensor)\n",
        "\n",
        "        return video_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import random\n",
        "\n",
        "class YogaVideoDataset(Dataset):\n",
        "    def __init__(self, csv_path, sequence_path, pose_list, video_dir, preprocessed_dir,\n",
        "                 spatial_aug_config=None, temporal_aug_config=None, use_augmentation=True, aug_ratio = 0.5):\n",
        "        with open(sequence_path) as f:\n",
        "            sequence_list = f.read().splitlines()\n",
        "            sequence_list = [int(x) for x in sequence_list]\n",
        "\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.df = self.df[self.df['sequence_id'].isin(sequence_list)]\n",
        "        self.df = self.df[self.df['l3_pose'].isin(pose_list)]\n",
        "\n",
        "        self.pose_to_label = {pose: idx for idx, pose in enumerate(pose_list)}\n",
        "        self.length_of_dataset = len(self.df)\n",
        "\n",
        "        self.video_dir = video_dir\n",
        "        self.preprocessed_dir = preprocessed_dir\n",
        "        os.makedirs(self.preprocessed_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize augmentation pipeline\n",
        "        self.augmentation_pipeline = VideoAugmentationPipeline(\n",
        "            spatial_aug_config=spatial_aug_config,\n",
        "            temporal_aug_config=temporal_aug_config\n",
        "        )\n",
        "\n",
        "        self.cache = dict()\n",
        "        self.augmentation_ratio = aug_ratio\n",
        "        self.use_augmentation = use_augmentation\n",
        "        \n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize((FRAME_HEIGHT, FRAME_WIDTH)),\n",
        "            transforms.ToTensor(),\n",
        "            # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "            #                      std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        self.normalization = transforms.Compose([\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    def __len__(self):\n",
        "        return self.length_of_dataset\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        sequence_id = self.df.iloc[i]['sequence_id']\n",
        "        pose = self.df.iloc[i]['l3_pose']\n",
        "        label = self.pose_to_idx[pose]\n",
        "\n",
        "        # Load preprocessed frames\n",
        "        if sequence_id in self.cache:\n",
        "            frames = self.cache[sequence_id]\n",
        "        else:\n",
        "            file_path = os.path.join(self.preprocessed_dir, f\"{sequence_id}.pt\")\n",
        "            if not os.path.exists(file_path):\n",
        "                video_path = os.path.join(self.video_dir, f\"{sequence_id}.mp4\")\n",
        "                frames = self._get_frames(video_path)\n",
        "                torch.save(frames, file_path)\n",
        "            else:\n",
        "                frames = torch.load(file_path, weights_only=True)\n",
        "            self.cache[sequence_id] = frames\n",
        "\n",
        "        # Choose whether to use augmented or original data\n",
        "        use_augmented_data = self.use_augmentation and self.augmentation_ratio < random.random()\n",
        "        if use_augmented_data:\n",
        "            frames = self.augmentation_pipeline(frames)\n",
        "        \n",
        "        frames = self.normalization(frames)\n",
        "        return frames, label\n",
        "\n",
        "    def _get_frames(self, video_path, sequence_length = SEQUENCE_LENGTH):\n",
        "        reader = imageio.get_reader(video_path, 'ffmpeg')\n",
        "        fps = imageio.get_reader(video_path, 'ffmpeg').get_meta_data()['fps'] // 1\n",
        "        total_frames = reader.count_frames()\n",
        "\n",
        "        indices = np.arange(0, fps - 1, fps // SEQUENCE_LENGTH, dtype=int)\n",
        "\n",
        "        # print(indices)\n",
        "        frames = []\n",
        "        for i, frame in enumerate(reader):\n",
        "            if i % fps in indices:\n",
        "                frame = Image.fromarray(frame)\n",
        "                frame = self.transforms(frame)\n",
        "                frames.append(frame)\n",
        "                # print('Frame number', i % fps, 'Frame', i)\n",
        "\n",
        "        reader.close()\n",
        "        # print('Frame Length', len(frames))\n",
        "        frames = torch.stack(frames)\n",
        "        return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "p8m8Y6qls6qv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    \"\"\"Basic attention mechanism for sequence processing\"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(input_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, lstm_output, mask=None):\n",
        "        att_scores = self.V(torch.tanh(self.W(lstm_output))).squeeze(-1)\n",
        "        \n",
        "        if mask is not None:\n",
        "            att_scores = att_scores.masked_fill(mask == 0, -1e9)\n",
        "            \n",
        "        att_weights = F.softmax(att_scores, dim=1)\n",
        "        context = (lstm_output * att_weights.unsqueeze(-1)).sum(1)\n",
        "        return context, att_weights\n",
        "\n",
        "class CNNLSTM(nn.Module):\n",
        "    \"\"\"Modular video action classifier with various configuration options\"\"\"\n",
        "    def __init__(self, num_classes, \n",
        "                 lstm_hidden_size=512,\n",
        "                 lstm_layers=1,\n",
        "                 dropout=0.5,\n",
        "                 freeze_cnn=True,\n",
        "                 use_attention=False,\n",
        "                 cnn_model='resnet18'):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.cnn_feature_size = 512  # Default for ResNet18\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        # CNN Feature Extractor\n",
        "        self.cnn, self.cnn_feature_size = self._build_cnn(cnn_model)\n",
        "        self._set_cnn_freeze(freeze_cnn)\n",
        "\n",
        "        # Sequence Processing\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.cnn_feature_size,\n",
        "            hidden_size=lstm_hidden_size,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            # bidirectional=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Attention Mechanism\n",
        "        if self.use_attention:\n",
        "            self.attention = AttentionLayer(\n",
        "                input_size=lstm_hidden_size*2, \n",
        "                hidden_size=lstm_hidden_size\n",
        "            )\n",
        "\n",
        "        # Classification Head\n",
        "        self.classifier = self._build_classifier(\n",
        "            lstm_hidden_size*1, \n",
        "            num_classes, \n",
        "            dropout\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _build_cnn(self, model_name):\n",
        "        \"\"\"Initialize CNN feature extractor\"\"\"\n",
        "        if model_name == 'resnet18':\n",
        "            cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "            return nn.Sequential(*list(cnn.children())[:-2]), 512  # Remove avgpool and fc\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported CNN model: {model_name}\")\n",
        "\n",
        "    def _build_classifier(self, input_size, num_classes, dropout):\n",
        "        \"\"\"Build modular classification head\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input_size, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "            \n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights for classification layers\"\"\"\n",
        "        for m in self.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _set_cnn_freeze(self, freeze):\n",
        "        \"\"\"Freeze/unfreeze CNN parameters\"\"\"\n",
        "        for param in self.cnn.parameters():\n",
        "            param.requires_grad = not freeze\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Process frames through CNN\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "        x = x.view(batch_size*seq_len, *x.size()[2:])\n",
        "        x = self.cnn(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1)).view(batch_size, seq_len, -1)\n",
        "\n",
        "        # Process sequence through LSTM\n",
        "        packed_x = rnn_utils.pack_padded_sequence(\n",
        "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "        packed_out, _ = self.lstm(packed_x)\n",
        "        lstm_out, _ = rnn_utils.pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        # Get final representation\n",
        "        if self.use_attention:\n",
        "            mask = self._create_attention_mask(lstm_out.size(1), lengths)\n",
        "            context, _ = self.attention(lstm_out, mask)\n",
        "        else:\n",
        "            # Get last valid time step output\n",
        "            context = lstm_out[torch.arange(batch_size), lengths-1, :]\n",
        "\n",
        "        return self.classifier(context)\n",
        "\n",
        "    def _create_attention_mask(self, max_len, lengths):\n",
        "        \"\"\"Create attention mask from sequence lengths\"\"\"\n",
        "        device = lengths.device\n",
        "        return torch.arange(max_len, device=device).expand(len(lengths), max_len) < lengths.unsqueeze(1)\n",
        "\n",
        "    def unfreeze_cnn_layers(self, num_layers=3, start_from_end=True):\n",
        "        \"\"\"Gradually unfreeze CNN layers for fine-tuning\"\"\"\n",
        "        conv_layers = [m for m in self.cnn.modules() if isinstance(m, nn.Conv2d)]\n",
        "        num_total = len(conv_layers)\n",
        "        \n",
        "        if start_from_end:\n",
        "            layers_to_unfreeze = conv_layers[-num_layers:]\n",
        "        else:\n",
        "            layers_to_unfreeze = conv_layers[:num_layers]\n",
        "            \n",
        "        for layer in layers_to_unfreeze:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def count_parameters(self):\n",
        "        total_params = 0\n",
        "        for name, parameter in self.named_parameters():\n",
        "            if parameter.requires_grad:\n",
        "                params = parameter.numel()\n",
        "                print(f\"{name}: {params}\")\n",
        "                total_params += params\n",
        "        print(f\"Total Trainable Params: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "GFgBfmOSCZC9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "def save_checkpoint(model, optimizer, epoch, history, save_path = SAVE_PATH, best_path=None):\n",
        "    if best_path is not None:\n",
        "        chk_path = os.path.join(save_path, f'best_model.pth')\n",
        "        print(f\"Saving checkpoint to {chk_path}\")\n",
        "    else:\n",
        "        chk_path = os.path.join(save_path, f'checkpath_model.pth')\n",
        "        print(f\"Saving checkpoint to {chk_path}\")\n",
        "\n",
        "    # Combine model, optimizer, and history into one dictionary\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,  # Save the next epoch number for resuming\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'history': history  # Save history along with the model and optimizer\n",
        "    }\n",
        "\n",
        "    # Save everything in a single file using torch.save\n",
        "    torch.save(checkpoint, chk_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch + 1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "mZprkJQvCZC9"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Load model and training state from a checkpoint\n",
        "    \"\"\"\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, weights_only = False)\n",
        "\n",
        "    # Load model and optimizer states\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Get the epoch number to resume from\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "    # Load training history with new metrics\n",
        "    history = checkpoint.get('history', {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': [],\n",
        "        'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
        "        'val_precision': [], 'val_recall': [], 'val_f1': [],\n",
        "        'learning_rates': []\n",
        "    })\n",
        "\n",
        "    return model, optimizer, start_epoch, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "dkDP9PAlCZC9"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        # on default = 7 successive val_loss increase stop\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "g8OLd0ZqOHVS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_per_class_metric(precision, recall, f1, epoch=0, pose_list=None, master_save_path=SAVE_PATH):\n",
        "    \"\"\"\n",
        "    Plots and saves per-class metrics (Precision, Recall, F1-score) for the given epoch.\n",
        "\n",
        "    Args:\n",
        "        precision (list): Per-class precision values.\n",
        "        recall (list): Per-class recall values.\n",
        "        f1 (list): Per-class F1-score values.\n",
        "        epoch (int): Current epoch number.\n",
        "        pose_list (list): List of class names (strings) corresponding to class indices.\n",
        "        master_save_path (str): Directory to save the plot.\n",
        "    \"\"\"\n",
        "    if pose_list is None:\n",
        "        pose_list = [str(i) for i in range(len(precision))]  # Default to numeric labels\n",
        "\n",
        "    save_dir = os.path.join(master_save_path, 'per_class_metric')\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    save_file_path = os.path.join(save_dir, f'per_class_metric_epoch_{epoch}.png')\n",
        "\n",
        "    # Adjust x-axis positions for grouped bars\n",
        "    x = range(len(pose_list))\n",
        "\n",
        "    plt.bar([i - 0.2 for i in x], precision, width=0.2, label='Precision', align='center')\n",
        "    plt.bar(x, recall, width=0.2, label='Recall', align='center')\n",
        "    plt.bar([i + 0.2 for i in x], f1, width=0.2, label='F1-Score', align='center')\n",
        "\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(f'Per-Class Metrics - Epoch {epoch}')\n",
        "    plt.xticks(x, pose_list, rotation=45, ha='right')  # Use pose_list for x-axis labels\n",
        "    plt.legend()\n",
        "    plt.tight_layout()  # Adjust layout to fit rotated labels\n",
        "    plt.savefig(save_file_path)\n",
        "    plt.show()\n",
        "\n",
        "def per_class_metric(true, pred, epoch, pose_list=pose_list, average=None):\n",
        "    \"\"\"\n",
        "    Computes and logs per-class metrics (Precision, Recall, F1-score).\n",
        "\n",
        "    Args:\n",
        "        true (list): Ground-truth labels.\n",
        "        pred (list): Predicted labels.\n",
        "        epoch (int): Current epoch number.\n",
        "        pose_list (list): List of class names (strings) corresponding to class indices.\n",
        "        average (str or None): Averaging method for sklearn metrics (None for per-class).\n",
        "    \"\"\"\n",
        "    per_class_precision = precision_score(true, pred, average=average, zero_division=0)\n",
        "    per_class_recall = recall_score(true, pred, average=average, zero_division=0)\n",
        "    per_class_f1 = f1_score(true, pred, average=average, zero_division=0)\n",
        "\n",
        "    print(f\"Per-Class Metrics for Epoch {epoch}:\")\n",
        "    for i, (prec, rec, f1) in enumerate(zip(per_class_precision, per_class_recall, per_class_f1)):\n",
        "        class_name = pose_list[i] if pose_list else f\"Class {i}\"\n",
        "        print(f\"{class_name}: Precision={prec:.2f}, Recall={rec:.2f}, F1-Score={f1:.2f}\")\n",
        "\n",
        "    plot_per_class_metric(per_class_precision, per_class_recall, per_class_f1, epoch, pose_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nNTAFuncCZC9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, dataset,fold, num_epochs=50, patience=18, log_interval=10, checkpoint_path=None, unfreeze_epoch=5, num_layers_unfreeze=3):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Initialize scheduler\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "    # Initialize early stopping\n",
        "    early_stopping = EarlyStopping(patience=patience, min_delta=1e-4)\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Initialize history for loss, accuracy, precision, recall, and F1\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': [],\n",
        "        'train_precision': [], 'train_recall': [], 'train_f1': [],\n",
        "        'val_precision': [], 'val_recall': [], 'val_f1': [],\n",
        "        'learning_rates': []\n",
        "    }\n",
        "\n",
        "    # Check for checkpoint and load if available\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        model, optimizer, start_epoch, history = load_checkpoint(model, optimizer, checkpoint_path)\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        dataset.use_augmentation = True #############\n",
        "        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        history['learning_rates'].append(current_lr)\n",
        "        print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
        "        train_true, train_pred = [], []\n",
        "\n",
        "        train_loader_tqdm = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n",
        "        for batch_idx, (inputs, labels) in train_loader_tqdm:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "            # Collect true and predicted labels for precision/recall\n",
        "            train_true.extend(labels.cpu().numpy())\n",
        "            train_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "            # Log batch-level updates\n",
        "            if batch_idx % log_interval == 0:\n",
        "                train_loader_tqdm.set_postfix({\n",
        "                    'loss': train_loss / (BATCH_SIZE * (batch_idx + 1)),\n",
        "                    'accuracy': 100.0 * train_correct / train_total\n",
        "                })\n",
        "\n",
        "        # Calculate training metrics\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = 100.0 * train_correct / train_total\n",
        "        train_precision = precision_score(train_true, train_pred, average='macro')\n",
        "        train_recall = recall_score(train_true, train_pred, average='macro')\n",
        "        train_f1 = f1_score(train_true, train_pred, average='macro')\n",
        "\n",
        "        per_class_metric(train_true, train_pred, epoch)\n",
        "\n",
        "        dataset.use_augmentation = False\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        val_true, val_pred = [], []\n",
        "\n",
        "        val_loader_tqdm = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, labels) in val_loader_tqdm:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)  # Pass sequence lengths to model forward function\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "                # Collect true and predicted labels for precision/recall\n",
        "                val_true.extend(labels.cpu().numpy())\n",
        "                val_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "                # Log batch-level updates for validation\n",
        "                if batch_idx % log_interval == 0:\n",
        "                    val_loader_tqdm.set_postfix({\n",
        "                        'loss': val_loss / (BATCH_SIZE * (batch_idx + 1)),\n",
        "                        'accuracy': 100.0 * val_correct / val_total\n",
        "                    })\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = 100.0 * val_correct / val_total\n",
        "        val_precision = precision_score(val_true, val_pred, average='macro')\n",
        "        val_recall = recall_score(val_true, val_pred, average='macro')\n",
        "        val_f1 = f1_score(val_true, val_pred, average='macro')\n",
        "        # Per-class metrics for validation\n",
        "        per_class_metric(val_true, val_pred, epoch)\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['train_precision'].append(train_precision)\n",
        "        history['train_recall'].append(train_recall)\n",
        "        history['train_f1'].append(train_f1)\n",
        "        history['val_precision'].append(val_precision)\n",
        "        history['val_recall'].append(val_recall)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Print metrics at the end of the epoch\n",
        "        print(f'\\nEpoch {epoch + 1}/{num_epochs} Summary:')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Train Precision: {train_precision:.2f} | Train Recall: {train_recall:.2f} | Train F1: {train_f1:.2f}')\n",
        "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Val Precision: {val_precision:.2f} | Val Recall: {val_recall:.2f} | Val F1: {val_f1:.2f}')\n",
        "\n",
        "        # Save the best model checkpoint\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_path = os.path.join(SAVE_PATH, f'best_model_fold_{fold}.pth')\n",
        "            if best_model_path is not None:\n",
        "                print(best_model_path)\n",
        "            save_checkpoint(model, optimizer, epoch, history, SAVE_PATH, best_model_path)\n",
        "            print(f\"New best model saved! Validation Loss: {best_val_loss:.4f}\")\n",
        "\n",
        "        # Adjust learning rate based on validation loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Check for early stopping\n",
        "        # early_stopping(val_loss)\n",
        "        # if early_stopping.early_stop:\n",
        "        #     print(\"Early stopping triggered\")\n",
        "        #     break\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "8MayBbiF0gtk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, class_names, spath=SAVE_PATH, fsave='confusion_matrix.png'):\n",
        "    \"\"\"\n",
        "    Evaluate model on test set\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        test_loader: DataLoader for test data\n",
        "        criterion: Loss function\n",
        "        class_names: List of class names\n",
        "        spath: Directory to save the plot\n",
        "        fsave: Filename for confusion matrix plot\n",
        "\n",
        "    Returns:\n",
        "        test_loss: Average test loss\n",
        "        accuracy: Test accuracy\n",
        "        precision: Macro precision score\n",
        "        recall: Macro recall score\n",
        "        f1: Macro F1 score\n",
        "    \"\"\"\n",
        "    csave = os.path.join(spath, fsave)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    test_loader_tqdm = tqdm(test_loader, desc=\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader_tqdm:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            test_total += labels.size(0)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "    accuracy = 100 * test_correct / test_total\n",
        "\n",
        "    # Calculate metrics with macro averaging\n",
        "    precision = precision_score(all_labels, all_predictions, average='macro')\n",
        "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "   # Create figure with adjusted size and subplot positioning\n",
        "    plt.figure(figsize=(12, 10))  # Increased figure size\n",
        "    plt.subplot(111, position=[0.1, 0.2, 0.8, 0.7])  # Adjust main plot position to leave room for text\n",
        "\n",
        "    # Create heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Prepare metrics text\n",
        "    metrics_text = (f'Test Loss: {test_loss:.4f}\\n'\n",
        "                    f'Test Accuracy: {accuracy:.2f}%\\n'\n",
        "                    f'Precision (Macro): {precision:.4f}\\n'\n",
        "                    f'Recall (Macro): {recall:.4f}\\n'\n",
        "                    f'F1 Score (Macro): {f1:.4f}')\n",
        "\n",
        "    # Add metrics text with adjusted position\n",
        "    plt.gcf().text(0, 0.02, metrics_text, fontsize=12, ha='left', va='bottom')\n",
        "\n",
        "    # Adjust layout to prevent cutting off\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.2)  # Leave space at bottom for metrics\n",
        "\n",
        "    # Save with tight layout to include all elements\n",
        "    plt.savefig(csave, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "    print(f'Precision (Macro): {precision:.4f}')\n",
        "    print(f'Recall (Macro): {recall:.4f}')\n",
        "    print(f'F1 Score (Macro): {f1:.4f}')\n",
        "\n",
        "    return test_loss, accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    frames, labels = zip(*batch)  # Separate frames and labels\n",
        "    # FRAMES == DATA same thing shape (N, T, C, H, W)\n",
        "    # Pad the sequences of frames for each video in the batch along the sequence dimension\n",
        "    frames_padded = pad_sequence(frames, batch_first=True, padding_value=0)  # Shape: [batch_size, max_seq_len, 3, 224, 224]\n",
        "    lengths = torch.tensor([len(seq) for seq in frames])  # Record original sequence lengths\n",
        "\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return frames_padded, labels, lengths  # Return lengths for packing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "7EWkYX8sCZC-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "def train_test_split(dataset, test_split = TEST_SPLIT):\n",
        "    total_size = len(dataset)\n",
        "    test_size = int(test_split * total_size)\n",
        "    train_size = total_size - test_size\n",
        "\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        dataset,\n",
        "        [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
        "    )\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "def create_data_loaders(train_dataset, test_dataset, batch_size = BATCH_SIZE):\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "-nVQoc_DCZC-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_training_curves(history, fold):\n",
        "    fsave=f'training_curves_fold_{fold}.png'\n",
        "    tsave = os.path.join(SAVE_PATH, fsave)\n",
        "    # plt.style.use('seaborn')\n",
        "    fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "    # Loss curves\n",
        "    axs[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "    axs[0, 0].plot(history['val_loss'], label='Validation Loss', marker='o')\n",
        "    axs[0, 0].set_title('Loss')\n",
        "    axs[0, 0].legend()\n",
        "\n",
        "    # Accuracy curves\n",
        "    axs[0, 1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "    axs[0, 1].plot(history['val_acc'], label='Validation Accuracy', marker='o')\n",
        "    axs[0, 1].set_title('Accuracy')\n",
        "    axs[0, 1].legend()\n",
        "\n",
        "    # Learning rate\n",
        "    axs[0, 2].plot(history['learning_rates'], label='Learning Rate', marker='o')\n",
        "    axs[0, 2].set_title('Learning Rate')\n",
        "    axs[0, 2].set_yscale('log')\n",
        "    axs[0, 2].legend()\n",
        "\n",
        "    # Precision\n",
        "    axs[1, 0].plot(history['train_precision'], label='Train Precision', marker='o')\n",
        "    axs[1, 0].plot(history['val_precision'], label='Validation Precision', marker='o')\n",
        "    axs[1, 0].set_title('Precision')\n",
        "    axs[1, 0].legend()\n",
        "\n",
        "    # Recall\n",
        "    axs[1, 1].plot(history['train_recall'], label='Train Recall', marker='o')\n",
        "    axs[1, 1].plot(history['val_recall'], label='Validation Recall', marker='o')\n",
        "    axs[1, 1].set_title('Recall')\n",
        "    axs[1, 1].legend()\n",
        "\n",
        "    # F1 Score\n",
        "    axs[1, 2].plot(history['train_f1'], label='Train F1', marker='o')\n",
        "    axs[1, 2].plot(history['val_f1'], label='Validation F1', marker='o')\n",
        "    axs[1, 2].set_title('F1 Score')\n",
        "    axs[1, 2].legend()\n",
        "\n",
        "    for ax in axs.flat:\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(tsave)\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7LXXcX28u5Ar"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from collections import Counter\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "def get_class_weights(pose_list, num_classes, strategy='balanced'):\n",
        "    \"\"\"\n",
        "    Calculate class weights with proper class index alignment.\n",
        "\n",
        "    Args:\n",
        "        pose_list: List of labels/poses in the dataset\n",
        "        num_classes: Total number of classes\n",
        "        strategy: Weighting strategy ('balanced', 'inverse', 'effective_samples', 'sqrt_inverse')\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Class weights tensor aligned with class indices\n",
        "    \"\"\"\n",
        "    # Count samples per class\n",
        "    class_counts = Counter(pose_list)\n",
        "    total_samples = len(pose_list)\n",
        "\n",
        "    # Initialize weights array with zeros for all possible classes\n",
        "    weights = np.zeros(num_classes)\n",
        "\n",
        "    if strategy == 'balanced':\n",
        "        # Use sklearn's balanced weighting\n",
        "        unique_classes = sorted(class_counts.keys())\n",
        "        sklearn_weights = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=np.array(unique_classes),\n",
        "            y=pose_list\n",
        "        )\n",
        "        # Map weights to correct indices\n",
        "        for idx, class_label in enumerate(unique_classes):\n",
        "            weights[class_label] = sklearn_weights[idx]\n",
        "\n",
        "    elif strategy == 'inverse':\n",
        "        # Inverse of sample frequency\n",
        "        for class_label, count in class_counts.items():\n",
        "            weights[class_label] = total_samples / (num_classes * count)\n",
        "\n",
        "    elif strategy == 'effective_samples':\n",
        "        # Effective number of samples weighting\n",
        "        beta = 0.9999\n",
        "        for class_label, count in class_counts.items():\n",
        "            weights[class_label] = (1 - beta) / (1 - beta ** count)\n",
        "\n",
        "    elif strategy == 'sqrt_inverse':\n",
        "        # Square root of inverse frequency\n",
        "        for class_label, count in class_counts.items():\n",
        "            weights[class_label] = np.sqrt(total_samples / (num_classes * count))\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown weighting strategy: {strategy}\")\n",
        "\n",
        "    # Convert to tensor and normalize\n",
        "    weights = torch.FloatTensor(weights)\n",
        "    weights = weights / weights.sum() * len(weights)\n",
        "\n",
        "    return weights\n",
        "\n",
        "def create_weighted_criterion(pose_list, num_classes, strategy='balanced'):\n",
        "    \"\"\"\n",
        "    Create a weighted CrossEntropyLoss criterion.\n",
        "\n",
        "    Args:\n",
        "        pose_list: List of labels/poses in the dataset\n",
        "        num_classes: Total number of classes\n",
        "        strategy: Weighting strategy for calculating class weights\n",
        "\n",
        "    Returns:\n",
        "        nn.CrossEntropyLoss: Weighted loss criterion\n",
        "    \"\"\"\n",
        "    weights = get_class_weights(pose_list, num_classes, strategy)\n",
        "    print(\"Class weights aligned with indices:\", weights)\n",
        "    if torch.cuda.is_available():\n",
        "        weights = weights.cuda()\n",
        "    return nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "def analyze_class_distribution(pose_list):\n",
        "    \"\"\"\n",
        "    Analyze and print class distribution information.\n",
        "\n",
        "    Args:\n",
        "        pose_list: List of labels/poses in the dataset\n",
        "    \"\"\"\n",
        "    class_counts = Counter(pose_list)\n",
        "    total_samples = len(pose_list)\n",
        "\n",
        "    print(\"\\nClass Distribution Analysis:\")\n",
        "    print(\"-\" * 50)\n",
        "    for class_idx, count in sorted(class_counts.items()):\n",
        "        percentage = (count / total_samples) * 100\n",
        "        print(f\"Class {class_idx}: {count} samples ({percentage:.2f}%)\")\n",
        "\n",
        "    # Calculate imbalance metrics\n",
        "    max_count = max(class_counts.values())\n",
        "    min_count = min(class_counts.values())\n",
        "    imbalance_ratio = max_count / min_count\n",
        "\n",
        "    print(\"\\nImbalance Statistics:\")\n",
        "    print(f\"Imbalance Ratio (max/min): {imbalance_ratio:.2f}\")\n",
        "    print(f\"Maximum class size: {max_count}\")\n",
        "    print(f\"Minimum class size: {min_count}\")\n",
        "    print(f\"Average class size: {total_samples/len(class_counts):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "fqt2Ho_GOHVT"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "def create_data_loaders(train_idx, val_idx, dataset, batch_size=BATCH_SIZE):\n",
        "    # Create subsets for this fold\n",
        "    train_subset = Subset(dataset, train_idx)\n",
        "    val_subset = Subset(dataset, val_idx)\n",
        "\n",
        "    # Create data loaders with optional collate_fn\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_subset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "RcWrvy4ZNiYj"
      },
      "outputs": [],
      "source": [
        "def compute_average_history(histories):\n",
        "    \"\"\"\n",
        "    Compute the average history across multiple folds.\n",
        "\n",
        "    Args:\n",
        "        histories (list of dict): List of history dictionaries from all folds. Each dictionary contains\n",
        "                                  metrics like 'train_loss', 'val_loss', 'train_accuracy', etc., as lists.\n",
        "\n",
        "    Returns:\n",
        "        dict: Averaged history containing the same keys as the input histories.\n",
        "    \"\"\"\n",
        "    avg_history = {}\n",
        "    num_folds = len(histories)\n",
        "\n",
        "    for key in histories[0]:  # Iterate over metric names\n",
        "        # Initialize a list for each metric\n",
        "        avg_history[key] = [0.0] * len(histories[0][key])  # Assume all folds have same length histories\n",
        "\n",
        "        # Sum across all folds\n",
        "        for fold_history in histories:\n",
        "            for i, value in enumerate(fold_history[key]):\n",
        "                avg_history[key][i] += value\n",
        "\n",
        "        # Divide by the number of folds to compute the average\n",
        "        avg_history[key] = [val / num_folds for val in avg_history[key]]\n",
        "\n",
        "    return avg_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gX9YM1zJNj3A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def save_folds(meta_info_path, skf, folds, architecture=ARCHITECTURE):\n",
        "    if architecture == '2D_CNN_LSTM':\n",
        "        folds_path = os.path.join(meta_info_path, f\"{architecture}_folds.pkl\")\n",
        "        skf_path = os.path.join(meta_info_path, f\"{architecture}_skf.pkl\")\n",
        "    else:\n",
        "        folds_path = os.path.join(meta_info_path, \"folds.pkl\")\n",
        "        skf_path = os.path.join(meta_info_path, \"skf.pkl\")\n",
        "\n",
        "    with open(folds_path, \"wb\") as f:\n",
        "        pickle.dump(folds, f)\n",
        "\n",
        "    with open(skf_path, \"wb\") as f:\n",
        "        pickle.dump(skf, f)\n",
        "\n",
        "def load_folds(meta_info_path, architecture=ARCHITECTURE):\n",
        "    if architecture == '2D_CNN_LSTM':\n",
        "        folds_path = os.path.join(meta_info_path, f\"{architecture}_folds.pkl\")\n",
        "        skf_path = os.path.join(meta_info_path, f\"{architecture}_skf.pkl\")\n",
        "    else:\n",
        "        folds_path = os.path.join(meta_info_path, \"folds.pkl\")\n",
        "        skf_path = os.path.join(meta_info_path, \"skf.pkl\")\n",
        "\n",
        "    if os.path.exists(folds_path) and os.path.exists(skf_path):\n",
        "        with open(folds_path, \"rb\") as f:\n",
        "            folds = pickle.load(f)\n",
        "        with open(skf_path, \"rb\") as f:\n",
        "            skf = pickle.load(f)\n",
        "        return skf, folds\n",
        "    return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Vn5XpTqG6_6J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "def get_weighted_loss_criterion(train_dataset, train_indices=None, meta_info_path=None, fold=None):\n",
        "    \"\"\"\n",
        "    Get or compute weighted loss criterion, with caching support.\n",
        "\n",
        "    Args:\n",
        "        train_dataset: The training dataset\n",
        "        train_indices: Optional indices for cross-validation fold\n",
        "        meta_info_path: Path to save/load cached criteria\n",
        "        fold: Current fold number (required if using caching)\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.CrossEntropyLoss with computed weights\n",
        "    \"\"\"\n",
        "    if meta_info_path and fold is not None:\n",
        "        criterion_cache_path = os.path.join(meta_info_path, f'criterion_fold_{fold}.pkl')\n",
        "\n",
        "        # Try to load cached criterion\n",
        "        if os.path.exists(criterion_cache_path):\n",
        "            try:\n",
        "                with open(criterion_cache_path, 'rb') as f:\n",
        "                    cached_data = pickle.load(f)\n",
        "\n",
        "                # Verify the cached criterion matches current data\n",
        "                if verify_criterion_cache(cached_data, train_indices):\n",
        "                    print(f\"Loading cached criterion for fold {fold}\")\n",
        "                    return cached_data['criterion']\n",
        "                else:\n",
        "                    print(f\"Cached criterion for fold {fold} is invalid, recomputing...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading cached criterion: {e}, recomputing...\")\n",
        "\n",
        "    # Compute criterion if no cache exists or verification failed\n",
        "    if train_indices is not None:\n",
        "        labels = [train_dataset[i][1] for i in train_indices]\n",
        "    else:\n",
        "        labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
        "\n",
        "    analyze_class_distribution(labels)\n",
        "    criterion = create_weighted_criterion(\n",
        "        labels,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        strategy='effective_samples'\n",
        "    )\n",
        "\n",
        "    # Cache the computed criterion if path is provided\n",
        "    if meta_info_path and fold is not None:\n",
        "        os.makedirs(meta_info_path, exist_ok=True)\n",
        "        cache_data = {\n",
        "            'criterion': criterion,\n",
        "            'train_indices': train_indices,\n",
        "            'fold': fold\n",
        "        }\n",
        "        with open(criterion_cache_path, 'wb') as f:\n",
        "            pickle.dump(cache_data, f)\n",
        "        print(f\"Cached criterion for fold {fold}\")\n",
        "\n",
        "    return criterion\n",
        "\n",
        "def verify_criterion_cache(cached_data, current_train_indices):\n",
        "    \"\"\"\n",
        "    Verify that cached criterion matches current training indices.\n",
        "\n",
        "    Args:\n",
        "        cached_data: Dictionary containing cached criterion and metadata\n",
        "        current_train_indices: Current training indices to verify against\n",
        "\n",
        "    Returns:\n",
        "        bool: True if cache is valid, False otherwise\n",
        "    \"\"\"\n",
        "    cached_indices = cached_data['train_indices']\n",
        "    if cached_indices is None and current_train_indices is None:\n",
        "        return True\n",
        "    if cached_indices is None or current_train_indices is None:\n",
        "        return False\n",
        "    return len(cached_indices) == len(current_train_indices) and all(\n",
        "        a == b for a, b in zip(sorted(cached_indices), sorted(current_train_indices))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "_G8_YTgfNq3R"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def main(lstm_hidden_size= LSTM_HIDDEN_SIZE,lstm_layers = LSTM_LAYERS, num_classes=NUM_CLASSES,LR=LEARNING_RATE, Epochs=50):\n",
        "    spatial_aug_config = {\n",
        "        'gaussian_blur': 0.5,\n",
        "        'random_horizontal_flip':  0.5,\n",
        "        'color_jitter': 0.5,\n",
        "        'random_rotation': 0.5\n",
        "    }\n",
        "\n",
        "    # temporal_aug_config = {\n",
        "    #     'temporal_crop': {'enabled': True, 'crop_size': 0.8},\n",
        "    #     'temporal_mask': {'enabled': True, 'mask_size': 0.2, 'n_masks': 2}\n",
        "    # }\n",
        "    dataset, train_dataset, test_dataset = prepare_dataset(spatial_aug_config=spatial_aug_config, temporal_aug_config=None)\n",
        "    skf, folds = load_folds(meta_info_path, architecture=ARCHITECTURE)\n",
        "\n",
        "    if folds is None:\n",
        "        k_folds = 5\n",
        "        skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "        folds = list(skf.split(np.arange(len(train_dataset)), get_all_the_labels(train_dataset)))\n",
        "        save_folds(meta_info_path, skf, folds,architecture=ARCHITECTURE)\n",
        "\n",
        "    best_model = None\n",
        "    best_val_loss = float('inf')\n",
        "    best_fold = 0\n",
        "    all_metrics = []\n",
        "\n",
        "    print('Starting cross-validation...')\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(folds):\n",
        "        print(f\"Fold {fold + 1}/{len(folds)}\")\n",
        "        train_loader, val_loader = create_data_loaders(train_idx, val_idx, train_dataset)\n",
        "\n",
        "        criterion = get_weighted_loss_criterion(\n",
        "            train_dataset,\n",
        "            train_idx,\n",
        "            meta_info_path=meta_info_path,\n",
        "            fold=fold\n",
        "        )\n",
        "        # model = CNNLSTM(in_channels, hidden_channels, num_classes)\n",
        "        model = CNNLSTM(num_classes=num_classes, lstm_hidden_size=lstm_hidden_size, lstm_layers=lstm_layers, dropout=DROPOUT)\n",
        "        # optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "\n",
        "        model, history = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            criterion,\n",
        "            optimizer,\n",
        "            dataset,\n",
        "            fold=fold,\n",
        "            num_epochs=Epochs,\n",
        "            patience=10,\n",
        "            log_interval=1,\n",
        "            checkpoint_path=None\n",
        "        )\n",
        "        plot_training_curves(history, fold)\n",
        "        all_metrics.append(history)\n",
        "\n",
        "        if history['val_loss'][-1] < best_val_loss:\n",
        "            best_val_loss = history['val_loss'][-1]\n",
        "            best_model = model\n",
        "            best_fold = fold\n",
        "\n",
        "    avg_history = compute_average_history(all_metrics)\n",
        "    plot_training_curves(avg_history, 'average')\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    evaluate_model(best_model, test_loader, criterion, pose_list)\n",
        "\n",
        "    model_save_path = os.path.join(SAVE_PATH, f'my_model_{best_fold}.pth')\n",
        "    torch.save(best_model.state_dict(), model_save_path)\n",
        "\n",
        "def prepare_dataset(spatial_aug_config=None, temporal_aug_config=None):\n",
        "    dataset = YogaVideoDataset(csv_path, sequence_path, pose_list, video_dir, preprocessed_dir, spatial_aug_config=spatial_aug_config, temporal_aug_config=temporal_aug_config)\n",
        "    train_dataset, test_dataset = train_test_split(dataset)\n",
        "    return dataset,train_dataset,test_dataset\n",
        "\n",
        "def get_all_the_labels(dataset):\n",
        "    return [label for _, label in dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GpaMPADECZC-",
        "outputId": "233dafd3-5067-40b0-bb3f-74018804c9fa"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "zNhSc5nau5Ar"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "\n",
        "def find_sus(model, test_loader, criterion, class_names, spath=SAVE_PATH, fsave='confusion_matrix_all.png'):\n",
        "    \"\"\"\n",
        "    Evaluate model on test set\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        test_loader: DataLoader for test data\n",
        "        criterion: Loss function\n",
        "        class_names: List of class names\n",
        "        save_path: Directory to save the plot\n",
        "        fsave: Filename for confusion matrix plot\n",
        "    \"\"\"\n",
        "    csave = os.path.join(spath, fsave)\n",
        "    sussave = os.path.join(spath, 'sus.csv')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    misclassified_data = []\n",
        "\n",
        "    # Create progress bar\n",
        "    test_loader_tqdm = tqdm(test_loader, desc=\"Testing\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(test_loader_tqdm):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            # Calculate accuracy\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            test_total += labels.size(0)\n",
        "\n",
        "            # Store predictions and labels for confusion matrix\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Check if the sample is misclassified\n",
        "            misclassified_indices = (predicted != labels).nonzero(as_tuple=True)[0]\n",
        "            for misclassified_idx in misclassified_indices:\n",
        "                global_idx = batch_idx * inputs.size(0) + misclassified_idx.item()\n",
        "                sequence_id = test_loader.dataset.idx_to_seq[global_idx]\n",
        "                correct_label = class_names[labels[misclassified_idx].item()]\n",
        "                prediction = class_names[predicted[misclassified_idx].item()]\n",
        "                misclassified_data.append([sequence_id, correct_label, prediction])\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "    accuracy = 100 * test_correct / test_total\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=pose_list, yticklabels=pose_list)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(csave)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Save misclassified data to CSV\n",
        "    with open(sussave, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['sequence_id', 'correct_label', 'prediction'])\n",
        "        writer.writerows(misclassified_data)\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    return test_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "60euC5Fmu5As",
        "outputId": "dcc9b6be-c237-4209-bb91-7d2414b8b48b"
      },
      "outputs": [],
      "source": [
        "dataset, train_dataset, test_dataset = prepare_dataset()\n",
        "label_to_pose = {v:k for k,v in dataset.pose_to_label.items()}\n",
        "\n",
        "model = CNNLSTM(num_classes=NUM_CLASSES, lstm_hidden_size=LSTM_HIDDEN_SIZE, lstm_layers=LSTM_LAYERS, dropout=DROPOUT)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "all_labels = [dataset[i][1] for i in range(len(dataset))]\n",
        "criterion = create_weighted_criterion(\n",
        "        all_labels,\n",
        "        num_classes= NUM_CLASSES,\n",
        "        strategy='inverse'  # Try different strategies\n",
        "    )\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint_path = os.path.join(SAVE_PATH, 'best_model.pth')\n",
        "# Plot the training curves\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        model, optimizer, start_epoch, history = load_checkpoint(\n",
        "            model, optimizer, checkpoint_path\n",
        "        )\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "# plot_training_curves(history)\n",
        "find_sus(model, loader, criterion, label_to_pose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1dkqFDp2y5HK",
        "outputId": "19c35ac4-9b11-433c-a0ec-79b68083dccf"
      },
      "outputs": [],
      "source": [
        "dataset, train_dataset, test_dataset = prepare_dataset()\n",
        "\n",
        "model = CNNLSTM(num_classes=NUM_CLASSES, lstm_hidden_size=LSTM_HIDDEN_SIZE, lstm_layers=LSTM_LAYERS, dropout=DROPOUT)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint_path = os.path.join(SAVE_PATH, 'best_model.pth')\n",
        "# Plot the training curves\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        model, optimizer, start_epoch, history = load_checkpoint(\n",
        "            model, optimizer, checkpoint_path\n",
        "        )\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "evaluate_model(model, test_loader, criterion, pose_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "g6ITIMLZr5zR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
