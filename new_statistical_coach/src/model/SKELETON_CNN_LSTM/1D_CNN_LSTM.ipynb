{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 3, 20, 33])\n",
      "Output shape: torch.Size([32, 13])\n",
      "batch_norm.weight: 99\n",
      "batch_norm.bias: 99\n",
      "conv1.0.weight: 288\n",
      "conv1.0.bias: 32\n",
      "conv1.1.weight: 32\n",
      "conv1.1.bias: 32\n",
      "blk1.conv1.weight: 6144\n",
      "blk1.conv1.bias: 64\n",
      "blk1.bn1.weight: 64\n",
      "blk1.bn1.bias: 64\n",
      "blk1.conv2.weight: 12288\n",
      "blk1.conv2.bias: 64\n",
      "blk1.bn2.weight: 64\n",
      "blk1.bn2.bias: 64\n",
      "blk1.shortcut.0.weight: 2048\n",
      "blk1.shortcut.0.bias: 64\n",
      "blk1.shortcut.1.weight: 64\n",
      "blk1.shortcut.1.bias: 64\n",
      "blk2.conv1.weight: 24576\n",
      "blk2.conv1.bias: 128\n",
      "blk2.bn1.weight: 128\n",
      "blk2.bn1.bias: 128\n",
      "blk2.conv2.weight: 49152\n",
      "blk2.conv2.bias: 128\n",
      "blk2.bn2.weight: 128\n",
      "blk2.bn2.bias: 128\n",
      "blk2.shortcut.0.weight: 8192\n",
      "blk2.shortcut.0.bias: 128\n",
      "blk2.shortcut.1.weight: 128\n",
      "blk2.shortcut.1.bias: 128\n",
      "blk3.conv1.weight: 98304\n",
      "blk3.conv1.bias: 256\n",
      "blk3.bn1.weight: 256\n",
      "blk3.bn1.bias: 256\n",
      "blk3.conv2.weight: 196608\n",
      "blk3.conv2.bias: 256\n",
      "blk3.bn2.weight: 256\n",
      "blk3.bn2.bias: 256\n",
      "blk3.shortcut.0.weight: 32768\n",
      "blk3.shortcut.0.bias: 256\n",
      "blk3.shortcut.1.weight: 256\n",
      "blk3.shortcut.1.bias: 256\n",
      "blk4.conv1.weight: 393216\n",
      "blk4.conv1.bias: 512\n",
      "blk4.bn1.weight: 512\n",
      "blk4.bn1.bias: 512\n",
      "blk4.conv2.weight: 786432\n",
      "blk4.conv2.bias: 512\n",
      "blk4.bn2.weight: 512\n",
      "blk4.bn2.bias: 512\n",
      "blk4.shortcut.0.weight: 131072\n",
      "blk4.shortcut.0.bias: 512\n",
      "blk4.shortcut.1.weight: 512\n",
      "blk4.shortcut.1.bias: 512\n",
      "lstm.weight_ih_l0: 1572864\n",
      "lstm.weight_hh_l0: 262144\n",
      "lstm.bias_ih_l0: 1024\n",
      "lstm.bias_hh_l0: 1024\n",
      "fc.weight: 3328\n",
      "fc.bias: 13\n",
      "Total Trainable Params: 3590419\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3,\n",
    "                              padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, input_channels,hidden_size=256, num_classes=NUM_CLASSES, num_nodes=33):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(input_channels * num_nodes)\n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.blk1 = ResBlock(32, 64, stride=2)\n",
    "        self.blk2 = ResBlock(64, 128, stride=2)\n",
    "        self.blk3 = ResBlock(128, 256, stride=2)\n",
    "        self.blk4 = ResBlock(256, 512, stride=2)\n",
    "\n",
    "        self.V_downsampled = 3\n",
    "        self.lstm = nn.LSTM(512 * self.V_downsampled, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, T, V = x.size()\n",
    "        \n",
    "        # BATCH NORM\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        x = x.view(N, V * C, T)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.view(N, V, C, T).permute(0, 2, 3, 1).contiguous()\n",
    "        \n",
    "        # CNN LAYER APPLICATION\n",
    "        x = x.view(N*T, C, V)\n",
    "        x = self.conv1(x)\n",
    "        x = self.blk1(x)\n",
    "        x = self.blk2(x)\n",
    "        x = self.blk3(x)\n",
    "        x = self.blk4(x)\n",
    "        \n",
    "        # Current shape: (N*T, 512, V_downsampled)\n",
    "        _, C_out, V_down = x.size()\n",
    "        x = x.view(N, T, C_out, V_down)\n",
    "        \n",
    "        # no interleaving (x0,y0,z0, x1,y1,z1,..) instead (x0,x1,..x32, y1, y2,...)\n",
    "        x = x.view(N, T, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    def count_parameters(self):\n",
    "        total_params = 0\n",
    "        for name, parameter in self.named_parameters():\n",
    "            if parameter.requires_grad:\n",
    "                params = parameter.numel()\n",
    "                print(f\"{name}: {params}\")\n",
    "                total_params += params\n",
    "        print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "   \n",
    "\n",
    "# Example usage:\n",
    "def main():\n",
    "    # Example parameters\n",
    "    batch_size = 32\n",
    "    input_channels = 3\n",
    "    temporal_length = 20\n",
    "    num_vertices = 33  # Example for skeleton data\n",
    "    num_classes = 13   # Example for action recognition\n",
    "    \n",
    "    # Create model\n",
    "    model = CNNLSTM(input_channels=input_channels, \n",
    "                    num_classes=num_classes)\n",
    "    \n",
    "    # Example input\n",
    "    x = torch.randn(batch_size, input_channels, temporal_length, num_vertices)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    model.count_parameters()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
