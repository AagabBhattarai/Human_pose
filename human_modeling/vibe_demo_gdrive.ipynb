{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QmY4dDYpmfB"
      },
      "source": [
        "# VIBE: Video Inference for Human Body Pose and Shape Estimation\n",
        "\n",
        "Demo of the original PyTorch based implementation provided here: https://github.com/mkocabas/VIBE\n",
        "\n",
        "## Note\n",
        "Before running this notebook make sure that your runtime type is 'Python 3 with GPU acceleration'. Go to Edit > Notebook settings > Hardware Accelerator > Select \"GPU\".\n",
        "\n",
        "## More Info\n",
        "- Paper: https://arxiv.org/abs/1912.05656\n",
        "- Repo: https://github.com/mkocabas/VIBE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvd4cfPk5a0e"
      },
      "source": [
        "# Clone the repo\n",
        "!git clone https://github.com/mkocabas/VIBE.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sui0UeZR5vCy"
      },
      "source": [
        "%cd VIBE/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs6UTvVO6Fxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2147848a-092b-4976-a914-d593ad86e13d"
      },
      "source": [
        "# Install the other requirements\n",
        "# !pip install torch==1.4.0 numpy==1.17.5\n",
        "# !pip install git+https://github.com/giacaglia/pytube.git --upgrade\n",
        "# # !pip install git+https://github.com/mkocabas/multi-person-tracker.git\n",
        "# !pip install -r requirements.txt\n",
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/VIBE\n",
        "%cd /content/VIBE\n",
        "\n",
        "!source scripts/prepare_data.sh\n",
        "\n",
        "!pip install -q git+https://github.com/mkocabas/multi-person-tracker\n",
        "!pip install -q git+https://github.com/mkocabas/yolov3-pytorch\n",
        "!pip install -q git+https://github.com/mattloper/chumpy\n",
        "!pip install -q git+https://github.com/giacaglia/pytube\n",
        "!pip install -q yacs smplx trimesh pyrender progress filterpy scikit-video"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'VIBE'...\n",
            "remote: Enumerating objects: 418, done.\u001b[K\n",
            "remote: Counting objects: 100% (225/225), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 418 (delta 164), reused 133 (delta 132), pack-reused 193 (from 1)\u001b[K\n",
            "Receiving objects: 100% (418/418), 15.10 MiB | 15.57 MiB/s, done.\n",
            "Resolving deltas: 100% (209/209), done.\n",
            "/content/VIBE\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1untXhYOLQtpNEy4GTY_0fL_H-k6cTf_r\n",
            "From (redirected): https://drive.google.com/uc?id=1untXhYOLQtpNEy4GTY_0fL_H-k6cTf_r&confirm=t&uuid=e471a00d-66a4-44ac-b1bc-f661797ffbb3\n",
            "To: /content/VIBE/data/vibe_data.zip\n",
            "100% 561M/561M [00:06<00:00, 90.5MB/s]\n",
            "Archive:  vibe_data.zip\n",
            "   creating: vibe_data/\n",
            "  inflating: vibe_data/smpl_mean_params.npz  \n",
            "  inflating: vibe_data/vibe_model_w_3dpw.pth.tar  \n",
            "  inflating: vibe_data/gmm_08.pkl    \n",
            "  inflating: vibe_data/J_regressor_h36m.npy  \n",
            "  inflating: vibe_data/vibe_model_wo_3dpw.pth.tar  \n",
            "  inflating: vibe_data/SMPL_NEUTRAL.pkl  \n",
            "  inflating: vibe_data/J_regressor_extra.npy  \n",
            "  inflating: vibe_data/spin_model_checkpoint.pth.tar  \n",
            "  inflating: vibe_data/sample_video.mp4  \n",
            "  inflating: vibe_data/yolov3.weights  \n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for multi_person_tracker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for yolov3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for chumpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pytube (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.8/704.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m941.6/941.6 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for PyOpenGL (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brZt0q3Y6X5W"
      },
      "source": [
        "# Download pretrained weights and SMPL data\n",
        "!source scripts/prepare_data.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "# Define base folder path\n",
        "base_path = '/content/gdrive/MyDrive/RGB_data_stream'\n",
        "sys.path.append(os.path.abspath(\"VIBE\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR6L7zKRHRY1",
        "outputId": "24b41b52-01ae-4632-9ab8-7795fc6db315"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_dir = os.path.join(base_path, 'short')\n",
        "output_dir = os.path.join(base_path, 'VIBE')\n",
        "os.makedirs(output_dir, exist_ok = True)"
      ],
      "metadata": {
        "id": "P2-3hKTtHWyu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_video = os.path.join(video_dir, '1002.mp4')"
      ],
      "metadata": {
        "id": "Y83IVu8UHf9v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7A7eakXIn9A"
      },
      "source": [
        "### Run the demo code.\n",
        "\n",
        "Check https://github.com/mkocabas/VIBE/blob/master/doc/demo.md for more details about demo.\n",
        "\n",
        "**Note:** Final rendering is slow compared to inference. We use pyrender with GPU accelaration and it takes 2-3 FPS per image. Please let us know if you know any faster alternative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVNszfLQ7rC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68851506-e66d-4bfa-ed92-d152fc56598d"
      },
      "source": [
        "# Run the demo\n",
        "!python demo.py --vid_file {test_video} --output_folder {output_dir} --sideview\n",
        "\n",
        "# You may use --sideview flag to enable from a different viewpoint, note that this doubles rendering time.\n",
        "# !python demo.py --vid_file sample_video.mp4 --output_folder output/ --sideview\n",
        "\n",
        "# You may also run VIBE on a YouTube video by providing a link\n",
        "# python demo.py --vid_file https://www.youtube.com/watch?v=c4DAnQ6DtF8 --output_folder output/ --display"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running \"ffmpeg -i /content/gdrive/MyDrive/RGB_data_stream/short/1002.mp4 -f image2 -v error /tmp/1002_mp4/%06d.png\"\n",
            "Images saved to \"/tmp/1002_mp4\"\n",
            "Input video number of frames 151\n",
            "Running Multi-Person-Tracker\n",
            "  0% 0/13 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/yolov3/models.py:131: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n",
            "100% 13/13 [00:09<00:00,  1.33it/s]\n",
            "Finished. Detection + Tracking FPS 15.46\n",
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/content/VIBE/lib/models/vibe.py:147: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(pretrained)\n",
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "/content/VIBE/lib/models/vibe.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_dict = torch.load(pretrained)['model']\n",
            "=> loaded pretrained model from 'data/vibe_data/spin_model_checkpoint.pth.tar'\n",
            "/content/VIBE/demo.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(pretrained_file)\n",
            "Performance of pretrained model on 3DPW: 56.56075477600098\n",
            "Loaded pretrained weights from \"data/vibe_data/vibe_model_wo_3dpw.pth.tar\"\n",
            "Running VIBE on each tracklet...\n",
            "  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 1/1 [00:03<00:00,  3.90s/it]\n",
            "VIBE FPS: 38.67\n",
            "Total time spent: 16.13 seconds (including model loading time).\n",
            "Total FPS (including model loading time): 9.36.\n",
            "Saving output results to \"/content/gdrive/MyDrive/RGB_data_stream/VIBE/1002/vibe_output.pkl\".\n",
            "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n",
            "Rendering output video, writing frames to /tmp/1002_mp4_output\n",
            " 79% 119/151 [00:51<00:16,  1.90it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8zxBa_K-FJf"
      },
      "source": [
        "A# Play the generated video\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def video(path):\n",
        "  mp4 = open(path,'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "  return HTML('<video width=500 controls loop> <source src=\"%s\" type=\"video/mp4\"></video>' % data_url)\n",
        "\n",
        "video_result = os.path.join('')\n",
        "video(output)\n",
        "video('output/sample_video/sample_video_vibe_result.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGcw0HzhtPxj"
      },
      "source": [
        "# Inspect the output file content\n",
        "import joblib\n",
        "output = joblib.load('output/sample_video/vibe_output.pkl')\n",
        "print('Track ids:', output.keys(), end='\\n\\n')\n",
        "\n",
        "print('VIBE output file content:', end='\\n\\n')\n",
        "for k,v in output[1].items():\n",
        "  if k != 'joints2d':\n",
        "    print(k, v.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}